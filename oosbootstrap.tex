\documentclass[12pt,fleqn]{article}
\input{tex/setup}
\input{VERSION}
\begin{document}

\author{Gray Calhoun\thanks{ Economics Department; Iowa State
    University; Ames, IA 50011.  Telephone: (515) 294-6271.  Email:
    \guillemotleft\protect\url{gcalhoun@iastate.edu}\guillemotright.  Web:
    \guillemotleft\protect\url{http://gray.clhn.co}\guillemotright.  I'd like to
    thank Helle Bunzel, Todd Clark, and Michael McCracken
    for helpful comments and discussions.  I'd also like to thank Amit
    Goyal for providing computer code and data for his 2008
    RFS paper with Ivo
    Welch \citep{GoW:08}.} \\
  Iowa State University}

\title{A simple block bootstrap for asymptotically normal
  out-of-sample test statistics}

\maketitle

\begin{abstract} \noindent%
  This paper proposes an improved block bootstrap method for
  out-of-sample statistics. Previous block bootstrap methods for these
  statistics have centered the bootstrap out-of-sample average on the
  observed out-of-sample average, which can cause the distribution to
  be miscentered under the null --- these papers have used either a
  short out-of-sample period or an adjustment to the model parameter
  estimators under the bootstrap to correct this centering
  problem. Our approach centers the bootstrap replications correctly
  under the null while continuing to use the standard formulas to
  estimate the model parameters under the bootstrap, while allowing
  the out-of-sample period to remain large. The resulting approach is
  computationally more efficient and easier to program.

\strut

\noindent Keywords: Forecast Evaluation, Martingale Difference
Sequence, Model Selection, Family-Wise Error Rate; Multiple Testing;
Bootstrap; Reality Check

\strut

\noindent JEL Classification Numbers: C22, C53

\end{abstract}

\newpage

\section{Introduction}

This paper develops a block bootstrap method that can be used to
consistently estimate the distributions of asymptotically normal
out-of-sample (\oos) test statistics. Since the asymptotic variance of
these statistics can have a complicated formula in many applications,
bootstrap methods have the potential to be very useful in applied
work. However, existing bootstraps are either based on parametric
models of the null hypothesis (typically using sieve arguments, so
that the effect of imposing a functional form dies out in large
samples; see [fill in]), only hold with strict restrictions on the
size of the test sample \citep{Whi:00,Han:05}, or require complicated
estimation strategies in the bootstrap \citep{CoS:07}, making this
approach impractical. In this paper, we show that standard
nonparametric block bootstraps are also consistent and derive the
correct centering term to ensure consistency.

The next section presents our block bootstrap result.
Section~\ref{sec:4} concludes.

\section{The Bootstrap for Out-of-Sample Statistics}

We'll develop our theoretical results in a fairly general
framework. Let $y_{t+1}$ be the target variable of interest --- the
variable that is being predicted --- and let $x_t$ be a vector of
other variables that are potentially informative about $y_{t+1}$ ---
these are our predictors. The forecast $\hat y_{t+1}$ depends on the
variables $x_t$ and an estimated parameter $\bh_t$. In the research
project that we're trying to model, we're interested in a function of
these variables and parameters, and the \oos\ average of that function
is our test statistic.

In symbols, we're interested in statistics of the form
\begin{equation*}
  \fb = \oclt{t} f(y_{t+1}, x_t, \bh_t).
\end{equation*}
To make the notation cleaner, we'll define $f_t(\beta) \equiv f(y_{t+1},
x_t, \beta)$. We're also going to assume that $(y_{t+1}, x_t)$ is
strictly stationary to simplify our presentation. One could derive the
same results under the marginally weaker assumption that certain
functions of these variables are weakly stationary.

The coefficients are updated each period to mimic a true \oos\
forecasting exercise. In this paper, we're going to assume that
$\bh_t$ is an $M$-estimator. We expect that other classes of
estimators can be used as well, as long as they are amenable to the
bootstrap.

Using standard terminology, the estimator $\bh_t$ is defined as
\begin{equation}\label{eq:9}
  \hat\beta_t = \begin{cases}
    \argmin_\beta \sum_{s=1}^{t-1} q(y_{s+1}, x_s, \beta) & \text{recursive window} \\
    \argmin_\beta \sum_{s=t-R+1}^{t-1} q(y_{s+1}, x_s, \beta) & \text{rolling window} \\
    \argmin_\beta \sum_{s=1}^{R-1} q(y_{s+1}, x_s, \beta) & \text{fixed window},
  \end{cases}
\end{equation}
and, as before, to make the notation cleaner, define $q_s(\beta)
\equiv q(y_{s+1},x_s,\beta)$. Obviously, for this to be a reasonable
estimation approach $q_s$ will need to satisfy standard assumptions
that we'll discuss soon.

So far, this is is a very standard setup. Our novelty is in how we
propose bootstrapping $\fb$. We're going to use a block bootstrap
here. For the moment, consider the circular block bootstrap, which is
implemented by drawing blocks of $\ell_1$ consecutive observations
from the original dataset. For the circular block bootstrap, if the
block extends beyond the dataset it is ``wrapped around,'' so, for
example, $(T-1, T, 1, 2, 3, 4)$ is a possible block of indices when
the block length is 6.

We know from West's (1996) paper that, under appropriate assumptions,
$\sqrt{P} (\fb - \E f_t(\btrue))$ is asymptotically normal with mean
zero. The key insight in our paper is that we can match this result in
the bootstrap, but we need to be careful about the exact centering
term.  In particular, under the circular bootstrap, $\btrue^*$ is the
equivalent of $\btrue$ and the sample average is the equivalent of the
population mean. That means that we should expect
\begin{equation*}
  \sqrt{P} (\fb^* - \E^* f_t^*(\btrue^*))
\end{equation*}
to have the same asymptotic distribution and to give reliable
confidence intervals, etc. where, for the circular and stationary
block bootstraps,
\begin{equation*}
  \E^* f_t^*(\beta) = \tfrac{1}{T-1} \sum_{t=1}^{T-1} f_t(\beta)
\end{equation*}
and
\begin{equation*}
  \btrue^* = \argmin_\beta \sum_{s=1}^{T-1} q_s(y_{s+1}, x_s, \beta).
\end{equation*}
For the moving blocks bootstrap, a slight correction is necessary
since observations at the ends of the sample are less likely to be
selected, but the same equations hold approximately. And, after
spelling out our specific assumptions, that's what we'll show!

But, just to spell it out first, here
\begin{equation}
  \fb^* = \oavg{t} f(y_{t+1}^*, x_t^*, \bh_t^*) \equiv \oavg{t} f_t^*(\bh_t^*)
\end{equation}
where $\bh_t^*$ is estimated exactly the same way as $\bh_t$:
\begin{equation}\label{eq:10}
  \hat\beta_t^* = \begin{cases}
    \argmin_\beta \sum_{s=1}^t q_{s}^*(\beta) & \text{recursive window} \\
    \argmin_\beta \sum_{s=t-R+1}^t q_{s}^*(\beta) & \text{rolling window} \\
    \argmin_\beta \sum_{s=1}^R q_{s}^*(\beta) & \text{fixed window}.
  \end{cases}
\end{equation}

In general, let $\to^{p^{*}}$ and
$\to^{d^{*}}$ refer to convergence in probability or distribution
conditional on the observed data.  Similarly, $\E^{*}$, $\var^{*}$,
and $\cov^{*}$ refer to the expectation, variance, and covariance with
respect to the probability measure induced by the bootstrap, and
$y_t^{*}$, etc. is the random variable $y_t$ but under the
bootstrap-induced \cdf.

\begin{asmp}\label{a1}
  The estimators $\bh_t$ and $\bh_t^*$ are estimated as defined in
  Equations~\eqref{eq:9} and~\eqref{eq:10}. Moreover $\btrue =
  \argmin_\beta \E\, q_s(\beta)$ is unique.
\end{asmp}

\begin{asmp}\label{a3}
  The stochastic process $(y_{t+1}, x_t)$ is strictly stationary and
  strong-mixing of size $-r/(r-2)$ or uniform mixing of size
  $-r/(2r-2)$ with $r>2$.
\end{asmp}

\begin{asmp}\label{a2}
  Both $f_t$ and $q_t$ are twice continuously differentiable in an
  open neighborhood $N$ of $\btrue$ and $\E \nabla^2 q_t(\beta)$ is
  positive definite uniformly in $N$. Also there exists a sequence of
  random variables $m_t$ such that $\sup_{i,\beta \in N} |\nabla_i^2
  q_t(\beta)| \leq m_t$, $\sup_{i,\beta \in N} |\nabla_i^2
  f_t(\beta)| \leq m_t$, $\sup_{i,\beta \in N} |\nabla_i q_t(\beta)|
  \leq m_t$, and $\sup_{i,\beta \in N} |\nabla_i f_t(\beta)|
  \leq m_t$ almost surely and $\E m_t$ is uniformly finite.
\end{asmp}

\begin{asmp}\label{a4}
  The bootstrap sequence $(y_2^*, x_1^*),\dots,(y_T^*, x_{T-1}^*)$ is
  constructed using a moving blocks, circular blocks, or stationary
  bootstrap with block lengths drawn from the geometric distribution.
  The (expected) block length $\ell_1$ satisfies $\ell_1 \to \infty$
  and $\ell_1/P \to 0$.
\end{asmp}

\begin{asmp}\label{a6}
  The asymptotic variance matrix of $\bar{f}(\btrue)$ is uniformly
  positive definite.
\end{asmp}

\begin{asmp}\label{a7}
  $R, P \to \infty$ as $T \to \infty$.
\end{asmp}

Then the main result is proving consistency of the bootstrap
distribution and the bootstrap variance.

\begin{thm}\label{res:3}
  Under Assumptions~\ref{a1} -- \ref{a7},
  \begin{equation}\label{eq:14}
    \var(\fb)/\var^*(\fb^*) \to^p 1
  \end{equation}
  and
  \begin{equation}\label{eq:15}
    \pr\big[\sup_x \big\lvert \pr^*[\sqrt{P} (\fb^* - \E^* f_t^*) \leq x]
    - \pr[\sqrt{P}( \bar{f} - \E f_t) \leq x] \big\rvert > \epsilon\big] \to 0
  \end{equation}
  for all $\epsilon > 0$.
\end{thm}

\begin{rem}
  \citet{Mcc:00} proves asymptotic normality under weaker smoothness
  conditions on $f_t(\beta)$ and $h_t(\beta)$: only their expectations
  must be continuously differentiable.  It may be possible to extend
  Theorem~\ref{res:3} to those weaker conditions, but the current
  proof relies on a theorem of \citepos{JoD:00} establishing the
  consistency of \hac\ estimators and their theorem requires differentiability of
  the observations.  Extending \citepos{JoD:00} result to
  nondifferentiable functions should be possible, but is beyond the
  scope of this paper.
\end{rem}

\begin{rem}
  \citet{Whi:00} and \citet{Han:05} resample the forecasts but do not
  reestimate any of them which requires the additional assumption that
  $\tfrac{P}{R} \log \log R \to 0$ or that the forecasts themselves
  have no estimated parameters.%
\footnote{\citet{Whi:00} lists several
    different sets of assumptions that give the same result, but these
    seem to be the most general.} %
\end{rem}

\begin{rem}
  \citet{CoS:07} use the distribution of $\sqrt{P}(\bar{f}^{*} -
  \bar{f})$ to approximate that of $\sqrt{P}(\bar{f} - \E
  \bar{f}(\btrue))$.  But it is clear that $\bar{f}(\btrue^*)$
  is the bootstrap analogue of $\E \bar{f}(\btrue)$, the parameter of
  interest.  Because their bootstrap is miscentered, \citet{CoS:07}
  must redefine $\hat{\beta}_t^{*}$ to achieve consistency.  In this
  paper, though, consistency arises naturally.
\end{rem}

\begin{rem}
  It may be unnecessary to assume that $\bar{f}(\btrue)$ has positive
  definite asymptotic variance; if so, the bootstrap would work in the
  setup of \citet{ClM:05,ClM:01} and \citet{Mcc:07}.  That question is
  left to future research.
\end{rem}

\section{Conclusion}\label{sec:4}
The paper improves block bootstrap procedures of \oos\
statistics.

\appendix
\section{Proofs}\label{sec:B}

For the appendix, introduce the notation

\subsection{Main theoretical results}
\newcommand{\WesA}[1][]{\oclt{t}
  (F_t^{#1} - \E^{#1} F_t^{#1}) B^{#1} H_t^{#1}}
\newcommand{\WesB}[1][]{\tfrac{1}{\sqrt{P}} \E^{#1} F_t^{#1} \osum{t} (B_t^{#1} -
  B^{#1}) H_t^{#1}}
\newcommand{\WesC}[1][]{\oclt{t}
  (F_t^{#1} - \E^{#1} F_t^{#1}) (B_t^{#1} - B^{#1}) H_t^{#1}}
\begin{proof}[Proof of Theorem~\ref{res:3}]
  As in much of this literature, we begin by expanding
  $f_t^{*}(\bh_t^*)$ around $\btrue^*$ to get
  \begin{align*}
    \sqrt{P} (\fb^* - \E^* f_t^*)
    &= \oclt{t} (f_t^{*} - \E^* f_t^*)
     + \oclt{t} \nabla f_t^*(\btrue^*) (\bh_t^* - \btrue^*)
     + \oclt{t} w_t^* \\
    &= \oclt{t} (f_t^{*} - \E^* f_t^*)
     + F^* B^* \tfrac{1}{\sqrt{P}} \sum_{t=1}^{T-1} a_t \nabla q_t^*(\btrue^*) + o_{p^*}(1)
  \end{align*}
  where (similar to \citealp{Wes:96}) the $i$th element of $w_t^*$ is
  \begin{equation*}
    w_{it} = \tfrac12 (\bh_t^* - \btrue^*)' \nabla^2 f_{it}^*(b_{it}^*) (\bh_t^* - \btrue^*)
  \end{equation*}
  and each $b_{it}^*$ lies between $\bh_t^*$ and $\btrue^*$. The second equality holds because
  $\oclt{t} w_t^* = o_{p^*}(1)$ and
  \begin{equation*}
    \oclt{t} \nabla f_t^*(\btrue^*) (\bh_t^* - \btrue^*)
    = F^* B^* \tfrac{1}{\sqrt{P}} \sum_{t=1}^{T-1} a_t \nabla q_t^* + o_{p^*}(1)
  \end{equation*}
  from Lemma~\ref{res:a4}.

  Now we'll procede using arguments very similar to
  \cite{Cal:14}. Define $u_1,\dots,u_J$ to be the first period of each
  block of the circular bootstrap, and, for each $j = 1,\dots,J$,
  define the $\sigma$-fields
  \[
  \Ms_j = \sigma(u_1,\dots,u_j)
  \]
  and
  \[
  \Ms_j^* = \sigma(u_1,\dots,u_j; y_1,\dots,y_T; x_1,\dots,x_T),
  \]
  and the vector
  \[
  Z_j^* = \tfrac{1}{\sqrt{P}} \sum_{s=(j-1) \ell + 1}^{j\ell}
  \begin{pmatrix}
    f_s^* - \E^* f_s^* \\
    a_s \nabla q_s^*
  \end{pmatrix}.
  \]
  Also let $l = T - J \ell$ be the number of elements in the last block.
  By construction, $\E^*(\nabla q_t^*) = \tfrac{1}{T-1}
  \sum_{s=1}^{T-1} \nabla q_t^* = 0$ almost surely, so $\E(Z_j \mid
  \Ms_{j-1}^*) = 0$ almost surely and $\{Z_j^*, \Ms_j^*\}$ is a
  martingale difference sequence.

  Now, let $\gamma$ be an arbitrary nonzero vector and define $z_j^* =
  \gamma'Z_j^* / (\gamma'\Sigma^*\gamma)^{1/2}$ Lemma aaaa implies
  that $z_j^{*2}$ is uniformly integrable, in the sense that
  (clarify). From the \mds\ property, we have
  \begin{equation*}
    \sum_{j=1}^J z_j^* \to^d N(0, 1)
  \end{equation*}
  as long as the following properties hold:
  \begin{equation}\label{eq:7}
    \sum_{j=1}^J \E(z_j^{*2} 1\{z_j^{*2} > \epsilon\} \mid \Ms_{j-1}^*) \to^p 0
  \end{equation}
  and
  \begin{equation}\label{eq:8}
    \pr^*\Big[ \big| \sum_{j=1}^J z_j^{*2} - 1 \big| > \epsilon\Big] \to^p 0.
  \end{equation}

  For~\eqref{eq:8}, we have the usual bound
  \begin{equation*}
    \pr^*\Big[ \big|\sum_{j=1}^J z_j^{*2} - 1 \big| > \varepsilon\Big] \leq
    \pr^*\Big[ 1\{\btrue^* \in N\} \big| \sum_{j=1}^J z_j^{*2} - 1 \big| > \epsilon \Big]
     + \pr^*[ \btrue^* \notin N ]
  \end{equation*}
  and we can rewrite the summation in the first term as
  \begin{equation*}
    1\{\btrue^* \in N\} \Big( \sum_{j=1}^J z_j^{*2} - 1 \Big)
    =  \sum_{j=1}^J \big(z_j^{*2}1\{\btrue^* \in N\} - \E(z_j^{*2} 1\{\btrue^* \in N\} \mid \Ms_{j-1}^*) \big).
   \end{equation*}
   This term is the sum of a uniformly integrable martingale
   difference sequence by Lemma aaaa and satisfies the \lln\
   (i.e. Davidson's, 1994, Theorem 19.7), and so it converges in
   (conditional) probability to zero.
   The second term converges in probability to zero by consistency of $\btrue^*$ (prove).

   The convergence in probability in~\eqref{eq:7} is implied by
   convergence in $L_1$, i.e.
   \begin{equation*}
     \sum_{j=1}^J \E(z_j^{*2} 1\{z_j^{*2} > \epsilon\}) \to 0,
   \end{equation*}
   and we have, defining
   \begin{equation*}
     \zeta_{st}^* =
     (\gamma'\Sigma^*\gamma)^{-1/2} \gamma'\begin{pmatrix}
       f_t^* - \E^* f_t^* \\
       a_s \nabla q_t^*
     \end{pmatrix},
   \end{equation*}
   \begin{align*}
     \sum_{j=1}^J &\E(z_j^{*2} 1\{z_j^{*2} > \epsilon\}) \\
     &= \sum_{j=1}^J \E(z_j^{*2} 1\{z_j^{*2} > \epsilon\}) \\
     &= \sum_{j=1}^{J-1} \E\Big(\sum_{s,t =1}^{\ell_1} \zeta^*_{\ell_1 (j-1)+s,\ell_1 (j-1)+s}\zeta^*_{\ell_1 (j-1)+t,\ell_1 (j-1)+t}
     1\Big\{ \sum_{s,t = 1}^{\ell_1} \zeta^*_{\ell_1 (j-1)+s,\ell_1 (j-1)+s}\zeta^*_{\ell_1 (j-1)+t,\ell_1 (j-1)+t} > \epsilon\Big\}\Big) \\
     &\quad +
     \E\Big(\sum_{s,t = T-\ell_2+1}^{T} \zeta^*_{ss}\zeta^*_{tt} 1\Big\{\sum_{s,t = T - \ell_2+1}^{T} \zeta^*_{ss}\zeta^*_{tt} > \epsilon\Big\}\Big) \\
     &= \tfrac{1}{T} \sum_{u=1}^T \sum_{j=1}^{J-1} \E\Big(\sum_{s,t = 1}^{\ell_1} \zeta_{\ell_1(j-1) + s,u+s}\zeta_{\ell_1(j-1) + t,u+t}
     1\Big\{\sum_{s,t = 1}^{\ell_1} \zeta_{\ell_1(j-1) + s,u+s}\zeta_{\ell_1(j-1) + t,u+t} > \epsilon\Big\}\Big) \\
     &\quad +
     \tfrac{1}{T} \sum_{u=1}^T \E\Big(\sum_{s,t = 1}^{\ell_2} \zeta_{T-\ell_2 + s,u+s}\zeta_{T-\ell_2+t,u+t}
     1\Big\{\sum_{s,t = 1}^{\ell_2} \zeta_{T-\ell_2 + s,u+s}\zeta_{T-\ell_2+t,u+t} > \epsilon\Big\}\Big) \\
     &\leq \sum_{j=1}^{J-1} B(\epsilon T / \ell_1) \ell_1 / T
     + B(\epsilon T / \ell_2) \ell_2/T \\
     &\to 0.
   \end{align*}
   The rest of the proof is fairly straightforward.
\end{proof}

\subsection{Supporting Results}

Define the following notation to describe the bootstrap process. Let
$u_1,\dots,u_{J}$ denote the starting periods of each block of the
bootstrap, let $\ell_i$ denote their lengths, define the sigma fields
\[
  \Ms_i = \sigma(u_1,\dots,u_i; \ell_1,\dots,\ell_i; J)
\]
and
\[
  \Ms_i^* = \sigma(y_1,\dots,y_T; x_1,\dots, x_T;u_1,\dots,u_i; \ell_1,\dots,\ell_i; J)
\]
and let $j^*_t$ denote the block that contains the $t$th observation
of the bootstrap process, so
\[
  t \in \{1 + \sum_{i=1}^{j^*_t-1} \ell_i, \sum_{i=1}^{j^*_t} \ell_i\}.
\]


\begin{lema}\label{res:a2}
  Suppose $a \in [0,1/2)$ and the conditions of Theorem~\ref{res:3}
  hold.
  \begin{enumerate}
  \item $P^a \sup_t | H_{t} | \to^p 0$ and $P^a \sup_t | H_{t}^{*} |
    \to^{p^{*}} 0$.
  \item $P^a \sup_t | \hat{\beta}_{t} - \beta_{0} | \to^{p} 0$ and
    $P^a \sup_t | \hat{\beta}^{*}_{t} - \hat{\beta}_T |
    \to^{p^{*}} 0$.
  \item $\tfrac{1}{\sqrt{P}} \sum_{t=R+1}^T (F_t - \E F_t) = O_{p}(1)$
    and $\tfrac{1}{\sqrt{P}} \sum_{t=R+1}^T (F_t^* - \E F_t^*) =
    O_{p^*}(1)$.
  \end{enumerate}
\end{lema}

\begin{proof}[Proof of Lemma~\ref{res:a2}]
  \begin{enumerate}
  \item Convergence for the original process is essentially the same
    as the proof in \citet{Wes:96}, with minor tweaks as in
    \citet{Cal:15}; we will repeat it here for completeness. Our
    assumptions ensure that $h_t$ is an $L_2$-mixingale of size $-1/2$;
    let $c_t$ and $\zeta_k$ denote its mixingale constants and
    coefficients and note that $h_t/t$ is also an $L_2$-mixingale array
    with constants $c_s/t$ and coefficients $\zeta_k$, since
    \begin{align*}
      \| \E_{t-k} h_t / t \| = t^{-1} \| \E_{t-k} h_t \| \leq (c_t/t)\, \zeta_k
    \end{align*}
    and
    \begin{equation*}
      \| h_t/t - \E_{t+k} h_t/t \| = t^{-1} \|  h_t - \E_{t+k} h_t \| \leq (c_t/t)\, \zeta_{k+1}.
    \end{equation*}

    Then
    \begin{align*}
      P^{2a} \E\Bigg[\omax{t} \Big| \sum_{s=1}^t h_s/t \Big|^2\Bigg]
      &\leq P^{2a} \E\Bigg[\omax{t} \Big| \sum_{s=1}^t h_s/s \Big|^2\Bigg] \\
      &= O(P^{2a}) \sum_{s=1}^{T-1} s^{-2}
    \end{align*}
    where the second line follows from \citepos{Mcl:75} maximal
    inequality (also available as \citealp{Dav:94}, Theorem 16.9 and
    Corollary 16.10). This term converges to zero from \citepos{Wes:96} Lemma A1.

    For the bootstrap process, we will use an argument similar to
    those in \cite{Cal:14} and make liberal use of the
    LIE. Define $\Hs_i^* = \sum_{t=K_{i-1}+1}^{K_i}
    h_t^*/t$, so
    \begin{align*}
      \omax{t} | H_t^* |
      &\leq \omax{t} \Big| \sum_{i=1}^{j^*_t - 1}\Hs_i^* \Big| + \omax{t} \Big|\sum_{s=K_{j^*_t - 1}+1}^t h_s^*/t \Big|
    \end{align*}
    Now observe that $\{\sum_{i=1}^j \Hs_i^*; \Ms_j^*\}$ is a martingale, so
    \begin{equation*}
      \pr^*\Bigg[ \omax{t} \Big| \sum_{i=1}^{j^*_t - 1}\Hs_i^* \Big| > \epsilon \Bigg]
      \leq \epsilon^{-2} \E^* \sum_{i=1}^J \E^*(\Hs_i^{*2} \mid \Ms_{i-1}^*).
    \end{equation*}

    We can deal with this last term as follows. The expectation
    $\E^*(\Hs_i^{*2} \mid \Ms_{i-1}^*; \ell_i)$ only averages over the
    start of the block used by $\Hs_i^*$, so
    \begin{align*}
      \E^*(\Hs_i^{*2} \mid \Ms_{i-1}^*; \ell_i)
      &= \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \Bigg[\sum_{t = 1}^{\ell_{i}} \tilde{h}_{u + t}(\btrue^*) / (K_{i-1} + t + 1)\Bigg]^2 \\
      &= \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \Bigg[\sum_{t = 0}^{\ell_{i}-1}
      (\tilde{h}_{u + t}(\btrue) + \tilde{h}_{u + t}(\btrue^*) - \tilde{h}_{u + t}(\btrue))/ (K_{i-1} + t + 1)\Bigg]^2
    \end{align*}
    and we want to show that
    \begin{equation}\label{eq:4}
      P^a \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \sum_{i=1}^J \Bigg[\sum_{t = 1}^{\ell_{i}}
      \tilde{h}_{u + t}(\btrue)/ (K_{i-1} + t)\Bigg]^2 \to^p 0
    \end{equation}
    and
    \begin{equation}\label{eq:5}
      P^a \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \Bigg[\sum_{t = 1}^{\ell_{i}}
      (\tilde{h}_{u + t}(\btrue^*) - \tilde{h}_{u + t}(\btrue))/ (K_{i-1} + t)\Bigg]^2
      \to^p 0.
    \end{equation}

    For~\eqref{eq:4}, the $L_2$-mixingale property again ensures that
    \begin{align*}
      \E\Bigg|P^a \tfrac{1}{T-1} &\sum_{u = 0}^{T-2} \sum_{i=1}^J \Bigg[\sum_{t = 1}^{\ell_{i}}
      \tilde{h}_{u + t}(\btrue)/ (K_{i-1} + t)\Bigg]^2 \Bigg| \\
      &= P^a \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \E \sum_{i=1}^J \E\Bigg(\Bigg[\sum_{t = 1}^{\ell_{i}}
      \tilde{h}_{u + t}(\btrue)/ (K_{i-1} + t)\Bigg]^2 \,\Big|\, \Ms_{i-1}, \ell_i\Bigg) \\
      &\leq P^a \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \E \sum_{i=1}^J C \sum_{t = 1}^{\ell_{i}} (K_{i-1} + t)^{-2} \\
      &\leq P^a \sum_{t=1}^{T-1} t^{-2}
    \end{align*}
    (should be $2a$ in all of the above)
    which converges to zero as before. For~\eqref{eq:5},
    \begin{align*}
      \tfrac{1}{T-1} &\sum_{u = 0}^{T-2} \Bigg[\sum_{t = 1}^{\ell_{i}}
      (\tilde{h}_{u + t}(\btrue^*) - \tilde{h}_{u + t}(\btrue))/ (K_{i-1} + t)\Bigg]^2 \\
      &= \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \Bigg[\sum_{t = 1}^{\ell_{i}}
      \nabla \tilde{h}_{u + t}(\tilde \beta_T)'(\btrue^* - \btrue)/ (K_{i-1} + t)\Bigg]^2 \\
      &= O_p(1/T) \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \Bigg[\sum_{t = 1}^{\ell_{i}}
      \lVert \nabla \tilde{h}_{u + t}(\tilde \beta_T) \rVert / (K_{i-1} + t)\Bigg]^2
    \end{align*}
    and
    \begin{align*}
      \E \Bigg| (P^{2a}/T) &\tfrac{1}{T-1} \sum_{u = 0}^{T-2} \sum_{i=1}^J \Bigg[\sum_{t =
        1}^{\ell_{i}} \lVert \nabla \tilde{h}_{u + t}(\tilde \beta_T)
      \rVert / (K_{i-1} + t)\Bigg]^2 \Bigg| \\
      &= (P^{2a}/T) \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \E \sum_{i=1}^J
      \E \Bigg(\Bigg[\sum_{t =
        1}^{\ell_{i}} \lVert \nabla \tilde{h}_{u + t}(\tilde \beta_T)
      \rVert / (K_{i-1} + t)\Bigg]^2 \,\Bigg|\, \Ms_{i-1}; \ell_i \Bigg) \\
      &\leq (P^{2a}/T) \E \sum_{i=1}^J \ell_i^2/K_{i-1}^2
    \end{align*}
    which converges to zero by Lemma~[add!].

    Since
    \[
    \omax{t} \Big|\sum_{s=K_{j^*_t - 1}+1}^t h_s^*/t \Big| \leq \sum_{i=1}^{J} \max_{t = K_{i-1}+1,\dots,K_i} \Big| \sum_{s=K_{i-1}+1}^t h_s^*/t \Big|
    \]
    the proof that the second component converges to zero is similar.
  \item We have
    \begin{multline}
      P^a \sup_t | \hat{\beta}_t - \btrue | = P^a \sup_t |\hat{B}_{t}
      H_{t}| \\ \leq \sup_{t,u} \Big| [ \hat{B}_u - B]
      \tfrac{P^a}{t} \sum_{s=1}^t h_{s} \Big| + \sup_{t} \Big|
      B \tfrac{P^a}{t} \sum_{s=1}^t h_{s} \Big|
    \end{multline}
    and both terms converge to zero in (conditional) probability by
    the previous argument and by assumption.  The same argument holds
    for $\hat{\beta}_t^{*} - \btrue^*$ as well.
  \item $\tfrac{1}{\sqrt{P}} \sum_{t=R+1}^T (F_t - \E F_t)$ obeys
    \citepos{Jon:97} \clt, so the result is trivial. For the
    bootstrapped process, we know that the bootstrap variance is
    equivalent to (...HAC estimator...) which is consistent by
    \citepos{JoD:00} Theorem (2?).\qedhere
  \end{enumerate}
\end{proof}

\begin{lema}\label{res:a4}
  Under the conditions of Theorem~\ref{res:3},
  \begin{equation}\label{eq:16}
    \oclt{t} (\bh_t^* - \btrue^*)' \nabla^2 f_{it}^*(b_{it}^*) (\bh_t^* - \btrue^*) \to^{p^*} 0
  \end{equation}
  and
  \begin{equation}\label{eq:17}
    \oclt{t} F_t^*(\btrue^*) (\bh_t^* - \btrue^*)
    = F^* B^* \tfrac{1}{\sqrt{P}} \sum_{t=1}^{T-1} a_t \nabla q_t^* + o_{p^*}(1)
  \end{equation}
Equations
  \eqref{eq:7}--\eqref{eq:10} and \eqref{eq:11}--\eqref{eq:13} hold.
\end{lema}

\begin{proof}[Proof of Lemma~\ref{res:a4}]
For~\eqref{eq:16}, we have
\begin{align*}
  \pr\Big[& \big|\oclt{t} (\bh_t^* - \btrue^*)' \nabla^2 f_{it}^*(b_{it}^*) (\bh_t^* - \btrue^*) \big| > \epsilon \Big] \\
  &\leq \pr\Big[1\{\btrue^* \in N, \bh_t^* \in N\ \text{for all}\ t\} \big|\oclt{t} (\bh_t^* - \btrue^*)' \nabla^2 f_{it}^*(b_{it}^*) (\bh_t^* - \btrue^*) \big| > \epsilon \Big] \\
  &\quad + \pr[\btrue^* \notin N] + \pr[\bh_t^* \notin N\ \text{for some}\ t\}
\end{align*}
The second two probabilities converge to zero by
Lemma~\ref{res:a2}. And
\begin{multline*}
  1\{\btrue^* \in N, \bh_t^* \in N\ \text{for all}\ t\}
  \oclt{t} (\bh_t^* - \btrue^*)' \nabla^2 f_{it}^*(b_{it}^*) (\bh_t^* - \btrue^*)
  \\ \leq
  \big(\sup_t |P^{1/4}(\bh_t^* - \btrue^*)|^2\big) \oavg{t}  \nabla^2 f_{it}^*(b_{it}^*) 1\{\btrue^* \in N, \bh_t^* \in N\}.
\end{multline*}
The summation is $O_p(1)$ by assumption and the supremum converges to
zero by Lemma~\ref{res:a2} again.

For~\eqref{eq:17},
\begin{multline*}
  \big\lvert \oclt{t} \big(F_t^*(\btrue^*) (\bh_t^* - \btrue^*) - F^* B^* a_t \nabla q_t^*\big) \big\rvert \leq \\
   \sup_s |\bh_s^* - \btrue^*|\; \oclt{t} (F_t^*(\btrue^*) - F^*)
  + \big\lvert F^* \oclt{t} \big((\bh_t^* - \btrue^*) - B^* a_t \nabla q_t^*\big) \big\rvert
\end{multline*}
The first term converges in conditional probability to zero by
Lemma~\ref{res:a2}.  For the second, expand each $\sum_{s=1}^t \nabla
q_s^*(\bh_t^*)$ around $\btrue^*$ to get
\begin{equation*}
  \oclt{t} (\bh_t^* - \btrue^*)
  = \oclt{t} \Big[\tfrac{1}{t} \sum_{s=1}^t \nabla^2 q_s^*(b_t)\Big]^{-1} \tfrac{1}{t} \sum_{s=1}^t \nabla q_s^*
\end{equation*}
and then
\begin{align*}
  \big\lvert \oclt{t} \big((\bh_t^* - \btrue^*) - B^* a_t \nabla q_t^*\big) \big\rvert
  &\leq \sup_{t=R,\dots,T-1} \Big\lvert\Big[\tfrac{1}{t} \sum_{s=1}^t \nabla^2 q_s^*(b_t)\Big]^{-1} - B^*\Big\rvert
  \Big\lvert\oclt{t} \tfrac{1}{t} \sum_{s=1}^t \nabla q_s^* \Big\rvert \\
  &= \sup_{t=R,\dots,T-1} \Big\lvert\Big[\tfrac{1}{t} \sum_{s=1}^t \nabla^2 q_s^*(b_t)\Big]^{-1} - B^*\Big\rvert \cdot O_{p^*}(1)
\end{align*}
by Lemma~(fill in).

Now,
\begin{multline*}
  \pr\Big[\big\lvert \tfrac{1}{t} \sum_{s=1}^t \nabla^2 q_s^*(b_t^*) - B^{*-1} \big\rvert > \epsilon \Big]
  \leq
  \pr\Big[\big\lvert \tfrac{1}{t} \sum_{s=1}^t \nabla^2 q_s^* - B^{*-1} \big\rvert 1\{\beta_0^* \in N\}> \epsilon \Big] \\
  + \pr\Big[\big\lvert \tfrac{1}{t} \sum_{s=1}^t (\nabla^2 q_s^*(b_t^*) - \nabla^2 q_s^*) \big\rvert
  1\{\beta_0^* \in N,\ \bh_t^* \in N\} > \epsilon \Big] + \pr[\beta_0^* \notin N] + \pr[\bh_t^* \notin N]
\end{multline*}
As elsewhere, the last two probabilities converge to zero by (explain).

- first probability converges by \lln;

- second converges by continuity of $\nabla^2 q_s$ in a neighborhood of $\beta_0$.

This implies that
\begin{equation*}
  \Big[\tfrac{1}{t} \sum_{s=1}^t \nabla^2 q_s^*(b_t)\Big]^{-1} - B^* \to^{p^*} 0
\end{equation*}
for any $t > R$. Then need to establish \emph{uniform} \lln\ and you'll be done.
\end{proof}

\begin{lema}\label{res:a3}
  Suppose the conditions of Theorem~\ref{res:3} hold, let
  \[\psi_t^{*}(\beta) =
  \binom{f_t^{*}(\beta) - \bar{f}(\beta)}{h_t^{*}(\beta) - \bar h(\beta)},\]
  and define $\psi_t = \psi_t(\btrue)$, $\hat\psi_t = \psi_t(\btrue^*)$, and
  $\psi_t^* = \psi_t^*(\btrue^*)$.  Then
  \begin{equation}\label{eq:1}
    \pr\Big[\sup_x \Big| \pr^{*}\Big[ \oclt{t} \psi_{t}^{*}
    \leq x \Big] - \pr\Big[ \oclt{t} \psi_{t}
    \leq x \Big] \Big| > \epsilon \Big] \to 0
  \end{equation}
  for all positive $\epsilon$, where the inequalities hold element-by-element.
\end{lema}

\begin{proof}[Proof of Lemma~\ref{res:a3}]
  Note that both $\psi_t$ and $\psi_t^*$ satisfy
  \clt s, (\citealp[Theorem~2]{Jon:97} and
  \citealp[Theorem~1]{Cal:14}) so it suffices to prove that
  \[ \tfrac{ 1 }{ P } \var^* \osum{t} \psi_t^*
  - \tfrac{ 1 }{ P } \var \osum{t} \psi_t \to^p
  0. \] I will prove this result for the Circular Block Bootstrap
  \citep{PoR:92}---the proof for other block bootstraps is similar and
  follows as in \cite{Cal:14}.

  \newcommand{\su}{\tfrac{ 1 }{ b T } \sum_{ \tau = 1 }^{ T }%
  \sum_{ s, t \in I( \tau ) }}

  Since $\E^* \psi_t^* = 0$ by construction, we have
  \begin{equation}\label{eq:3}
  \tfrac{ 1 }{ P } \var^* \sum_{ t = R + 1 }^T \psi_t^*
  = \su \hat \psi_s \hat \psi_t'
  \end{equation}
  almost surely, with
  \begin{equation*}
    I(\tau) =
    \begin{cases}
      \{ \tau, \dots, \tau + b - 1 \} & \text{if } \tau \leq T - b + 1 \\
      \{ \tau, \dots, T \} \cup \{ 1, \dots, \tau - ( T - b + 1 ) \}
      & \text{if } \tau > T - b + 1.
    \end{cases}
  \end{equation*}
  We can rewrite the right side of \eqref{eq:3} as
  \begin{multline}\label{eq:6}
    \su \hat\psi_s \hat\psi_t'
    = \tfrac{1}{T} \sum_{s,t=1}^T \hat\psi_s \hat\psi_t' \;
      \big(1 - \tfrac{\lvert s - t \rvert}{b}\big)^+ \\
      + \tfrac{1}{bT} \sum_{s = 1}^{b - 1} \sum_{t = T - b + s}^{T}
        (\hat\psi_s \hat\psi_t' +
         \hat\psi_t \hat\psi_s') \;
        \big(1 - \tfrac{\lvert s - t + T\rvert}{b}\big)^+.
  \end{multline}

  The first term of \eqref{eq:6} can be shown to converge in
  probability to $\tfrac{1}{P} \var \osum{t} \psi_t$ by Theorem 2.2 of
  \citet{JoD:00}.  Most of the necessary conditions of their theorem
  hold by assumption and it remains to prove their condition (2.9)
  holds, namely that
  \begin{equation}\label{eq:2}
    \sup_{\beta \in N} \Bigg\lVert \tfrac{1}{T} \sum_{t=1}^T e^{i \xi t/b}
    \Bigg( \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} -
          \E \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} \Bigg)
    \Bigg\rVert_2 \to 0
  \end{equation}
  for all $j$, $k$, and $\xi$.  For any fixed $\beta \in N$,
  \begin{equation*}
    \tfrac{1}{T} \sum_{t=1}^T \cos( \xi t / b )
    \Bigg( \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} -
          \E \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} \Bigg)
    \to^p 0
  \end{equation*}
  and
  \begin{equation*}
    \tfrac{1}{T} \sum_{t=1}^T \sin( \xi t / b )
    \Bigg( \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} -
          \E \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} \Bigg)
    \to^p 0
  \end{equation*}
  by a mixingale \lln\ \citep{Dav:93}.%
\footnote{Remember that \ned\
    processes are also mixingales.  See, for example, \citet[Section
    17.2]{Dav:94}.} %
  Convergence in $L_2$ for each $\beta \in N$ then
  follows from the existence of the $r$th moment ($r > 2$) and uniform
  convergence is a consequence of our bound on the second derivative
  of $\psi_t$.

  The conclusion then holds if the second term of~\eqref{eq:6} is
  $o_p(1)$.  We have, for each element of that matrix,
  \begin{equation*}
    \Big| \tfrac{1}{bT} \sum_{s = 1}^{b - 1} \sum_{t = T - b + s}^{T}
        \hat\psi_{ks} \hat\psi_{jt}
        \big(1 - \tfrac{\lvert s - t + T\rvert}{b}\big)^+ \Big| \leq
     \tfrac{1}{b T} \sum_{s = 1}^{b - 1} |\hat\psi_{ks}|
         \sum_{t = T - b + 1}^{T} | \hat\psi_{jt} |
  \end{equation*}
  almost surely.  For any positive $\delta$,
  \begin{equation*}
    \pr \Bigg[ \tfrac{1}{T} \sum_{s = 1}^{b - 1} | \hat\psi_{is} |
              > \delta \Bigg] \leq
    \pr \Bigg[\tfrac{1}{T} \sum_{s = 1}^{b - 1} \sup_{ \beta \in N } | \psi_{is}( \beta ) |
              > \delta \Bigg] +
    \pr \big[ \hat \beta_{T+1} \notin N \big].
  \end{equation*}
  Both terms converge to zero by assumption, so $\tfrac{1}{T} \sum_{s
    = 1}^{b - 1} |\hat\psi_{is}| = o_p(1)$.  A similar
  argument shows that $\tfrac{1}{b} \sum_{t = T - b + 1}^{T} |
  \hat\psi_{jt} | = O_p(1)$, completing the proof.
\end{proof}

\begin{lema}\label{res:a5}
  If the conditions of Theorem~\ref{res:3}, except for those governing
  the bootstrap process, hold then
  \begin{equation}
    \oclt{t}
    (\hat f_t - \E f_t) \to^d N(0, \sigma^2),
  \end{equation}
  with
  \begin{align}
  \sigma^2 &= S_{ff} + \lambda_{fh} (F B S_{fh}' + S_{fh} B' F') + 2 \lambda_{hh} F V_\beta F' \\
  (\lambda_{fh}, \lambda_{hh}) &=
  \begin{cases}
    (1, 2) \times \big(1 - \tfrac{1}{\pi} \ln(1 + \pi)\big)
      & \text{recursive window} \\
    \big(\tfrac{\pi}{2}, \pi - \tfrac{\pi^2}{3}\big)
      & \text{rolling window with $\pi \leq 1$} \\
    \big(1 - \tfrac{1}{2\pi}, 1 - \tfrac{1}{3\pi}\big)
      & \text{rolling window with $\pi > 1$} \\
    (0, \pi) & \text{fixed window},
  \end{cases}
  \end{align}
  $\Big(\begin{smallmatrix}S_{ff} & S_{fh} \\ S_{fh}' &
    S_{hh} \end{smallmatrix} \Big)$ the asymptotic variance of $(\bar
  f(\btrue)', \bar h(\btrue)')'$, and $V_\beta$ the asymptotic
  variance of~$\btrue^*$.
\end{lema}

\begin{proof}[Proof of Theorem~\ref{res:3}]
  As in \citet{Wes:96} and \citet{WeM:98} (and as in the proof of
  Theorem~\ref{res:3}), expand $\hat f_t$ around
  $\btrue$ to get
  \begin{align*}
    \sqrt{P} \big(\bar{f} - \bar{f}(\btrue)\big) &= \oclt{t}
    \big(f_t - \E f_t\big) +
    \E F_t B \oclt{t} H_t \\
    & \quad + \WesA + \WesB \\ & \quad + \WesC + \oclt{t} w_t
  \end{align*}
  where (again, as in \citealp{Wes:96}) the $i$th element of $w_t$ is
  \begin{equation*}
    w_{it} = \tfrac12 (\hat\beta_t - \btrue)'
    \Big[\tfrac{\partial^2}{\partial \beta \partial\beta'}
    f_{it}(\tilde\beta_{it}) \Big]
    (\hat\beta_t - \btrue)
  \end{equation*}
  and each $\tilde\beta_{it}$ lies between $\hat\beta_t$ and
  $\btrue$; $\oclt{t} w_t = o_{p}(1)$ as in Theorem~\ref{res:3}.
  Then, as before,
  \begin{gather}
    \WesA \to^{p} 0 \label{eq:11} \\
    \WesB \to^{p} 0 \label{eq:12}\\
  \intertext{and}
    \WesC \to^{p} 0 \label{eq:13}
  \end{gather}
  from Lemma~\ref{res:a4}.  The formula of the asymptotic variance can
  be derived exactly as in \citet{Wes:96} and \citet{WeM:98}. Since
  $f_t$ and $H_t$ both satisfy \clt s, this completes the
  proof.
\end{proof}

\bibliography{texextra/references}
\end{document}

% LocalWords:  ClW JEL ISI Google GiW Mcc ClM CoS CCS StW IMA GiR WeM fh X'X hh
% LocalWords:  PaT AllRefs isi ima uc sv PeT GoW lm il GoK RoW Econometrica PoR
% LocalWords:  Finan StepM studentizing studentization Whi HuW RSW recentering
% LocalWords:  DiM LiS Kun McCracken lt filtrations GoJR JoD McCracken's Econom
% LocalWords:  Corradi unstudentized studentized fg gg GoJ gcalhoun HLN li PoW
% LocalWords:  Econometricians reestimate PPW resample miscentered AnG eq Helle
% LocalWords:  Bunzel Yu Hsu Pincheira HHK th AtO ik DoH Wolak's Wol stepdown
% LocalWords:  Hsu's iq mk Ames Amit Goyal rfs Ivo Welch Wes covariance MeR McW
% LocalWords:  familywise prespecified CoD Gia pointwise misspecified InK MeP
% LocalWords:  VeR xtable Dah dbframe oos parametrizations iid dgp return's CaT
% LocalWords:  outperformance Hmisc Har nondifferentiable bT jt Meng
% LocalWords:  stationarity mixingale mixingales texextra Timmermann