\documentclass[12pt,fleqn]{article}
\input{tex/setup}
\input{VERSION}
\begin{document}

\author{Gray Calhoun\thanks{ Economics Department; Iowa State
    University; Ames, IA 50011.  Telephone: (515) 294-6271.  Email:
    \guillemotleft\protect\url{gcalhoun@iastate.edu}\guillemotright.  Web:
    \guillemotleft\protect\url{http://gray.clhn.co}\guillemotright.  I'd like to
    thank Helle Bunzel, Todd Clark, and Michael McCracken
    for helpful comments and discussions.  I'd also like to thank Amit
    Goyal for providing computer code and data for his 2008
    RFS paper with Ivo
    Welch \citep{GoW:08}.} \\
  Iowa State University}

\title{A simple block bootstrap for asymptotically normal
  out-of-sample test statistics}

\maketitle

\begin{abstract} \noindent%
  This paper proposes an improved block bootstrap method for
  out-of-sample statistics. Previous block bootstrap methods for these
  statistics have centered the bootstrap out-of-sample average on the
  observed out-of-sample average, which can cause the distribution to
  be miscentered under the null --- these papers have used either a
  short out-of-sample period or an adjustment to the model parameter
  estimators under the bootstrap to correct this centering
  problem. Our approach centers the bootstrap replications correctly
  under the null while continuing to use the standard formulas to
  estimate the model parameters under the bootstrap, while allowing
  the out-of-sample period to remain large. The resulting approach is
  computationally more efficient and easier to program.

\strut

\noindent Keywords: Forecast Evaluation, Martingale Difference
Sequence, Model Selection, Family-Wise Error Rate; Multiple Testing;
Bootstrap; Reality Check

\strut

\noindent JEL Classification Numbers: C22, C53

\end{abstract}

\newpage

\section{Introduction}

This paper develops a block bootstrap method that can be used to
consistently estimate the distributions of asymptotically normal
out-of-sample (\oos) test statistics. Since the asymptotic variance of
these statistics can have a complicated formula in many applications,
bootstrap methods have the potential to be very useful in applied
work. However, existing bootstraps are either based on parametric
models of the null hypothesis (typically using sieve arguments, so
that the effect of imposing a functional form dies out in large
samples; see [fill in]), only hold with strict restrictions on the
size of the test sample \citep{Whi:00,Han:05}, or require complicated
estimation strategies in the bootstrap \citep{CoS:07}, making this
approach impractical. In this paper, we show that standard
nonparametric block bootstraps are also consistent and derive the
correct centering term to ensure consistency.

The next section presents our block bootstrap result.
Section~\ref{sec:4} concludes.

\section{The Bootstrap for Out-of-Sample Statistics}

We'll develop our theoretical results in a fairly general
framework. Let $y_{t+1}$ be the target variable of interest --- the
variable that is being predicted --- and let $x_t$ be a vector of
other variables that are potentially informative about $y_{t+1}$ ---
these are our predictors. The forecast $\hat y_{t+1}$ depends on the
variables $x_t$ and an estimated parameter $\bh_t$. In the research
project that we're trying to model, we're interested in a function of
these variables and parameters, and the \oos\ average of that function
is our test statistic.

In symbols, we're interested in statistics of the form
\begin{equation*}
  \fb = \oclt{t} f(y_{t+1}, x_t, \bh_t).
\end{equation*}
To make the notation cleaner, we'll define $f_t(\beta) \equiv f(y_{t+1},
x_t, \beta)$. We're also going to assume that $(y_{t+1}, x_t)$ is
strictly stationary to simplify our presentation. One could derive the
same results under the marginally weaker assumption that certain
functions of these variables are weakly stationary.

The coefficients are updated each period to mimic a true \oos\
forecasting exercise. In this paper, we're going to assume that
$\bh_t$ is an $M$-estimator. We expect that other classes of
estimators can be used as well, as long as they are amenable to the
bootstrap.

Using standard terminology, the estimator $\bh_t$ is defined as
\begin{equation}\label{eq:9}
  \hat\beta_t = \begin{cases}
    \argmin_\beta \sum_{s=1}^{t-1} q(y_{s+1}, x_s, \beta) & \text{recursive window} \\
    \argmin_\beta \sum_{s=t-R+1}^{t-1} q(y_{s+1}, x_s, \beta) & \text{rolling window} \\
    \argmin_\beta \sum_{s=1}^{R-1} q(y_{s+1}, x_s, \beta) & \text{fixed window},
  \end{cases}
\end{equation}
and, as before, to make the notation cleaner, define $q_s(\beta)
\equiv q(y_{s+1},x_s,\beta)$. Obviously, for this to be a reasonable
estimation approach $q_s$ will need to satisfy standard assumptions
that we'll discuss soon.

So far, this is is a very standard setup. Our novelty is in how we
propose bootstrapping $\fb$. We're going to use a block bootstrap
here. For the moment, consider the circular block bootstrap, which is
implemented by drawing blocks of $\ell_1$ consecutive observations
from the original dataset. For the circular block bootstrap, if the
block extends beyond the dataset it is ``wrapped around,'' so, for
example, $(T-1, T, 1, 2, 3, 4)$ is a possible block of indices when
the block length is 6.

We know from West's (1996) paper that, under appropriate assumptions,
$\sqrt{P} (\fb - \E f_t(\btrue))$ is asymptotically normal with mean
zero. The key insight in our paper is that we can match this result in
the bootstrap, but we need to be careful about the exact centering
term.  In particular, under the circular bootstrap, $\btrue^*$ is the
equivalent of $\btrue$ and the sample average is the equivalent of the
population mean. That means that we should expect
\begin{equation*}
  \sqrt{P} (\fb^* - \E^* f_t^*(\btrue^*))
\end{equation*}
to have the same asymptotic distribution and to give reliable
confidence intervals, etc. where, for the circular and stationary
block bootstraps,
\begin{equation*}
  \E^* f_t^*(\beta) = \tfrac{1}{T-1} \sum_{t=1}^{T-1} f_t(\beta)
\end{equation*}
and
\begin{equation*}
  \btrue^* = \argmin_\beta \sum_{s=1}^{T-1} q_s(y_{s+1}, x_s, \beta).
\end{equation*}
For the moving blocks bootstrap, a slight correction is necessary
since observations at the ends of the sample are less likely to be
selected, but the same equations hold approximately. And, after
spelling out our specific assumptions, that's what we'll show!

But, just to spell it out first, here
\begin{equation}
  \fb^* = \oavg{t} f(y_{t+1}^*, x_t^*, \bh_t^*) \equiv \oavg{t} f_t^*(\bh_t^*)
\end{equation}
where $\bh_t^*$ is estimated exactly the same way as $\bh_t$:
\begin{equation}\label{eq:10}
  \hat\beta_t^* = \begin{cases}
    \argmin_\beta \sum_{s=1}^t q_{s}^*(\beta) & \text{recursive window} \\
    \argmin_\beta \sum_{s=t-R+1}^t q_{s}^*(\beta) & \text{rolling window} \\
    \argmin_\beta \sum_{s=1}^R q_{s}^*(\beta) & \text{fixed window}.
  \end{cases}
\end{equation}

In general, let $\to^{p^{*}}$ and
$\to^{d^{*}}$ refer to convergence in probability or distribution
conditional on the observed data.  Similarly, $\E^{*}$, $\var^{*}$,
and $\cov^{*}$ refer to the expectation, variance, and covariance with
respect to the probability measure induced by the bootstrap, and
$y_t^{*}$, etc. is the random variable $y_t$ but under the
bootstrap-induced \cdf.

\begin{asmp}\label{a1}
  The estimators $\bh_t$ and $\bh_t^*$ are estimated as defined in
  Equations~\eqref{eq:9} and~\eqref{eq:10}. Moreover $\btrue =
  \argmin_\beta \E\, q_s(\beta)$ is unique.
\end{asmp}

\begin{asmp}\label{a3}
  The stochastic process $(y_{t+1}, x_t)$ is strictly stationary and
  strong-mixing of size $-r/(r-2)$ or uniform mixing of size
  $-r/(2r-2)$ with $r>2$.
\end{asmp}

\begin{asmp}\label{a2}
  Both $f_t$ and $q_t$ are twice continuously differentiable in an
  open neighborhood $N$ of $\btrue$ and $\E \nabla^2 q_t(\beta)$ is
  positive definite uniformly in $N$. Also there exists a sequence of
  random variables $m_t$ such that $\sup_{i,\beta \in N} |\nabla_i^2
  q_t(\beta)| \leq m_t$, $\sup_{i,\beta \in N} |\nabla_i^2
  f_t(\beta)| \leq m_t$, $\sup_{i,\beta \in N} |\nabla_i q_t(\beta)|
  \leq m_t$, and $\sup_{i,\beta \in N} |\nabla_i f_t(\beta)|
  \leq m_t$ almost surely and $\E m_t$ is uniformly finite.
\end{asmp}

\begin{asmp}\label{a4}
  The bootstrap sequence $(y_2^*, x_1^*),\dots,(y_T^*, x_{T-1}^*)$ is
  constructed using a moving blocks, circular blocks, or stationary
  bootstrap with block lengths drawn from the geometric distribution.
  The (expected) block length $\ell_1$ satisfies $\ell_1 \to \infty$
  and $\ell_1/P \to 0$.
\end{asmp}

\begin{asmp}\label{a6}
  The asymptotic variance matrix of $\bar{f}(\btrue)$ is uniformly
  positive definite.
\end{asmp}

\begin{asmp}\label{a7}
  $R, P \to \infty$ as $T \to \infty$.
\end{asmp}

Then the main result is proving consistency of the bootstrap
distribution and the bootstrap variance.

\begin{thm}\label{res:3}
  Under Assumptions~\ref{a1} -- \ref{a7},
  \begin{equation}\label{eq:14}
    \var(\fb)/\var^*(\fb^*) \to^p 1
  \end{equation}
  and
  \begin{equation}\label{eq:15}
    \pr\big[\sup_x \big\lvert \pr^*[\sqrt{P} (\fb^* - \E^* f_t^*) \leq x]
    - \pr[\sqrt{P}( \bar{f} - \E f_t) \leq x] \big\rvert > \epsilon\big] \to 0
  \end{equation}
  for all $\epsilon > 0$.
\end{thm}

\begin{rem}
  \citet{Mcc:00} proves asymptotic normality under weaker smoothness
  conditions on $f_t(\beta)$ and $h_t(\beta)$: only their expectations
  must be continuously differentiable.  It may be possible to extend
  Theorem~\ref{res:3} to those weaker conditions, but the current
  proof relies on a theorem of \citepos{JoD:00} establishing the
  consistency of \hac\ estimators and their theorem requires differentiability of
  the observations.  Extending \citepos{JoD:00} result to
  nondifferentiable functions should be possible, but is beyond the
  scope of this paper.
\end{rem}

\begin{rem}
  \citet{Whi:00} and \citet{Han:05} resample the forecasts but do not
  reestimate any of them which requires the additional assumption that
  $\tfrac{P}{R} \log \log R \to 0$ or that the forecasts themselves
  have no estimated parameters.%
\footnote{\citet{Whi:00} lists several
    different sets of assumptions that give the same result, but these
    seem to be the most general.} %
\end{rem}

\begin{rem}
  \citet{CoS:07} use the distribution of $\sqrt{P}(\bar{f}^{*} -
  \bar{f})$ to approximate that of $\sqrt{P}(\bar{f} - \E
  \bar{f}(\btrue))$.  But it is clear that $\bar{f}(\btrue^*)$
  is the bootstrap analogue of $\E \bar{f}(\btrue)$, the parameter of
  interest.  Because their bootstrap is miscentered, \citet{CoS:07}
  must redefine $\hat{\beta}_t^{*}$ to achieve consistency.  In this
  paper, though, consistency arises naturally.
\end{rem}

\begin{rem}
  It may be unnecessary to assume that $\bar{f}(\btrue)$ has positive
  definite asymptotic variance; if so, the bootstrap would work in the
  setup of \citet{ClM:05,ClM:01} and \citet{Mcc:07}.  That question is
  left to future research.
\end{rem}

\section{Conclusion}\label{sec:4}
The paper improves block bootstrap procedures of \oos\
statistics.

\appendix
\section*{Appendix: Additional mathematical results}\label{sec:B}
\renewcommand{\thesubsection}{\Alph{subsection}}

To make the mathematical results in this appendix clearer, we will
introduce the following additional notation:
\begin{itemize}
\item $F_t(\beta) = \nabla f_t(\beta)$ and $F_t^*(\beta) = \nabla f_t^*(\beta)$,
\item $F = \E F_t$ and $F^* = \E^* F_t^*$,
\item $h_t(\beta) = \nabla q_t(\beta)$ and $h_t^*(\beta) = \nabla q_t^*(\beta)$,
\item $h_t = h_t(\btrue)$ and $h_t^* = h_t^*(\btrue^*)$.
\end{itemize}

Moreover, we will also prove our results under the simplifying
assumption that there is a single sequence of $M$-estimators
$\bh_t$. Since we are assuming non-degeneracy of the models, this
assumption does not appreciably change our arguments.

Also define $u_1,\dots,u_J$ to be the first period of each
block of the circular bootstrap, and, for each $j = 1,\dots,J$,
define the $\sigma$-fields
\[
\Ms_j = \sigma(u_1,\dots,u_j)
\]
and
\[
\Ms_j^* = \sigma(u_1,\dots,u_j; y_1,\dots,y_T; x_1,\dots,x_T).
\]
Also let $l = T - J \ell$ be the number of elements in the last block.

\newcommand{\WesA}[1][]{\oclt{t}
  (F_t^{#1} - \E^{#1} F_t^{#1}) B^{#1} H_t^{#1}}
\newcommand{\WesB}[1][]{\tfrac{1}{\sqrt{P}} \E^{#1} F_t^{#1} \osum{t} (B_t^{#1} -
  B^{#1}) H_t^{#1}}
\newcommand{\WesC}[1][]{\oclt{t}
  (F_t^{#1} - \E^{#1} F_t^{#1}) (B_t^{#1} - B^{#1}) H_t^{#1}}

\subsection*{Proof of Theorem~\ref{res:3}}
  As in much of this literature, we begin by expanding
  $f_t^{*}(\bh_t^*)$ around $\btrue^*$ to get
  \begin{align*}
    \sqrt{P} (\fb^* - \E^* f_t^*)
    &= \oclt{t} (f_t^{*} - \E^* f_t^*)
     + \oclt{t} F_t^* \cdot (\bh_t^* - \btrue^*)
     + \oclt{t} w_t^* \\
    &= \oclt{t} (f_t^{*} - \E^* f_t^*)
     + \E^*(F^*) B^* \tfrac{1}{\sqrt{P}} \sum_{t=1}^{T-1} a_t h_t^* + o_{p^*}(1)
  \end{align*}
  where (similar to \citealp{Wes:96})
  \begin{equation*}
    w_{t} = \tfrac12 (\bh_t^* - \btrue^*)' \nabla^2 f_{t}^*(b_{t}^*) (\bh_t^* - \btrue^*)
  \end{equation*}
  and each $b_{t}^*$ lies between $\bh_t^*$ and $\btrue^*$. The second equality holds because
  $\oclt{t} w_t^* = o_{p^*}(1)$ and
  \begin{equation*}
    \oclt{t} F_t^* \cdot (\bh_t^* - \btrue^*)
    = \E^*(F^*) B^* \tfrac{1}{\sqrt{P}} \sum_{t=1}^{T-1} a_t h_t^* + o_{p^*}(1)
  \end{equation*}
  from Lemma~\ref{res:a4}. By Lemma~?,
  \begin{equation}\label{eq:19}
    \oclt{t} \begin{pmatrix}
      f_t^* - \E^* f_t^* \\ a_t h_t^*
    \end{pmatrix} \to^d N(0, \Sigma^*)
  \end{equation}
  and, by Lemma~?, $\E^* F^* \to^p \E F$, $\Sigma^* \to^p \Sigma$, and
  $B^* \to^p B$.  Since the original \oos\ statistic converges in
  distribution to $N(0, V)$ as well, the proof follows from standard
  arguments. (See \citealp{Cal:14}, for an example of the details.)\qed

\subsection{Supporting Results}

\begin{lema}\label{res:a2}
  Suppose $a \in [0,1/2)$ and the conditions of Theorem~\ref{res:3}
  hold.
  \begin{enumerate}
  \item $P^a \sup_t | H_{t} | \to^p 0$ and $P^a \sup_t | H_{t}^{*} |
    \to^{p^{*}} 0$.
  \item $P^a \sup_t | \hat{\beta}_{t} - \beta_{0} | \to^{p} 0$ and
    $P^a \sup_t | \hat{\beta}^{*}_{t} - \hat{\beta}_T |
    \to^{p^{*}} 0$.
  \item $\tfrac{1}{\sqrt{P}} \sum_{t=R+1}^T (F_t - \E F_t) = O_{p}(1)$
    and $\tfrac{1}{\sqrt{P}} \sum_{t=R+1}^T (F_t^* - \E F_t^*) =
    O_{p^*}(1)$.
  \end{enumerate}
\end{lema}

\begin{proof}[Proof of Lemma~\ref{res:a2}]
  \begin{enumerate}
  \item Convergence for the original process is essentially the same
    as the proof in \citet{Wes:96}, with minor tweaks as in
    \citet{Cal:15}; we will repeat it here for completeness. Our
    assumptions ensure that $h_t$ is an $L_2$-mixingale of size $-1/2$;
    let $c_t$ and $\zeta_k$ denote its mixingale constants and
    coefficients and note that $h_t/t$ is also an $L_2$-mixingale array
    with constants $c_s/t$ and coefficients $\zeta_k$, since
    \begin{align*}
      \| \E_{t-k} h_t / t \| = t^{-1} \| \E_{t-k} h_t \| \leq (c_t/t)\, \zeta_k
    \end{align*}
    and
    \begin{equation*}
      \| h_t/t - \E_{t+k} h_t/t \| = t^{-1} \|  h_t - \E_{t+k} h_t \| \leq (c_t/t)\, \zeta_{k+1}.
    \end{equation*}

    Then
    \begin{align*}
      P^{2a} \E\Bigg[\omax{t} \Big| \sum_{s=1}^t h_s/t \Big|^2\Bigg]
      &\leq P^{2a} \E\Bigg[\omax{t} \Big| \sum_{s=1}^t h_s/s \Big|^2\Bigg] \\
      &= O(P^{2a}) \sum_{s=1}^{T-1} s^{-2}
    \end{align*}
    where the second line follows from \citepos{Mcl:75} maximal
    inequality (also available as \citealp{Dav:94}, Theorem 16.9 and
    Corollary 16.10). This term converges to zero from \citepos{Wes:96} Lemma A1.

    For the bootstrap process, we will use an argument similar to
    those in \cite{Cal:14} and make liberal use of the
    LIE. Define $\Hs_i^* = \sum_{t=K_{i-1}+1}^{K_i}
    h_t^*/t$, so
    \begin{align*}
      \omax{t} | H_t^* |
      &\leq \omax{t} \Big| \sum_{i=1}^{j^*_t - 1}\Hs_i^* \Big| + \omax{t} \Big|\sum_{s=K_{j^*_t - 1}+1}^t h_s^*/t \Big|
    \end{align*}
    Now observe that $\{\sum_{i=1}^j \Hs_i^*; \Ms_j^*\}$ is a martingale, so
    \begin{equation*}
      \pr^*\Bigg[ \omax{t} \Big| \sum_{i=1}^{j^*_t - 1}\Hs_i^* \Big| > \epsilon \Bigg]
      \leq \epsilon^{-2} \E^* \sum_{i=1}^J \E^*(\Hs_i^{*2} \mid \Ms_{i-1}^*).
    \end{equation*}

    We can deal with this last term as follows. The expectation
    $\E^*(\Hs_i^{*2} \mid \Ms_{i-1}^*; \ell_i)$ only averages over the
    start of the block used by $\Hs_i^*$, so
    \begin{align*}
      \E^*(\Hs_i^{*2} \mid \Ms_{i-1}^*; \ell_i)
      &= \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \Bigg[\sum_{t = 1}^{\ell_{i}} \tilde{h}_{u + t}(\btrue^*) / (K_{i-1} + t + 1)\Bigg]^2 \\
      &= \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \Bigg[\sum_{t = 0}^{\ell_{i}-1}
      (\tilde{h}_{u + t}(\btrue) + \tilde{h}_{u + t}(\btrue^*) - \tilde{h}_{u + t}(\btrue))/ (K_{i-1} + t + 1)\Bigg]^2
    \end{align*}
    and we want to show that
    \begin{equation}\label{eq:4}
      P^a \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \sum_{i=1}^J \Bigg[\sum_{t = 1}^{\ell_{i}}
      \tilde{h}_{u + t}(\btrue)/ (K_{i-1} + t)\Bigg]^2 \to^p 0
    \end{equation}
    and
    \begin{equation}\label{eq:5}
      P^a \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \Bigg[\sum_{t = 1}^{\ell_{i}}
      (\tilde{h}_{u + t}(\btrue^*) - \tilde{h}_{u + t}(\btrue))/ (K_{i-1} + t)\Bigg]^2
      \to^p 0.
    \end{equation}

    For~\eqref{eq:4}, the $L_2$-mixingale property again ensures that
    \begin{align*}
      \E\Bigg|P^a \tfrac{1}{T-1} &\sum_{u = 0}^{T-2} \sum_{i=1}^J \Bigg[\sum_{t = 1}^{\ell_{i}}
      \tilde{h}_{u + t}(\btrue)/ (K_{i-1} + t)\Bigg]^2 \Bigg| \\
      &= P^a \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \E \sum_{i=1}^J \E\Bigg(\Bigg[\sum_{t = 1}^{\ell_{i}}
      \tilde{h}_{u + t}(\btrue)/ (K_{i-1} + t)\Bigg]^2 \,\Big|\, \Ms_{i-1}, \ell_i\Bigg) \\
      &\leq P^a \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \E \sum_{i=1}^J C \sum_{t = 1}^{\ell_{i}} (K_{i-1} + t)^{-2} \\
      &\leq P^a \sum_{t=1}^{T-1} t^{-2}
    \end{align*}
    (should be $2a$ in all of the above)
    which converges to zero as before. For~\eqref{eq:5},
    \begin{align*}
      \tfrac{1}{T-1} &\sum_{u = 0}^{T-2} \Bigg[\sum_{t = 1}^{\ell_{i}}
      (\tilde{h}_{u + t}(\btrue^*) - \tilde{h}_{u + t}(\btrue))/ (K_{i-1} + t)\Bigg]^2 \\
      &= \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \Bigg[\sum_{t = 1}^{\ell_{i}}
      \nabla \tilde{h}_{u + t}(\tilde \beta_T)'(\btrue^* - \btrue)/ (K_{i-1} + t)\Bigg]^2 \\
      &= O_p(1/T) \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \Bigg[\sum_{t = 1}^{\ell_{i}}
      \lVert \nabla \tilde{h}_{u + t}(\tilde \beta_T) \rVert / (K_{i-1} + t)\Bigg]^2
    \end{align*}
    and
    \begin{align*}
      \E \Bigg| (P^{2a}/T) &\tfrac{1}{T-1} \sum_{u = 0}^{T-2} \sum_{i=1}^J \Bigg[\sum_{t =
        1}^{\ell_{i}} \lVert \nabla \tilde{h}_{u + t}(\tilde \beta_T)
      \rVert / (K_{i-1} + t)\Bigg]^2 \Bigg| \\
      &= (P^{2a}/T) \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \E \sum_{i=1}^J
      \E \Bigg(\Bigg[\sum_{t =
        1}^{\ell_{i}} \lVert \nabla \tilde{h}_{u + t}(\tilde \beta_T)
      \rVert / (K_{i-1} + t)\Bigg]^2 \,\Bigg|\, \Ms_{i-1}; \ell_i \Bigg) \\
      &\leq (P^{2a}/T) \E \sum_{i=1}^J \ell_i^2/K_{i-1}^2
    \end{align*}
    which converges to zero by Lemma~[add!].

    Since
    \[
    \omax{t} \Big|\sum_{s=K_{j^*_t - 1}+1}^t h_s^*/t \Big| \leq \sum_{i=1}^{J} \max_{t = K_{i-1}+1,\dots,K_i} \Big| \sum_{s=K_{i-1}+1}^t h_s^*/t \Big|
    \]
    the proof that the second component converges to zero is similar.
  \item We have
    \begin{multline}
      P^a \sup_t | \hat{\beta}_t - \btrue | = P^a \sup_t |\hat{B}_{t}
      H_{t}| \\ \leq \sup_{t,u} \Big| [ \hat{B}_u - B]
      \tfrac{P^a}{t} \sum_{s=1}^t h_{s} \Big| + \sup_{t} \Big|
      B \tfrac{P^a}{t} \sum_{s=1}^t h_{s} \Big|
    \end{multline}
    and both terms converge to zero in (conditional) probability by
    the previous argument and by assumption.  The same argument holds
    for $\hat{\beta}_t^{*} - \btrue^*$ as well.
  \item $\tfrac{1}{\sqrt{P}} \sum_{t=R+1}^T (F_t - \E F_t)$ obeys
    \citepos{Jon:97} \clt, so the result is trivial. For the
    bootstrapped process, we know that the bootstrap variance is
    equivalent to (...HAC estimator...) which is consistent by
    \citepos{JoD:00} Theorem (2?).\qedhere
  \end{enumerate}
\end{proof}

\begin{lema}\label{res:a4}
  Under the conditions of Theorem~\ref{res:3},
  \begin{equation}\label{eq:16}
    \oclt{t} (\bh_t^* - \btrue^*)' \nabla^2 f_{it}^*(b_{it}^*) (\bh_t^* - \btrue^*) \to^{p^*} 0
  \end{equation}
  and
  \begin{equation}\label{eq:17}
    \oclt{t} F_t^* \cdot (\bh_t^* - \btrue^*)
    = F^* B^* \tfrac{1}{\sqrt{P}} \sum_{t=1}^{T-1} a_t h_t^* + o_{p^*}(1)
  \end{equation}
Equations
  \eqref{eq:7}--\eqref{eq:10} and \eqref{eq:11}--\eqref{eq:13} hold.
\end{lema}

\begin{proof}[Proof of Lemma~\ref{res:a4}]
For~\eqref{eq:16}, we have
\begin{align*}
  \pr\Big[& \big|\oclt{t} (\bh_t^* - \btrue^*)' \nabla^2 f_{it}^*(b_{it}^*) (\bh_t^* - \btrue^*) \big| > \epsilon \Big] \\
  &\leq \pr\Big[1\{\btrue^* \in N, \bh_t^* \in N\ \text{for all}\ t\} \big|\oclt{t} (\bh_t^* - \btrue^*)' \nabla^2 f_{it}^*(b_{it}^*) (\bh_t^* - \btrue^*) \big| > \epsilon \Big] \\
  &\quad + \pr[\btrue^* \notin N] + \pr[\bh_t^* \notin N\ \text{for some}\ t = R,\dots,T-1\}
\end{align*}
The second two probabilities on the rhs converge to zero by
Lemma~\ref{res:a2} and the random variable inside the first probability is bounded by
\begin{multline*}
  1\{\btrue^* \in N, \bh_t^* \in N\ \text{for all}\ t\}
  \oclt{t} (\bh_t^* - \btrue^*)' \nabla^2 f_{it}^*(b_{it}^*) (\bh_t^* - \btrue^*)
  \\ \leq
  \big(\sup_{t=R,\dots,T-1} |P^{1/4}(\bh_t^* - \btrue^*)|^2\big) \oavg{t}  \nabla^2 f_{it}^*(b_{it}^*) 1\{\btrue^* \in N, \bh_t^* \in N\}.
\end{multline*}
The summation is $O_p(1)$ by assumption and the supremum converges to
zero by using Lemma~\ref{res:a2} again.

For~\eqref{eq:17}, we have the upper bound
\begin{multline*}
  \big\lvert \oclt{t} \big(F_t^* \cdot (\bh_t^* - \btrue^*) - F^* B^* a_t h_t^*\big) \big\rvert \leq \\
   \sup_{s=R,\dots,T-1} |\bh_s^* - \btrue^*|\; \big\lvert \oclt{t} (F_t^* - F^*) \big\rvert
  + \big\lvert F^* \oclt{t} \big((\bh_t^* - \btrue^*) - B^* a_t h_t^*\big) \big\rvert.
\end{multline*}
The first term converges in conditional probability to zero by
Lemma~\ref{res:a2}.  For the second, expand each $\sum_{s=1}^t \nabla
q_s^*(\bh_t^*)$ around $\btrue^*$ to get
\begin{equation*}
  \oclt{t} (\bh_t^* - \btrue^*)
  = \oclt{t} \Big[\tfrac{1}{t} \sum_{s=1}^t \nabla^2 q_s^*(b_t^*)\Big]^{-1} \tfrac{1}{t} \sum_{s=1}^t h_s^*
\end{equation*}
where $b_t^*$ is between $\bh_t^*$ and $\btrue^*$. Then
\begin{align*}
  \big\lvert \oclt{t} \big((\bh_t^* - \btrue^*) - B^* a_t h_t^*\big) \big\rvert
  &\leq \sup_{t=R,\dots,T-1} \Big\lvert\Big[\tfrac{1}{t} \sum_{s=1}^t \nabla^2 q_s^*(b_t)\Big]^{-1} - B^*\Big\rvert
  \Big\lvert\oclt{t} \tfrac{1}{t} \sum_{s=1}^t h_s^* \Big\rvert \\
  &= O_{p^*}(1) \sup_{t=R,\dots,T-1} \Big\lvert\Big[\tfrac{1}{t} \sum_{s=1}^t \nabla^2 q_s^*(b_t)\Big]^{-1} - B^*\Big\rvert
\end{align*}
by Lemma~\ref{res:a2} and it suffices to prove that the supremeum
converges to zero in probability.

First, observe that for any $\Delta$
\begin{align}\label{eq:18}
  \pr^*\Big[&\sup_{t = R,\dots,T-1} \big\lvert \tfrac{1}{t} \sum_{s=1}^t \nabla^2 q_s^*(b_t^*) - B^{*-1} \big\rvert > \Delta \Big] \\
  &\leq \pr^*\Big[\sup_{t=R,\dots,T-1} \big\lvert \tfrac{1}{t} \sum_{s=1}^t \nabla^2 q_s^* - B^{*-1} \big\rvert 1\{\beta_0^* \in N\}> \Delta \Big]\\
  &\quad + \pr\Big[\sup_{t=R,\dots,T-1} \big\lvert \tfrac{1}{t} \sum_{s=1}^t (\nabla^2 q_s^*(b_t^*) - \nabla^2 q_s^*) \big\rvert 1\{\beta_0^* \in N,\ \bh_t^* \in N\} > \Delta \Big] \\
  &\quad + \pr[\beta_0^* \notin N] + \pr[\bh_t^* \notin N \text{ for some } t = R,\dots,T-1]
\end{align}
As before, the last two probabilities converges to zero by
Lemma~\ref{res:a2}.  Moreover, just as in the proof of
Theorem~\ref{res:3}, $(1/t) \sum_{s=1}^t \nabla^2 q_s(\btrue^*)$ can
be reexpressed as the sum of a uniformly integrable \mds\ that obeys
a uniform \lln, so the first probability on the rhs of~\eqref{eq:18}
converges to zero. Finally, since $\nabla^2 q_s$ is continuous
uniformly in $N$, we can choose $\delta$ so that $|\beta_1 - \beta_2|
< \delta$ implies that $|\nabla^2 q_s(\beta_1) - \nabla^2
q_s(\beta_2)| < \Delta$. Then
\begin{multline*}
  \pr\Big[\sup_{t=R,\dots,T-1} \big\lvert \tfrac{1}{t} \sum_{s=1}^t (\nabla^2 q_s^*(b_t^*) - \nabla^2 q_s^*) \big\rvert 1\{\beta_0^* \in N,\ \bh_t^* \in N\} > \Delta \Big]
  \leq \\
\pr[\sup_{t=R,\dots,T-1}  \lvert b_t^* - \btrue^* \rvert > \delta\text{ and } \btrue^* \in N,\text{ and } \bh_t^* \in N \text{ for all } t = R,\dots,T-1]
\end{multline*}
which again converges to zero by Lemma~\ref{res:a2}.

Now choose $\Delta$ so that
\begin{equation*}
  \tfrac{1}{t} \sum_{s=1}^t \nabla^2 q_s^*(b_t) - B^{*-1} < \Delta
\end{equation*}
implies that
\begin{equation*}
\Big[\tfrac{1}{t} \sum_{s=1}^t \nabla^2 q_s^*(b_t)\Big]^{-1} - B^* < \epsilon.
\end{equation*}
Then
\begin{multline*}
  \pr^*\Big[\sup_{t=R,\dots,T-1} \Big\lvert\Big[\tfrac{1}{t} \sum_{s=1}^t \nabla^2 q_s^*(b_t)\Big]^{-1} - B^*\Big\rvert
  > \epsilon\Big] \leq \\
  \pr^*\Big[\sup_{t=R,\dots,T-1}\big\lvert\tfrac{1}{t} \sum_{s=1}^t \nabla^2 q_s^*(b_t) - B^{*-1}\big\rvert > \Delta\Big]
  \to^{p^*} 0,
\end{multline*}
completing the proof.
\end{proof}

\begin{lema}
  Under the conditions of Theorem~\ref{res:3}, \eqref{eq:19} holds.
\end{lema}
\begin{proof}

  We will use arguments very similar to \cite{Cal:14}. Define
  \[
  z_j^* = \tfrac{1}{\sigma^* \sqrt{P}} \sum_{s=(j-1) \ell + 1}^{j\ell}
  \big(\gamma_1'(f_s^* - \E^* f_s^*) + a_s \gamma_2' h_s^*\big)
  \]
  where $\gamma_1$ and $\gamma_2$ are arbitrary nonzero vectors.
  By construction, $\E^* h_t^* = 0$ almost surely, so $\E(z_j \mid
  \Ms_{j-1}^*) = 0$ almost surely and $\{z_j^*, \Ms_j^*\}$ is a
  martingale difference sequence.

  Lemma aaaa implies
  that $z_j^{*2}$ is uniformly integrable, in the sense that
  (clarify). From the \mds\ property, we have
  \begin{equation*}
    \sum_{j=1}^J z_j^* \to^d N(0, 1)
  \end{equation*}
  as long as the following properties hold:
  \begin{equation}\label{eq:7}
    \sum_{j=1}^J \E(z_j^{*2} 1\{z_j^{*2} > \epsilon\} \mid \Ms_{j-1}^*) \to^p 0
  \end{equation}
  and
  \begin{equation}\label{eq:8}
    \pr^*\Big[ \big| \sum_{j=1}^J z_j^{*2} - 1 \big| > \epsilon\Big] \to^p 0.
  \end{equation}

  For~\eqref{eq:8}, we have the usual bound
  \begin{equation*}
    \pr^*\Big[ \big|\sum_{j=1}^J z_j^{*2} - 1 \big| > \varepsilon\Big] \leq
    \pr^*\Big[ 1\{\btrue^* \in N\} \big| \sum_{j=1}^J z_j^{*2} - 1 \big| > \epsilon \Big]
     + \pr^*[ \btrue^* \notin N ]
  \end{equation*}
  and we can rewrite the summation in the first term as
  \begin{equation*}
    1\{\btrue^* \in N\} \Big( \sum_{j=1}^J z_j^{*2} - 1 \Big)
    =  \sum_{j=1}^J \big(z_j^{*2}1\{\btrue^* \in N\} - \E(z_j^{*2} 1\{\btrue^* \in N\} \mid \Ms_{j-1}^*) \big).
   \end{equation*}
   This term is the sum of a uniformly integrable martingale
   difference sequence by Lemma aaaa and satisfies the \lln\
   (i.e. Davidson's, 1994, Theorem 19.7), and so it converges in
   (conditional) probability to zero.
   The second term converges in probability to zero by consistency of $\btrue^*$ (prove).

   The convergence in probability in~\eqref{eq:7} is implied by
   convergence in $L_1$, i.e.
   \begin{equation*}
     \sum_{j=1}^J \E(z_j^{*2} 1\{z_j^{*2} > \epsilon\}) \to 0,
   \end{equation*}
   and we have, defining
   \begin{equation*}
     \zeta_{st}^* =
     (\gamma'\Sigma^*\gamma)^{-1/2} \gamma'\begin{pmatrix}
       f_t^* - \E^* f_t^* \\
       a_s h_t^*
     \end{pmatrix},
   \end{equation*}
   \begin{align*}
     \sum_{j=1}^J &\E(z_j^{*2} 1\{z_j^{*2} > \epsilon\}) \\
     &= \sum_{j=1}^J \E(z_j^{*2} 1\{z_j^{*2} > \epsilon\}) \\
     &= \sum_{j=1}^{J-1} \E\Big(\sum_{s,t =1}^{\ell_1} \zeta^*_{\ell_1 (j-1)+s,\ell_1 (j-1)+s}\zeta^*_{\ell_1 (j-1)+t,\ell_1 (j-1)+t}
     1\Big\{ \sum_{s,t = 1}^{\ell_1} \zeta^*_{\ell_1 (j-1)+s,\ell_1 (j-1)+s}\zeta^*_{\ell_1 (j-1)+t,\ell_1 (j-1)+t} > \epsilon\Big\}\Big) \\
     &\quad +
     \E\Big(\sum_{s,t = T-\ell_2+1}^{T} \zeta^*_{ss}\zeta^*_{tt} 1\Big\{\sum_{s,t = T - \ell_2+1}^{T} \zeta^*_{ss}\zeta^*_{tt} > \epsilon\Big\}\Big) \\
     &= \tfrac{1}{T} \sum_{u=1}^T \sum_{j=1}^{J-1} \E\Big(\sum_{s,t = 1}^{\ell_1} \zeta_{\ell_1(j-1) + s,u+s}\zeta_{\ell_1(j-1) + t,u+t}
     1\Big\{\sum_{s,t = 1}^{\ell_1} \zeta_{\ell_1(j-1) + s,u+s}\zeta_{\ell_1(j-1) + t,u+t} > \epsilon\Big\}\Big) \\
     &\quad +
     \tfrac{1}{T} \sum_{u=1}^T \E\Big(\sum_{s,t = 1}^{\ell_2} \zeta_{T-\ell_2 + s,u+s}\zeta_{T-\ell_2+t,u+t}
     1\Big\{\sum_{s,t = 1}^{\ell_2} \zeta_{T-\ell_2 + s,u+s}\zeta_{T-\ell_2+t,u+t} > \epsilon\Big\}\Big) \\
     &\leq \sum_{j=1}^{J-1} B(\epsilon T / \ell_1) \ell_1 / T
     + B(\epsilon T / \ell_2) \ell_2/T \\
     &\to 0.
   \end{align*}
   The rest of the proof is fairly straightforward.
\end{proof}

\begin{lema}\label{res:a3}
  Under the conditions of Theorem~\ref{res:3}, $\btrue^* \to^p
  \btrue$, $B^* \to^p B$, $F^* \to^p F$, and $\Sigma^* \to^p \Sigma$.
\end{lema}

\begin{proof}[Proof of Lemma~\ref{res:a3}]
  We'll present proofs of these results for the circular block
  bootstrap; the proofs for the moving blocks and stationary
  bootstraps are similar. For $\btrue^*$, by definition $\btrue^* =
  \argmin_\beta \sum_{s=2}^T q_s(\beta)$ and $\sum_{s=2}^T q_s(\beta)$
  obeys a uniform \lln\ and converges in probability to $\E
  q_s(\beta)$ for all $\beta \in \Theta$. Then consistency of
  $\btrue^*$ follows from, for example, Theorem~2.1 of \cite{NeM:94}.
  Moreover, $\Sigma^* \to^p \Sigma$ by Theorem 2.2 of
  \citet{JoD:00}.

  For $F^*$, we have
  \begin{equation*}
    \pr[|F^* - F| > \epsilon] \leq
    \pr[|F^* - F| 1\{\btrue^* \in N\} > \epsilon] + \pr[\btrue^* \notin N].
  \end{equation*}
  The second probability converges to zero by consistency of
  $\btrue^*$.  Now $F^* - F = F^* - F(\btrue^*) + F(\btrue^*) - F$,
  and $F^* - F(\btrue^*) \to^p 0$ by the uniform \lln. Now choose
  $\Delta$ so that $|\beta_1 - \beta_2| < \Delta$ implies that
  $|F(\beta_1) - F(\beta_2) | < \epsilon$. Then
  \begin{equation*}
     \pr[|F(\btrue^*) - F | > \epsilon] \leq \pr[|\btrue^* - \btrue| > \Delta]
  \end{equation*}
  which converges to zero by the first part of this Lemma. The proof
  for $B^*$ is similar.
\end{proof}

\bibliography{texextra/references}
\end{document}

% LocalWords:  ClW JEL ISI Google GiW Mcc ClM CoS CCS StW IMA GiR WeM fh X'X hh
% LocalWords:  PaT AllRefs isi ima uc sv PeT GoW lm il GoK RoW Econometrica PoR
% LocalWords:  Finan StepM studentizing studentization Whi HuW RSW recentering
% LocalWords:  DiM LiS Kun McCracken lt filtrations GoJR JoD McCracken's Econom
% LocalWords:  Corradi unstudentized studentized fg gg GoJ gcalhoun HLN li PoW
% LocalWords:  Econometricians reestimate PPW resample miscentered AnG eq Helle
% LocalWords:  Bunzel Yu Hsu Pincheira HHK th AtO ik DoH Wolak's Wol stepdown
% LocalWords:  Hsu's iq mk Ames Amit Goyal rfs Ivo Welch Wes covariance MeR McW
% LocalWords:  familywise prespecified CoD Gia pointwise misspecified InK MeP
% LocalWords:  VeR xtable Dah dbframe oos parametrizations iid dgp return's CaT
% LocalWords:  outperformance Hmisc Har nondifferentiable bT jt Meng
% LocalWords:  stationarity mixingale mixingales texextra Timmermann