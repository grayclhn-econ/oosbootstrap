\documentclass[12pt,fleqn]{article}
\input{tex/setup}
\input{VERSION}
\begin{document}

\author{Gray Calhoun\thanks{ Economics Department; Iowa State
    University; Ames, IA 50011.  Telephone: (515) 294-6271.  Email:
    \guillemotleft\protect\url{gcalhoun@iastate.edu}\guillemotright.  Web:
    \guillemotleft\protect\url{http://gray.clhn.co}\guillemotright.  I'd like to
    thank Helle Bunzel, Todd Clark, and Michael McCracken
    for helpful comments and discussions.  I'd also like to thank Amit
    Goyal for providing computer code and data for his 2008
    RFS paper with Ivo
    Welch \citep{GoW:08}.} \\
  Iowa State University}

\title{A block bootstrap for asymptotically normal out-of-sample
  test statistics}

\maketitle

\begin{abstract}
  This paper proposes an improved block bootstrap method for
  out-of-sample statistics.
\strut

\noindent Keywords: Forecast Evaluation, Martingale Difference
Sequence, Model Selection, Family-Wise Error Rate; Multiple Testing;
Bootstrap; Reality Check

\strut

\noindent JEL Classification Numbers: C22, C53

\end{abstract}

\newpage

\section{Introduction}

This paper develops a block bootstrap method to consistently estimate
out-of-sample (OOS) statistics' distributions.

\oos\ tests are commonly used in International Macroeconomics,
Macroeconomics, and Finance (see, for example, \citealt{MeR:83};
\citealt{StW:03}; and \citealt{GoW:08}) and there is a substantial
literature developing the theoretical properties of these statistics,
beginning primarily with \citet{DiM:95} and
\citet{Wes:96}.%
\footnote{Other papers in this literature include
  \citet{WeM:98}, \citet{Mcc:98,Mcc:00},
  \citet{ClM:01,ClM:05-2,ClM:05,ClM:11b,ClM:12,ClM:12b},
  \citet{CCS:01}, \citet{CoS:02,CoS:04,CoS:07}, \citet{Whi:00},
  \citet{InK:04,InK:06}, \citet{Han:05}, \citet{Ros:05},
  \citet{ClW:06,ClW:07}, \citet{Ana:07}, \citet{GiR:09,GiR:10},
  \citet{HuW:10}, \citet{HLN:11}, \cite{InR:11}, \cite{Pin:11},
  \cite{RoS:11,RoS:11b}, and \citet{Cal:11}, among others.  For recent
  reviews of this literature and additional references, see
  \citet{McW:02}, \citet{CoS:06}, \citet{Wes:06}, \citet{ClM:11c},
  \citet{CoD:11}, and \citet{Gia:11}} %
In a pair of papers,
\citet{ClW:06,ClW:07} develop an \oos\ test of the null hypothesis
that a small benchmark model is correctly specified.  Their test
compares the forecasting performance of a pair of nested models, and
the null hypothesis is that the innovations in the smaller model form
a martingale difference sequence.  This test procedure is popular, and
one assumes that this is due in part to the statistic's convenience,
the statistic is approximately normal after adjusting for the
estimation error of the larger model.  Normality comes from a
fixed-length rolling window, as in \citet{GiW:06}, and the adjustment
centers the statistic appropriately.  This statistic is especially
convenient because other \oos\ tests for similar hypotheses
(\citealt{CCS:01}; \citealt{ClM:01,ClM:05}; \citealt{CoS:02,CoS:04};
and \citealt{Mcc:07}; among others) have a nonstandard limit
distribution and place restrictions on the models under consideration,
while other asymptotically normal statistics test a different null
hypothesis \citep{GiW:06} or place assumptions on the models and \dgp\
that are often violated in empirical work (\citealt{DiM:95};
\citealt{Wes:96}; \citealt{WeM:98};
\citealt{Mcc:00}).%
\footnote{\citet{DiM:95} assume that the models are
  not estimated. \citet{Wes:96}, \citet{WeM:98}, and \citet{Mcc:00}
  implicitly assume that the models do not converge to the same limit,
  which rules out nesting.} %

However, Clark and West's statistic is only ``approximately normal''
in an informal sense.  Clark and West present Monte Carlo evidence of
the statistic's distribution, but only prove that the statistic is
asymptotically normal with mean zero when the benchmark model is a
random walk \citep{ClW:06}. Estimating the parameters of the smaller
model invalidates their proof.

In this paper, I show that a modified version of their statistic is
asymptotically normal even when the smaller model is estimated.  To
achieve normality, we need a consistent estimate of the pseudotrue
benchmark model, while maintaining an inconsistent estimate of the
larger model so that we can ignore nesting.  We can meet both needs by
using different window strategies for each model: the benchmark model
is estimated using a recursive window and the alternative with a
fixed-length rolling window.

Mixing window strategies is uncommon but needn't be. In most
applications, the null hypothesis imposes stability as well as equal
accuracy between the two models.  The benchmark model rarely allows
for breaks, parameter drift, or other forms of
instability,%
\footnote{Exceptions are \citepos{StW:07}
  IMA(1,1) and UC-SV models of inflation.} %
but the
researcher is typically concerned about instability.  Indeed, concern
about instability is often given as a reason for doing an \oos\
analysis, especially with a short rolling window.%
\footnote{This
  motivation is discussed by \citet{StW:03}, \citet{PeT:05,PeT:07},
  \cite{GiW:06}, \citet{GoW:08}, \citet{ClM:09c}, and
  \cite{GiR:09,GiR:10}, among others.} %
A researcher could impose
stability on both models by using a recursive window or relax
stability for both by using a rolling window; either approach should
not affect the test's size, but may affect power.  But the researcher
could instead impose stability on the benchmark and relax it for the
alternative by using a recursive window for the benchmark and a
rolling window for the alternative model.  This approach could have a
power advantage and is similar in spirit to using a Likelihood Ratio
Test instead of an LM or Wald test, which depend on just the
restricted or unrestricted model respectively.

This paper's statistic has a substantial advantage over existing \oos\
tests for nested models: the alternative can be essentially arbitrary
as long as high level moment conditions hold.  In particular,
researchers can use model selection techniques like the \aic\ or \bic\
to determine the number of lags to include, the particular exogenous
variables to include, etc.  Other methods that test a similar
hypothesis are unable to handle these models \citep[except][which does
not allow the benchmark to be estimated]{ClW:06}; \citet{GiW:06} are
able to handle such models for both the alternative and the benchmark
but, as mentioned earlier, they test a different aspect of forecasting
performance.

This paper focuses on nested models, as they have received the most
attention in the empirical and theoretical literature, but the
statistic can be used with non-nested models as well.  This generality
is useful, since \citepos{Wes:96} results do not apply to non-nested
models if they both encompass the true \dgp,%
\footnote{\citet{ClM:11b}
  call this scenario, ``overlapping models.''} %
which is allowable
under the null: in the limit, both models will converge to the \dgp\
and give identical forecasts.  Consequently, the naive \oos\ $t$-test
is invalid, even after correcting the standard error if necessary to
reflect parameter uncertainty.  \citet{ClM:11b} show that the fixed
window \oos\ $t$-test remains normal for these models but the
recursive and rolling windows (with the window size increasing to
$\infty$) do not, and provide a procedure for pointwise (but not
uniformly) valid tests for the recursive and rolling windows and
uniformly valid tests for the fixed window.  The test proposed in this
paper is uniformly valid and places fewer assumptions on the models
under study and the true \dgp.

Since researchers often have a set of potential models and want to
know which of them significantly outperform the benchmark, procedures
that compare a single pair of models are of limited practical value.
As \citet{Whi:00} demonstrates, looking at the naive $p$-values of
individual tests is misleading, but researchers can obtain a valid test
by using the bootstrap to approximate the distribution of the largest
individual test statistic under the null (White calls this procedure
the \textit{Bootstrap Reality Check} or \brc).  This paper presents a test
for equal predictive ability of multiple models based on
\citepos{RoW:05} StepM, which uses a step-down procedure that
iteratively rejects models to achieve higher power than the \brc\ and
indicate which of the models improves on the benchmark (see
Theorem~\ref{res:2} for details).

This paper also presents a new result for the validity of block
bootstraps with \oos\ statistics, which is necessary to verify that
the StepM is valid.  Existing results on bootstrapping \oos\
statistics have some drawbacks.  \citet{Whi:00} and \citet{Han:05} use
the stationary bootstrap, but require the test sample to be much
smaller than the training sample, which obviously does not hold here.
\citet{CoS:07} relax that requirement, but make the statistic more
complicated than necessary by adjusting the objective function of the
bootstrapped statistic to center it correctly.%
\footnote{Their
  recentering is required for consistency and does not imply higher
  order accuracy.} %
A parametric bootstrap, such as that used by
\cite{Mar:95}, \cite{Lut:99}, and \citet{ClM:12b}, is an alternative
method, but requires the benchmark model to be correctly specified.
Although I focus on the null hypothesis that the benchmark is
correctly specified, this paper's block bootstrap methods remain valid
when the benchmark is misspecified and are relatively easy to
implement.  So this paper's result for bootstrapping \oos\ statistics
considerably improves on existing procedures.

The next section presents our block bootstrap result.
Section~\ref{sec:4} concludes.

\section{The Bootstrap for Out-of-Sample Statistics}\label{sec:1b}
The validity of the bootstrap in Theorem~\ref{res:2} is a special case
of a result of independent interest---the validity in general of block
bootstraps for asymptotically normal \oos\ statistics.  This section
presents the general result.  In this section, let $\to^{p^{*}}$ and
$\to^{d^{*}}$ refer to convergence in probability or distribution
conditional on the observed data.  Similarly, $\E^{*}$, $\var^{*}$,
and $\cov^{*}$ refer to the expectation, variance, and covariance with
respect to the probability measure induced by the bootstrap, and
$y_t^{*}$, etc. is the random variable $y_t$ but under the
bootstrap-induced \cdf.

The notation in this section is more general than that of
Section~\ref{sec:1}.  The assumptions required are generalizations of
the conditions of Theorems~\ref{res:1} and~\ref{res:2}.  See
\citet{Wes:96,Wes:06}, \citet{WeM:98}, and \citet{Mcc:00} for a
discussion of these conditions, as theirs are nearly identical.
Theorem~\ref{res:3} gives the result.

\begin{asmp}\label{a1}
  The estimator $\hat{\beta}_t$ of $\btrue$ is estimated with a
  recursive, rolling, or fixed estimation window and satisfies
  $\hat{\beta}_{t} - \beta_{0} = \hat{B}_{t} H_t$; $\hat{B}_{t}$ is a
  sequence of $k \times q$ matrices such that $\sup_t |\hat{B}_t - B|
  \to^p 0$; $H_{t}$ is a sequence of $q$-vectors such that
  \begin{equation}
    H_{t} = \begin{cases}
      \tfrac1t \sum_{s=1}^t h_{s}(\btrue) & \text{recursive window} \\
      \tfrac1R \sum_{s=t-R+1}^t h_{s}(\btrue) & \text{rolling window} \\
      \tfrac1R \sum_{s=1}^R h_{s}(\btrue) & \text{fixed window} \\
    \end{cases}
  \end{equation}
  and $\E h_{s}(\btrue) = 0$ for all $s$.
\end{asmp}

\begin{asmp}\label{a2}
  Define $\psi_t(\beta) = (f_t(\beta), h_t(\beta))$; $\psi_t(\beta)$
  is covariance stationary and twice continuously differentiable in an
  open neighborhood $N$ of $\btrue$.  Also, $\E \sup_{\beta \in N}
  |\psi_t(\beta)|^r$ and $\E\sup_{\beta \in N}
  |\frac{\partial}{\partial \beta} \psi_t(\beta)|^r$ are uniformly
  bounded, and there exists a sequence of random variables $m_t$ such
  that $\sup_{i,\beta \in N} |\tfrac{\partial^2}{\partial
    \beta \partial \beta'} \psi_{it}(\beta)| \leq m_t$ almost surely
  and $\E m_t^2$ is uniformly finite.
\end{asmp}

\begin{asmp}\label{a3}
  Both $\psi_t(\beta)$ and $\frac{\partial}{\partial \beta}
  \psi_t(\beta)$ are $L_2$-\ned\ of size $-\frac12$ on the series
  $V_t$ with bounded \ned-coefficients for $\beta$ uniformly in $N$;
  $V_t$ is strong mixing of size $-\frac{r}{r-2}$ or uniform mixing of
  size $-\frac{r}{2r-2}$ with $r > 2$.%
  \footnote{For uniform mixing processes, $r = 2$ is also allowed as
    long as $\sup_{\beta \in N} \psi_t(\beta)^2$ and $\sup_{\beta \in
      N} (\partial \psi_t(\beta) / \partial \beta)^2$ are uniformly
    integrable.} %
\end{asmp}

\begin{asmp}\label{a4}
  The sequence $\hat f_1^{*},\dots,\hat f_T^{*}$ is constructed using
  the same procedure as the original \oos\ analysis over the bootstrap
  sample, where the bootstrap sample is generated by moving blocks,
  circular blocks, or stationary bootstrap with block lengths drawn
  from the geometric distribution.  The (expected) block length $b$
  satisfies $b \to \infty$ and $\frac{b}{P} \to 0$.
\end{asmp}

\begin{asmp}\label{a5}
  The bootstrapped estimator satisfies $\sup_t |\hat{B}_t^{*} - B^{*}|
  \to^p 0$, $B^{*} = B + o_p$ and $\frac1T \sum_{t=1}^T
  h_t^*(\btrue[*]) = 0$ a.s.
\end{asmp}

\begin{asmp}\label{a6}
  The asymptotic variance matrix of $\bar{f}(\btrue)$ is uniformly
  positive definite.
\end{asmp}

\begin{asmp}\label{a7}
  $R, P \to \infty$ as $T \to \infty$.
\end{asmp}

\begin{thm}\label{res:3}
  Under Assumptions~\ref{a1}--\ref{a7},
  \begin{equation}
    \pr\big[\sup_x \big\lvert \pr^*[\sqrt{P} (\bar{f}^* - \bar{f}(\btrue[*])) \leq x]
    - \pr[\sqrt{P}( \bar{f} - \E f_t) \leq x] \big\rvert > \epsilon\big] \to 0
  \end{equation}
  for all $\epsilon > 0$.
\end{thm}

\begin{rem}
  \citet{Mcc:00} proves asymptotic normality under weaker smoothness
  conditions on $f_t(\beta)$ and $h_t(\beta)$: only their expectations
  must be continuously differentiable.  It may be possible to extend
  Theorem~\ref{res:3} to those weaker conditions, but the current
  proof relies on a theorem of \citepos{JoD:00} establishing the
  consistency of \hac\ estimators and their theorem requires differentiability of
  the observations.  Extending \citepos{JoD:00} result to
  nondifferentiable functions should be possible, but is beyond the
  scope of this paper.
\end{rem}

\begin{rem}
  \citet{Whi:00} and \citet{Han:05} resample the forecasts but do not
  reestimate any of them which requires the additional assumption that
  $\tfrac{P}{R} \log \log R \to 0$ or that the forecasts themselves
  have no estimated parameters.%
\footnote{\citet{Whi:00} lists several
    different sets of assumptions that give the same result, but these
    seem to be the most general.} %
\end{rem}

\begin{rem}
  \citet{CoS:07} use the distribution of $\sqrt{P}(\bar{f}^{*} -
  \bar{f})$ to approximate that of $\sqrt{P}(\bar{f} - \E
  \bar{f}(\btrue))$.  But it is clear that $\bar{f}(\btrue[*])$
  is the bootstrap analogue of $\E \bar{f}(\btrue)$, the parameter of
  interest.  Because their bootstrap is miscentered, \citet{CoS:07}
  must redefine $\hat{\beta}_t^{*}$ to achieve consistency.  In this
  paper, though, consistency arises naturally.
\end{rem}

\begin{rem}
  It may be unnecessary to assume that $\bar{f}(\btrue)$ has positive
  definite asymptotic variance; if so, the bootstrap would work in the
  setup of \citet{ClM:05,ClM:01} and \citet{Mcc:07}.  That question is
  left to future research.
\end{rem}

\section{Conclusion}\label{sec:4}
The paper improves block bootstrap procedures of \oos\
statistics.

\appendix
\section{Proofs of Main Theoretical Results}\label{sec:B}

Define the additional notation
$F_t(\beta) = \tfrac{\partial}{\partial \beta} f_t(\beta)$,
$F_t = F_t(\btrue)$,
$\hat f_t^* = f_t^*(\hat\beta_t^*)$,
$F_t^*(\beta) = \tfrac{\partial}{\partial \beta} f_t^*(\beta)$,
$F_t^* = F_t^*(\btrue[*])$,
$h_t = h_t(\btrue)$,
$h_t^* = h_t^*(\btrue[*])$,
and
\begin{equation*}
      H_{t}^* = \begin{cases}
        \tfrac1t \sum_{s=1}^t h_{s}^* & \text{recursive window} \\
        \tfrac1R \sum_{s=t-R+1}^t h_{s}^* & \text{rolling window} \\
        \tfrac1R \sum_{s=1}^R h_{s}^* & \text{fixed window} \\
      \end{cases}
\end{equation*}

\newcommand{\WesA}[1][]{\oclt{t}
  (F_t^{#1} - \E^{#1} F_t^{#1}) B^{#1} H_t^{#1}}
\newcommand{\WesB}[1][]{\tfrac{1}{\sqrt{P}} \E^{#1} F_t^{#1} \osum{t} (B_t^{#1} -
  B^{#1}) H_t^{#1}}
\newcommand{\WesC}[1][]{\oclt{t}
  (F_t^{#1} - \E^{#1} F_t^{#1}) (B_t^{#1} - B^{#1}) H_t^{#1}}
\begin{proof}[Proof of Theorem~\ref{res:3}]
  Expand $f_t^{*}(\hat{\beta}_t^{*})$ around $\btrue[*]$
  to get
  \begin{align*}
    \sqrt{P} \big(\bar{f}^{*} - \bar{f}(\btrue[*])\big) &= \oclt{t}
    \big(f_t^{*} - \bar{f}(\btrue[*])\big) +
    \E^* F_t^{*} B^{*} \oclt{t} H_t^* \\
    & \quad + \WesA[*] + \WesB[*] \\ & \quad + \WesC[*] + \oclt{t} w_t^*
  \end{align*}
  where (as in \citealp{Wes:96}) the $i$th element of $w_t^*$ is
  \begin{equation*}
    w_{it} = \tfrac12 (\hat\beta_t^* - \hat\beta_{T+1})'
    \Big[\tfrac{\partial^2}{\partial \beta \partial\beta'}
    f_{it}^*(\tilde\beta_{it}^*) \Big]
    (\hat\beta_t^* - \hat\beta_{T+1})
  \end{equation*}
  and each $\tilde\beta_{it}^*$ lies between $\hat\beta_t^*$ and
  $\hat\beta_{T+1}$.  The argument that $\oclt{t} w_t^* = o_{p^*}(1)$ is
  identical to West's, using our Lemma~\ref{res:a2} in place of his
  Lemma A3.  Then
  \begin{gather}
    \WesA[*] \to^{p^*} 0 \label{eq:7} \\
    \WesB[*] \to^{p^*} 0 \label{eq:9} \\
  \intertext{and}
    \WesC[*] \to^{p^*} 0 \label{eq:10}
  \end{gather}
  from Lemma~\ref{res:a4} and the result follows from
  Lemma~\ref{res:a3}, with the form of the asymptotic variance
  following directly from \citet{Wes:96} and \citet{WeM:98}.
\end{proof}

\section{Supporting Results}

Define the following notation to describe the bootstrap process. Let
$u_1,\dots,u_{J}$ denote the starting periods of each block of the
bootstrap, let $\ell_i$ denote their lengths, define the sigma fields
\[
  \Ms_i = \sigma(u_1,\dots,u_i; \ell_1,\dots,\ell_i; J)
\]
and
\[
  \Ms_i^* = \sigma(y_1,\dots,y_T; x_1,\dots, x_T;u_1,\dots,u_i; \ell_1,\dots,\ell_i; J)
\]
and let $j^*_t$ denote the block that contains the $t$th observation
of the bootstrap process, so
\[
  t \in \{1 + \sum_{i=1}^{j^*_t-1} \ell_i, \sum_{i=1}^{j^*_t} \ell_i\}.
\]

\begin{lema}\label{res:a2}
  Suppose $a \in [0,1/2)$ and the conditions of Theorem~\ref{res:3}
  hold.
  \begin{enumerate}
  \item $P^a \sup_t | H_{t} | \to^p 0$ and $P^a \sup_t | H_{t}^{*} |
    \to^{p^{*}} 0$.
  \item $P^a \sup_t | \hat{\beta}_{t} - \beta_{0} | \to^{p} 0$ and
    $P^a \sup_t | \hat{\beta}^{*}_{t} - \hat{\beta}_T |
    \to^{p^{*}} 0$.
  \item $\tfrac{1}{\sqrt{P}} \sum_{t=R+1}^T (F_t - \E F_t) = O_{p}(1)$
    and $\tfrac{1}{\sqrt{P}} \sum_{t=R+1}^T (F_t^* - \E F_t^*) =
    O_{p^*}(1)$.
  \end{enumerate}
\end{lema}

\begin{proof}[Proof of Lemma~\ref{res:a2}]
  \begin{enumerate}
  \item Convergence for the original process is essentially the same
    as the proof in \citet{Wes:96}, with minor tweaks as in
    \citet{Cal:15}; we will repeat it here for completeness. Our
    assumptions ensure that $h_t$ is an $L_2$-mixingale of size $-1/2$;
    let $c_t$ and $\zeta_k$ denote its mixingale constants and
    coefficients and note that $h_t/t$ is also an $L_2$-mixingale array
    with constants $c_s/t$ and coefficients $\zeta_k$, since
    \begin{align*}
      \| \E_{t-k} h_t / t \| = t^{-1} \| \E_{t-k} h_t \| \leq (c_t/t)\, \zeta_k
    \end{align*}
    and
    \begin{equation*}
      \| h_t/t - \E_{t+k} h_t/t \| = t^{-1} \|  h_t - \E_{t+k} h_t \| \leq (c_t/t)\, \zeta_{k+1}.
    \end{equation*}

    Then
    \begin{align*}
      P^{2a} \E\Bigg[\omax{t} \Big| \sum_{s=1}^t h_s/t \Big|^2\Bigg]
      &\leq P^{2a} \E\Bigg[\omax{t} \Big| \sum_{s=1}^t h_s/s \Big|^2\Bigg] \\
      &= O(P^{2a}) \sum_{s=1}^{T-1} s^{-2}
    \end{align*}
    where the second line follows from \citepos{Mcl:75} maximal
    inequality (also available as \citealp{Dav:94}, Theorem 16.9 and
    Corollary 16.10). This term converges to zero from \citepos{Wes:96} Lemma A1.

    For the bootstrap process, we will use an argument similar to
    those in \cite{Cal:14} and make liberal use of the
    LIE. Define $\Hs_i^* = \sum_{t=K_{i-1}+1}^{K_i}
    h_t^*/t$, so
    \begin{align*}
      \omax{t} | H_t^* |
      &\leq \omax{t} \Big| \sum_{i=1}^{j^*_t - 1}\Hs_i^* \Big| + \omax{t} \Big|\sum_{s=K_{j^*_t - 1}+1}^t h_s^*/t \Big|
    \end{align*}
    Now observe that $\{\sum_{i=1}^j \Hs_i^*; \Ms_j^*\}$ is a martingale, so
    \begin{equation*}
      \pr^*\Bigg[ \omax{t} \Big| \sum_{i=1}^{j^*_t - 1}\Hs_i^* \Big| > \epsilon \Bigg]
      \leq \epsilon^{-2} \E^* \sum_{i=1}^J \E^*(\Hs_i^{*2} \mid \Ms_{i-1}^*).
    \end{equation*}

    We can deal with this last term as follows. The expectation
    $\E^*(\Hs_i^{*2} \mid \Ms_{i-1}^*; \ell_i)$ only averages over the
    start of the block used by $\Hs_i^*$, so
    \begin{align*}
      \E^*(\Hs_i^{*2} \mid \Ms_{i-1}^*; \ell_i)
      &= \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \Bigg[\sum_{t = 1}^{\ell_{i}} \tilde{h}_{u + t}(\btrue[*]) / (K_{i-1} + t + 1)\Bigg]^2 \\
      &= \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \Bigg[\sum_{t = 0}^{\ell_{i}-1}
      (\tilde{h}_{u + t}(\btrue) + \tilde{h}_{u + t}(\btrue[*]) - \tilde{h}_{u + t}(\btrue))/ (K_{i-1} + t + 1)\Bigg]^2
    \end{align*}
    and we want to show that
    \begin{equation}\label{eq:4}
      P^a \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \sum_{i=1}^J \Bigg[\sum_{t = 1}^{\ell_{i}}
      \tilde{h}_{u + t}(\btrue)/ (K_{i-1} + t)\Bigg]^2 \to^p 0
    \end{equation}
    and
    \begin{equation}\label{eq:5}
      P^a \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \Bigg[\sum_{t = 1}^{\ell_{i}}
      (\tilde{h}_{u + t}(\btrue[*]) - \tilde{h}_{u + t}(\btrue))/ (K_{i-1} + t)\Bigg]^2
      \to^p 0.
    \end{equation}

    For~\eqref{eq:4}, the $L_2$-mixingale property again ensures that
    \begin{align*}
      \E\Bigg|P^a \tfrac{1}{T-1} &\sum_{u = 0}^{T-2} \sum_{i=1}^J \Bigg[\sum_{t = 1}^{\ell_{i}}
      \tilde{h}_{u + t}(\btrue)/ (K_{i-1} + t)\Bigg]^2 \Bigg| \\
      &= P^a \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \E \sum_{i=1}^J \E\Bigg(\Bigg[\sum_{t = 1}^{\ell_{i}}
      \tilde{h}_{u + t}(\btrue)/ (K_{i-1} + t)\Bigg]^2 \,\Big|\, \Ms_{i-1}, \ell_i\Bigg) \\
      &\leq P^a \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \E \sum_{i=1}^J C \sum_{t = 1}^{\ell_{i}} (K_{i-1} + t)^{-2} \\
      &\leq P^a \sum_{t=1}^{T-1} t^{-2}
    \end{align*}
    (should be $2a$ in all of the above)
    which converges to zero as before. For~\eqref{eq:5},
    \begin{align*}
      \tfrac{1}{T-1} &\sum_{u = 0}^{T-2} \Bigg[\sum_{t = 1}^{\ell_{i}}
      (\tilde{h}_{u + t}(\btrue[*]) - \tilde{h}_{u + t}(\btrue))/ (K_{i-1} + t)\Bigg]^2 \\
      &= \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \Bigg[\sum_{t = 1}^{\ell_{i}}
      \nabla \tilde{h}_{u + t}(\tilde \beta_T)'(\btrue[*] - \btrue)/ (K_{i-1} + t)\Bigg]^2 \\
      &= O_p(1/T) \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \Bigg[\sum_{t = 1}^{\ell_{i}}
      \lVert \nabla \tilde{h}_{u + t}(\tilde \beta_T) \rVert / (K_{i-1} + t)\Bigg]^2
    \end{align*}
    and
    \begin{align*}
      \E \Bigg| (P^{2a}/T) &\tfrac{1}{T-1} \sum_{u = 0}^{T-2} \sum_{i=1}^J \Bigg[\sum_{t =
        1}^{\ell_{i}} \lVert \nabla \tilde{h}_{u + t}(\tilde \beta_T)
      \rVert / (K_{i-1} + t)\Bigg]^2 \Bigg| \\
      &= (P^{2a}/T) \tfrac{1}{T-1} \sum_{u = 0}^{T-2} \E \sum_{i=1}^J
      \E \Bigg(\Bigg[\sum_{t =
        1}^{\ell_{i}} \lVert \nabla \tilde{h}_{u + t}(\tilde \beta_T)
      \rVert / (K_{i-1} + t)\Bigg]^2 \,\Bigg|\, \Ms_{i-1}; \ell_i \Bigg) \\
      &\leq (P^{2a}/T) \E \sum_{i=1}^J \ell_i^2/K_{i-1}^2
    \end{align*}
    which converges to zero by Lemma~[add!].

    Since
    \[
    \omax{t} \Big|\sum_{s=K_{j^*_t - 1}+1}^t h_s^*/t \Big| \leq \sum_{i=1}^{J} \max_{t = K_{i-1}+1,\dots,K_i} \Big| \sum_{s=K_{i-1}+1}^t h_s^*/t \Big|
    \]
    the proof that the second component converges to zero is similar.
  \item We have
    \begin{multline}
      P^a \sup_t | \hat{\beta}_t - \btrue | = P^a \sup_t |\hat{B}_{t}
      H_{t}| \\ \leq \sup_{t,u} \Big| [ \hat{B}_u - B]
      \tfrac{P^a}{t} \sum_{s=1}^t h_{s} \Big| + \sup_{t} \Big|
      B \tfrac{P^a}{t} \sum_{s=1}^t h_{s} \Big|
    \end{multline}
    and both terms converge to zero in (conditional) probability by
    the previous argument and by assumption.  The same argument holds
    for $\hat{\beta}_t^{*} - \btrue[*]$ as well.
  \item $\tfrac{1}{\sqrt{P}} \sum_{t=R+1}^T (F_t - \E F_t)$ obeys
    \citepos{Jon:97} \clt, so the result is trivial.  Also,
   \begin{equation}
     \Big\lvert\tfrac{1}{\sqrt{P}} \sum_{t=R+1}^T
     ( F_t^* - \E^* F^*)\Big\rvert = O_{p^*}(\Omega^*(\hat\beta_{T+1})^{1/2}) = O_p(1)
    \end{equation}
    as in the proof of part 1, where now $\Omega^*(\beta) = \var^* F_t^*(\beta)$.
  \end{enumerate}
\end{proof}

\begin{lema}\label{res:a4}
  Under the conditions of Theorem~\ref{res:3}, Equations
  \eqref{eq:7}--\eqref{eq:10} and \eqref{eq:11}--\eqref{eq:13} hold.
\end{lema}

\begin{proof}[Proof of Lemma~\ref{res:a4}]
We can write
\begin{equation*}
  \Big\lvert \WesA[*] \Big\rvert \leq
  \Big\lvert \oclt{t} (F_t^* - \E^* F^*_t) B^* \Big\rvert
  \sup_t | H_t^* |.
\end{equation*}
From Lemma~\ref{res:a2}, $\sup_t | H_t^* | \to^p 0$ and $\oclt{t}
(F_t^* - \E^* F_t^*) = O_p(1)$, establishing~\eqref{eq:7}.  The proofs
of~\eqref{eq:9}, \eqref{eq:10}, and \eqref{eq:11}--\eqref{eq:13} are
similar.
\end{proof}

\begin{lema}\label{res:a3}
  Suppose the conditions of Theorem~\ref{res:3} hold, let
  \[\psi_t^{*}(\beta) =
  \binom{f_t^{*}(\beta) - \bar{f}(\beta)}{h_t^{*}(\beta) - \bar h(\beta)},\]
  and define $\psi_t = \psi_t(\btrue)$, $\hat\psi_t = \psi_t(\btrue[*])$, and
  $\psi_t^* = \psi_t^*(\btrue[*])$.  Then
  \begin{equation}\label{eq:1}
    \pr\Big[\sup_x \Big| \pr^{*}\Big[ \oclt{t} \psi_{t}^{*}
    \leq x \Big] - \pr\Big[ \oclt{t} \psi_{t}
    \leq x \Big] \Big| > \epsilon \Big] \to 0
  \end{equation}
  for all positive $\epsilon$, where the inequalities hold element-by-element.
\end{lema}

\begin{proof}[Proof of Lemma~\ref{res:a3}]
  Note that both $\psi_t$ and $\psi_t^*$ satisfy
  \clt s, (\citealp[Theorem~2]{Jon:97} and
  \citealp[Theorem~1]{Cal:14}) so it suffices to prove that
  \[ \tfrac{ 1 }{ P } \var^* \osum{t} \psi_t^*
  - \tfrac{ 1 }{ P } \var \osum{t} \psi_t \to^p
  0. \] I will prove this result for the Circular Block Bootstrap
  \citep{PoR:92}---the proof for other block bootstraps is similar and
  follows as in \cite{Cal:14}.

  \newcommand{\su}{\tfrac{ 1 }{ b T } \sum_{ \tau = 1 }^{ T }%
  \sum_{ s, t \in I( \tau ) }}

  Since $\E^* \psi_t^* = 0$ by construction, we have
  \begin{equation}\label{eq:3}
  \tfrac{ 1 }{ P } \var^* \sum_{ t = R + 1 }^T \psi_t^*
  = \su \hat \psi_s \hat \psi_t'
  \end{equation}
  almost surely, with
  \begin{equation*}
    I(\tau) =
    \begin{cases}
      \{ \tau, \dots, \tau + b - 1 \} & \text{if } \tau \leq T - b + 1 \\
      \{ \tau, \dots, T \} \cup \{ 1, \dots, \tau - ( T - b + 1 ) \}
      & \text{if } \tau > T - b + 1.
    \end{cases}
  \end{equation*}
  We can rewrite the right side of \eqref{eq:3} as
  \begin{multline}\label{eq:6}
    \su \hat\psi_s \hat\psi_t'
    = \tfrac{1}{T} \sum_{s,t=1}^T \hat\psi_s \hat\psi_t' \;
      \big(1 - \tfrac{\lvert s - t \rvert}{b}\big)^+ \\
      + \tfrac{1}{bT} \sum_{s = 1}^{b - 1} \sum_{t = T - b + s}^{T}
        (\hat\psi_s \hat\psi_t' +
         \hat\psi_t \hat\psi_s') \;
        \big(1 - \tfrac{\lvert s - t + T\rvert}{b}\big)^+.
  \end{multline}

  The first term of \eqref{eq:6} can be shown to converge in
  probability to $\tfrac{1}{P} \var \osum{t} \psi_t$ by Theorem 2.2 of
  \citet{JoD:00}.  Most of the necessary conditions of their theorem
  hold by assumption and it remains to prove their condition (2.9)
  holds, namely that
  \begin{equation}\label{eq:2}
    \sup_{\beta \in N} \Bigg\lVert \tfrac{1}{T} \sum_{t=1}^T e^{i \xi t/b}
    \Bigg( \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} -
          \E \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} \Bigg)
    \Bigg\rVert_2 \to 0
  \end{equation}
  for all $j$, $k$, and $\xi$.  For any fixed $\beta \in N$,
  \begin{equation*}
    \tfrac{1}{T} \sum_{t=1}^T \cos( \xi t / b )
    \Bigg( \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} -
          \E \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} \Bigg)
    \to^p 0
  \end{equation*}
  and
  \begin{equation*}
    \tfrac{1}{T} \sum_{t=1}^T \sin( \xi t / b )
    \Bigg( \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} -
          \E \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} \Bigg)
    \to^p 0
  \end{equation*}
  by a mixingale \lln\ \citep{Dav:93}.%
\footnote{Remember that \ned\
    processes are also mixingales.  See, for example, \citet[Section
    17.2]{Dav:94}.} %
  Convergence in $L_2$ for each $\beta \in N$ then
  follows from the existence of the $r$th moment ($r > 2$) and uniform
  convergence is a consequence of our bound on the second derivative
  of $\psi_t$.

  The conclusion then holds if the second term of~\eqref{eq:6} is
  $o_p(1)$.  We have, for each element of that matrix,
  \begin{equation*}
    \Big| \tfrac{1}{bT} \sum_{s = 1}^{b - 1} \sum_{t = T - b + s}^{T}
        \hat\psi_{ks} \hat\psi_{jt}
        \big(1 - \tfrac{\lvert s - t + T\rvert}{b}\big)^+ \Big| \leq
     \tfrac{1}{b T} \sum_{s = 1}^{b - 1} |\hat\psi_{ks}|
         \sum_{t = T - b + 1}^{T} | \hat\psi_{jt} |
  \end{equation*}
  almost surely.  For any positive $\delta$,
  \begin{equation*}
    \pr \Bigg[ \tfrac{1}{T} \sum_{s = 1}^{b - 1} | \hat\psi_{is} |
              > \delta \Bigg] \leq
    \pr \Bigg[\tfrac{1}{T} \sum_{s = 1}^{b - 1} \sup_{ \beta \in N } | \psi_{is}( \beta ) |
              > \delta \Bigg] +
    \pr \big[ \hat \beta_{T+1} \notin N \big].
  \end{equation*}
  Both terms converge to zero by assumption, so $\tfrac{1}{T} \sum_{s
    = 1}^{b - 1} |\hat\psi_{is}| = o_p(1)$.  A similar
  argument shows that $\tfrac{1}{b} \sum_{t = T - b + 1}^{T} |
  \hat\psi_{jt} | = O_p(1)$, completing the proof.
\end{proof}

\begin{lema}\label{res:a5}
  If the conditions of Theorem~\ref{res:3}, except for those governing
  the bootstrap process, hold then
  \begin{equation}
    \oclt{t}
    (\hat f_t - \E f_t) \to^d N(0, \sigma^2),
  \end{equation}
  with
  \begin{align}
  \sigma^2 &= S_{ff} + \lambda_{fh} (F B S_{fh}' + S_{fh} B' F') + 2 \lambda_{hh} F V_\beta F' \\
  (\lambda_{fh}, \lambda_{hh}) &=
  \begin{cases}
    (1, 2) \times \big(1 - \tfrac{1}{\pi} \ln(1 + \pi)\big)
      & \text{recursive window} \\
    \big(\tfrac{\pi}{2}, \pi - \tfrac{\pi^2}{3}\big)
      & \text{rolling window with $\pi \leq 1$} \\
    \big(1 - \tfrac{1}{2\pi}, 1 - \tfrac{1}{3\pi}\big)
      & \text{rolling window with $\pi > 1$} \\
    (0, \pi) & \text{fixed window},
  \end{cases}
  \end{align}
  $\Big(\begin{smallmatrix}S_{ff} & S_{fh} \\ S_{fh}' &
    S_{hh} \end{smallmatrix} \Big)$ the asymptotic variance of $(\bar
  f(\btrue)', \bar h(\btrue)')'$, and $V_\beta$ the asymptotic
  variance of~$\btrue[*]$.
\end{lema}

\begin{proof}[Proof of Theorem~\ref{res:3}]
  As in \citet{Wes:96} and \citet{WeM:98} (and as in the proof of
  Theorem~\ref{res:3}), expand $\hat f_t$ around
  $\btrue$ to get
  \begin{align*}
    \sqrt{P} \big(\bar{f} - \bar{f}(\btrue)\big) &= \oclt{t}
    \big(f_t - \E f_t\big) +
    \E F_t B \oclt{t} H_t \\
    & \quad + \WesA + \WesB \\ & \quad + \WesC + \oclt{t} w_t
  \end{align*}
  where (again, as in \citealp{Wes:96}) the $i$th element of $w_t$ is
  \begin{equation*}
    w_{it} = \tfrac12 (\hat\beta_t - \btrue)'
    \Big[\tfrac{\partial^2}{\partial \beta \partial\beta'}
    f_{it}(\tilde\beta_{it}) \Big]
    (\hat\beta_t - \btrue)
  \end{equation*}
  and each $\tilde\beta_{it}$ lies between $\hat\beta_t$ and
  $\btrue$; $\oclt{t} w_t = o_{p}(1)$ as in Theorem~\ref{res:3}.
  Then, as before,
  \begin{gather}
    \WesA \to^{p} 0 \label{eq:11} \\
    \WesB \to^{p} 0 \label{eq:12}\\
  \intertext{and}
    \WesC \to^{p} 0 \label{eq:13}
  \end{gather}
  from Lemma~\ref{res:a4}.  The formula of the asymptotic variance can
  be derived exactly as in \citet{Wes:96} and \citet{WeM:98}. Since
  $f_t$ and $H_t$ both satisfy \clt s, this completes the
  proof.
\end{proof}

\bibliography{texextra/references}
\end{document}

% LocalWords:  ClW JEL ISI Google GiW Mcc ClM CoS CCS StW IMA GiR WeM fh X'X hh
% LocalWords:  PaT AllRefs isi ima uc sv PeT GoW lm il GoK RoW Econometrica PoR
% LocalWords:  Finan StepM studentizing studentization Whi HuW RSW recentering
% LocalWords:  DiM LiS Kun McCracken lt filtrations GoJR JoD McCracken's Econom
% LocalWords:  Corradi unstudentized studentized fg gg GoJ gcalhoun HLN li PoW
% LocalWords:  Econometricians reestimate PPW resample miscentered AnG eq Helle
% LocalWords:  Bunzel Yu Hsu Pincheira HHK th AtO ik DoH Wolak's Wol stepdown
% LocalWords:  Hsu's iq mk Ames Amit Goyal rfs Ivo Welch Wes covariance MeR McW
% LocalWords:  familywise prespecified CoD Gia pointwise misspecified InK MeP
% LocalWords:  VeR xtable Dah dbframe oos parametrizations iid dgp return's CaT
% LocalWords:  outperformance Hmisc Har nondifferentiable bT jt Meng
% LocalWords:  stationarity mixingale mixingales texextra Timmermann