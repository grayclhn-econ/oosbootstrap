\documentclass[11pt,fleqn]{article}

\usepackage{amsfonts}
\usepackage{amsmath,amsthm,amssymb,graphicx,setspace,url,booktabs,tabularx,enumerate,slantsc}
\usepackage[T1]{fontenc}
\usepackage[nolists,nomarkers]{endfloat}
\usepackage[sort,round,comma]{natbib}
\usepackage[margin=1.25in]{geometry}
\usepackage[small]{caption}
\newcolumntype{C}{>{\centering\arraybackslash}X}
\onehalfspacing
\bibliographystyle{abbrvnat}
\newcommand\citepos[2][]{\citeauthor{#2}'s \citeyearpar[#1]{#2}}
\newcommand\poscw{\citeauthor{ClW:06}'s \citeyearpar{ClW:06,ClW:07}}
\newcommand\citen[1]{\citeauthor{#1}, \citeyear{#1}}
\frenchspacing
\input{tex/mcDef}
\input{tex/ap}

% These commands are generated when the monte carlo and applied
% sections are run; I'm giving them definitions now so that LaTeX will
% run.
\providecommand\bootsize{[missing]}
\providecommand\empiricalcriticalvalue{[missing]}
\providecommand\nboot{[missing]}
\providecommand\testsize{[missing]}
\providecommand\totalsims{[missing]}
\providecommand\windowlength{[missing]}
\providecommand\empiricaltable{[missing]}

\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{claim}[thm]{Claim}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lema}{Lemma}[section]
\newtheorem{alg}{Algorithm}
\newtheorem{asmp}{Assumption}[section]

\theoremstyle{definition}

\newtheorem{example}{Example}
\newtheorem{defn}{Definition}
\newtheorem{rem}{Remark}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
%\DeclareMathOperator{\vec}{vec}
\DeclareMathOperator{\vech}{vech}

\DeclareMathOperator{\pr}{Pr}

\newcommand{\btrue}[1][]{\if#1*\hat\beta_{T+1}\else\beta_0\fi}

\newcommand{\X}{\ensuremath{\mathrm{X}}}
\newcommand{\R}{\ensuremath{\mathrm{R}}}
\newcommand{\p}{\ensuremath{\mathrm{P}}}

\newcommand{\osum}[1]{\sum_{#1=R+1}^T}
\newcommand{\oavg}[1]{\tfrac{1}{P} \osum{#1}}
\newcommand{\oclt}[1]{\tfrac{1}{\sqrt{P}} \osum{#1}}

\newcommand{\aic}{\textsc{aic}}
\newcommand{\bic}{\textsc{bic}}
\newcommand{\brc}{\textsc{brc}}
\newcommand{\cdf}{\textsc{cdf}}
\newcommand{\clt}{\textsc{clt}}
\newcommand{\dd}[1]{\frac{\partial}{\partial #1}}
\newcommand{\dgp}{\textsc{dgp}}
\newcommand{\fclt}{\textsc{fclt}}
\newcommand{\fwe}{\textsc{fwe}}
\newcommand{\gdp}{\textsc{gdp}}
\newcommand{\hac}{\textsc{hac}}
\newcommand{\lln}{\textsc{lln}}
\newcommand{\ma}{\textsc{ma}}
\newcommand{\mds}{\textsc{mds}}
\newcommand{\ned}{\textsc{ned}}
\newcommand{\ols}{\textsc{ols}}
\newcommand{\oos}{\textsc{oos}}
\newcommand{\sfwe}{\textsc{sfwe}}
\newcommand{\spa}{\textsc{spa}}
\newcommand{\wfwe}{\textsc{wfwe}}

\renewcommand{\Re}{\ensuremath{\mathbb{R}}}

\renewcommand{\topfraction}{.85}
\renewcommand{\bottomfraction}{.7}
\renewcommand{\textfraction}{.15}
\renewcommand{\floatpagefraction}{.66}
\renewcommand{\dbltopfraction}{.66}
\renewcommand{\dblfloatpagefraction}{.66}
\setcounter{topnumber}{9}
\setcounter{bottomnumber}{9}
\setcounter{totalnumber}{20}
\setcounter{dbltopnumber}{9}

\begin{document}

\author{Gray Calhoun\thanks{ Economics Department; Iowa State
    University; Ames, IA 50011.  Telephone: (515) 294-6271.  Email:
    \texttt{gcalhoun@iastate.edu}.  Web:
    \texttt{http://gray.clhn.co}.  I'd like to
    thank Helle Bunzel, Todd Clark, Graham Elliott, Yu-Chin Hsu,
    Michael McCracken, Pablo Pincheira, Allan Timmermann, Stephane
    Meng-Feng Yen and participants at the 2011 Midwest Econometrics
    Group meeting for helpful comments and discussions.  I'd also like to thank Amit
    Goyal for providing computer code and data for his 2008
    \textsc{rfs} paper with Ivo
    Welch \citep{GoW:08}.} \\
  Iowa State University}

\title{An asymptotically normal out-of-sample
  test of equal predictive accuracy for nested models} 

\date{March 14, 2013}

\maketitle

\begin{abstract} 
  \noindent This paper proposes a modification of \citepos[\textit{J.
    Econom.}]{ClW:07} adjusted out-of-sample $t$-test.  The
  alternative model is still estimated with a fixed-length rolling
  window, but the benchmark is estimated with a recursive window. The
  resulting statistic is asymptotically normal even when the models
  are nested.  Moreover, the alternative model can be estimated using
  common model selection methods, such as the \aic\ or \bic.  This
  paper also presents a method to compare multiple models
  simultaneously while controlling familywise error, and
  substantially improves existing block bootstrap methods for
  out-of-sample statistics.  This procedure is then used to analyze
  \citepos[\textit{Rev. Finan. Stud.}]{GoW:08} excess returns dataset.

\strut

\noindent Keywords: Forecast Evaluation, Martingale Difference
Sequence, Model Selection, Family-Wise Error Rate; Multiple Testing;
Bootstrap; Reality Check

\strut

\noindent JEL Classification Numbers: C22, C53

\end{abstract}

\newpage 

\section{Introduction} This paper proposes an out-of-sample (\oos)
test statistic that is asymptotically normal with mean zero even when
the models studied are nested.  This paper also proves that common
block bootstrap methods consistently estimate the statistic's
distribution, and shows how the statistic can be used to study many
models simultaneously while maintaining strong control of the
familywise error (\fwe), i.e. ensuring that the probability that the
hypothesis that the model has equal predictive ability to a benchmark
is incorrectly rejected for any of the models is no higher than a
preset level.

\oos\ tests are commonly used in International Macroeconomics,
Macroeconomics, and Finance (see, for example, \citealt{MeR:83};
\citealt{StW:03}; and \citealt{GoW:08}) and there is a substantial
literature developing the theoretical properties of these statistics,
beginning primarily with \citet{DiM:95} and
\citet{Wes:96}.\footnote{Other papers in this literature include
  \citet{WeM:98}, \citet{Mcc:98,Mcc:00},
  \citet{ClM:01,ClM:05-2,ClM:05,ClM:11b,ClM:12,ClM:12b},
  \citet{CCS:01}, \citet{CoS:02,CoS:04,CoS:07}, \citet{Whi:00},
  \citet{InK:04,InK:06}, \citet{Han:05}, \citet{Ros:05},
  \citet{ClW:06,ClW:07}, \citet{Ana:07}, \citet{GiR:09,GiR:10},
  \citet{HuW:10}, \citet{HLN:11}, \cite{InR:11}, \cite{Pin:11},
  \cite{RoS:11,RoS:11b}, and \citet{Cal:11}, among others.  For recent
  reviews of this literature and additional references, see
  \citet{McW:02}, \citet{CoS:06}, \citet{Wes:06}, \citet{ClM:11c},
  \citet{CoD:11}, and \citet{Gia:11}} In a pair of papers,
\citet{ClW:06,ClW:07} develop an \oos\ test of the null hypothesis
that a small benchmark model is correctly specified.  Their test
compares the forecasting performance of a pair of nested models, and
the null hypothesis is that the innovations in the smaller model form
a martingale difference sequence.  This test procedure is popular, and
one assumes that this is due in part to the statistic's convenience,
the statistic is approximately normal after adjusting for the
estimation error of the larger model.  Normality comes from a
fixed-length rolling window, as in \citet{GiW:06}, and the adjustment
centers the statistic appropriately.  This statistic is especially
convenient because other \oos\ tests for similar hypotheses
(\citealt{CCS:01}; \citealt{ClM:01,ClM:05}; \citealt{CoS:02,CoS:04};
and \citealt{Mcc:07}; among others) have a nonstandard limit
distribution and place restrictions on the models under consideration,
while other asymptotically normal statistics test a different null
hypothesis \citep{GiW:06} or place assumptions on the models and \dgp\
that are often violated in empirical work (\citealt{DiM:95};
\citealt{Wes:96}; \citealt{WeM:98};
\citealt{Mcc:00}).\footnote{\citet{DiM:95} assume that the models are
  not estimated. \citet{Wes:96}, \citet{WeM:98}, and \citet{Mcc:00}
  implicitly assume that the models do not converge to the same limit,
  which rules out nesting.}

However, Clark and West's statistic is only ``approximately normal''
in an informal sense.  Clark and West present Monte Carlo evidence of
the statistic's distribution, but only prove that the statistic is
asymptotically normal with mean zero when the benchmark model is a
random walk \citep{ClW:06}. Estimating the parameters of the smaller
model invalidates their proof.

In this paper, I show that a modified version of their statistic is
asymptotically normal even when the smaller model is estimated.  To
achieve normality, we need a consistent estimate of the pseudo-true
benchmark model, while maintaining an inconsistent estimate of the
larger model so that we can ignore nesting.  We can meet both needs by
using different window strategies for each model: the benchmark model
is estimated using a recursive window and the alternative with a
fixed-length rolling window.

Mixing window strategies is uncommon but needn't be. In most
applications, the null hypothesis imposes stability as well as equal
accuracy between the two models.  The benchmark model rarely allows
for breaks, parameter drift, or other forms of
instability,\footnote{Exceptions are \citepos{StW:07}
  \textsc{ima}(1,1) and \textsc{uc-sv} models of inflation.} but the
researcher is typically concerned about instability.  Indeed, concern
about instability is often given as a reason for doing an \oos\
analysis, especially with a short rolling window.\footnote{This
  motivation is discussed by \citet{StW:03}, \citet{PeT:05,PeT:07},
  \cite{GiW:06}, \citet{GoW:08}, \citet{ClM:09c}, and
  \cite{GiR:09,GiR:10}, among others.} A researcher could impose
stability on both models by using a recursive window or relax
stability for both by using a rolling window; either approach should
not affect the test's size, but may affect power.  But the researcher
could instead impose stability on the benchmark and relax it for the
alternative by using a recursive window for the benchmark and a
rolling window for the alternative model.  This approach could have a
power advantage and is similar in spirit to using a Likelihood Ratio
Test instead of an \textsc{lm} or Wald test, which depend on just the
restricted or unrestricted model respectively.

This paper's statistic has a substantial advantage over existing \oos\
tests for nested models: the alternative can be essentially arbitrary
as long as high level moment conditions hold.  In particular,
researchers can use model selection techniques like the \aic\ or \bic\
to determine the number of lags to include, the particular exogenous
variables to include, etc.  Other methods that test a similar
hypothesis are unable to handle these models \citep[except][which does
not allow the benchmark to be estimated]{ClW:06}; \citet{GiW:06} are
able to handle such models for both the alternative and the benchmark
but, as mentioned earlier, they test a different aspect of forecasting
performance.

This paper focuses on nested models, as they have received the most
attention in the empirical and theoretical literature, but the
statistic can be used with non-nested models as well.  This generality
is useful, since \citepos{Wes:96} results do not apply to non-nested
models if they both encompass the true \dgp,\footnote{\citet{ClM:11b}
  call this scenario, ``overlapping models.''} which is allowable
under the null: in the limit, both models will converge to the \dgp\
and give identical forecasts.  Consequently, the naive \oos\ $t$-test
is invalid, even after correcting the standard error if necessary to
reflect parameter uncertainty.  \citet{ClM:11b} show that the fixed
window \oos\ $t$-test remains normal for these models but the
recursive and rolling windows (with the window size increasing to
$\infty$) do not, and provide a procedure for pointwise (but not
uniformly) valid tests for the recursive and rolling windows and
uniformly valid tests for the fixed window.  The test proposed in this
paper is uniformly valid and places fewer assumptions on the models
under study and the true \dgp.

Since researchers often have a set of potential models and want to
know which of them significantly outperform the benchmark, procedures
that compare a single pair of models are of limited practical value.
As \citet{Whi:00} demonstrates, looking at the naive $p$-values of
individual tests is misleading, but researchers can obtain a valid test
by using the bootstrap to approximate the distribution of the largest
individual test statistic under the null (White calls this procedure
the \textit{Bootstrap Reality Check} or \brc).  This paper presents a test
for equal predictive ability of multiple models based on
\citepos{RoW:05} StepM, which uses a step-down procedure that
iteratively rejects models to achieve higher power than the \brc\ and
indicate which of the models improves on the benchmark (see
Theorem~\ref{res:2} for details).

This paper also presents a new result for the validity of block
bootstraps with \oos\ statistics, which is necessary to verify that
the StepM is valid.  Existing results on bootstrapping \oos\
statistics have some drawbacks.  \citet{Whi:00} and \citet{Han:05} use
the stationary bootstrap, but require the test sample to be much
smaller than the training sample, which obviously does not hold here.
\citet{CoS:07} relax that requirement, but make the statistic more
complicated than necessary by adjusting the objective function of the
bootstrapped statistic to center it correctly.\footnote{Their
  recentering is required for consistency and does not imply higher
  order accuracy.}  A parametric bootstrap, such as that used by
\cite{Mar:95}, \cite{Lut:99}, and \citet{ClM:12b}, is an alternative
method, but requires the benchmark model to be correctly specified.
Although I focus on the null hypothesis that the benchmark is
correctly specified, this paper's block bootstrap methods remain valid
when the benchmark is misspecified and are relatively easy to
implement.  So this paper's result for bootstrapping \oos\ statistics
considerably improves on existing procedures.

The next section presents our new statistics and Section~\ref{sec:1b}
our block bootstrap result.  Section~\ref{sec:2} presents
simulations that compare our pairwise \oos\ test to \poscw\ original
statistics.  Section~\ref{sec:3} demonstrates the use of our statistic
by reanalyzing \citepos{GoW:08} study of excess return
predictability. Section~\ref{sec:4} concludes.

\section{Out-of-Sample Model Comparisons}\label{sec:1}
This section presents the new \oos\ statistic.  I will present the
single-comparison statistic first and then extend it to multiple
comparisons.  Suppose for now that a researcher is interested in
predicting the target variable $y_{t+1}$ with a vector of regressors
$x_t$; also let $v_t$ be another random process and suppose that
$(y_t, x_t, v_t)$ is stationary and weakly dependent
(Theorem~\ref{res:1} lists the assumptions formally).  In addition,
let $\btrue = (\E x_t x_t')^{-1} \E x_t y_{t+1}$ be the pseudo-true
coefficient of the regression of $y_{t+1}$ on $x_t$ and define
$\varepsilon_{t+1} = y_{t+1} - x_t'\btrue$.  If the linear model is
correctly specified, so $\varepsilon_{t+1}$ is a martingale difference
sequence with respect to $\mathcal{F}_t \equiv \sigma((x_t, v_t, y_t),
(x_{t-1}, v_{t-1}, y_{t-1}),\dots)$, then we can see immediately that
\begin{equation}
  \label{eq:4}
  \oclt{t} \varepsilon_{t+1} (v_t - x_t'\btrue)
\end{equation}
obeys an \mds\ \clt\ (assuming that its variance is uniformly
positive) and is asymptotically normal as $P \to \infty$, with $R$ an
arbitrary starting value\footnote{It will be clear momentarily why the
  summation begins at $R+1$ instead of $1$.} and $P = T - R$.
Straightforward algebra \citep{ClW:07} shows that
\begin{equation}
  \label{eq:5}
  \tfrac{1}{\sqrt{P}} \osum{t} \varepsilon_{t+1} (v_t -
  x_t'\btrue) = \tfrac{1}{2 \sqrt{P}} \osum{t} \Big[(y_{t+1} -
  x_t\btrue)^2 - (y_{t+1} - v_t)^2 + (x_t'\btrue - v_t)^2 \Big].
\end{equation}

\citet{ClW:06,ClW:07} base their \oos\ statistic on the right side
of~\eqref{eq:5}, where $R$ is the size of the training sample and $P$
the size of the test sample used to evaluate the forecasting models.
\citet{ClW:06} use a second forecast $\hat{y}_{t+1}$ as $v_t$.  They
use a rolling window of length $R$ (i.e., $\hat{y}_{t+1}$ is a
function of $y_t, x_{t-1}, z_{t-1} \dots, y_{t-R+1}, x_{t-R}$ and
$z_{t-R}$ where $z_t$ is another weakly dependent random process),
which is kept finite as $T \to \infty$, so $\hat{y}_{t+1}$ inherits
the weak dependence properties of the variables used to estimate it.
Keeping $R$ finite ensures that the conditional variance remains
positive, so the sum obeys a \clt.  This method of ensuring normality
was introduced by \citet{GiW:06}.  In \citet{ClW:06}, the coefficients $\btrue$ are
assumed to be zero so $\varepsilon_{t+1}$ is observed directly (under
the null hypothesis) and Clark and West propose using this statistic
to test the null that $\varepsilon_{t+1}$ is an \mds\ with respect to
$\mathcal{F}_t$.  In \citet{ClW:07}, $\btrue$ is unknown but is
estimated with a fixed-length rolling window as well, so
\begin{equation*}
  \tilde{\beta}_t = \Big(\sum_{s=t-R+1}^t x_{s-1} x_{s-1}'\Big)^{-1}
  \sum_{s=t-R+1}^t x_{s-1} y_s
\end{equation*}
and $\hat{\varepsilon}_{t+1} = y_{t+1} - x_t'\tilde{\beta}_t$ replaces
$\varepsilon_{t+1}$ in the test statistic for \mds.  Unfortunately,
$\hat{\varepsilon}_{t+1}$ is not an \mds\ even when
$\varepsilon_{t+1}$ is, so the statistic is no longer asymptotically
mean-zero normal, even though this approximation performs well in the
simulations reported by \citet{ClW:07}.

This paper proposes using the same basic \oos\ statistic, 
but using a recursive window to estimate $\btrue$ and produce
$\hat{\varepsilon}_{t+1}$; i.e.
\begin{equation}
  \label{eq:8}
  \hat{\beta}_t = \Big(\sum_{s=2}^{t} x_{t-1} x_{t-1}'\Big)^{-1}
  \sum_{s=2}^t x_{t-1} y_t.
\end{equation}
\citepos{Wes:96} Theorem 4.1 implies that
\begin{equation*}
  \oclt{t} \Big[(y_{t+1} -
  x_t\hat{\beta}_t)^2 - (y_{t+1} - v_t)^2 + (x_t'\hat{\beta}_t - v_t)^2 \Big]
\end{equation*}
is asymptotically normal with mean zero under the null for fairly
arbitrary processes $v_t$, as long as $v_t$ is weakly dependent and
the \oos\ statistic has uniformly positive variance.  Just as in
\citet{ClW:06}, these conditions are ensured if $v_t$ is another
forecast of $y_{t+1}$ based on a fixed length rolling window.
Theorem~\ref{res:1} presents the details of this result.  The
assumptions required are essentially the same in existing papers
(e.g. \citealp{Wes:96}; \citealp{WeM:98}; \citealp{Mcc:00};
\citealp{GiW:06}; and \citealp{ClW:06,ClW:07}).  Please see the
original papers for a discussion of these assumptions.

\begin{thm}\label{res:1}
  Suppose that we have two models $\hat{y}_{0t}$ and $\hat{y}_{1t}$ to
  forecast the variable $y_t$, and have observations for
  $t=1,\dots,T+1$.  Assume the following hold:
  \begin{enumerate}
  \item \label{item:1} The benchmark forecast $\hat{y}_{0,t+1}$, is
    estimated using \ols\ with a recursive window:
    $\hat{y}_{0,t+1} = x_t'\hat{\beta}_t$ for some vector of
    predictors $x_t$ with $\hat{\beta}_t$ given by
    Equation~\eqref{eq:8}.  Also define $\btrue = (\E x_{t-1}
    x_{t-1}')^{-1} \E x_{t-1} y_t$ and assume that $\btrue$ does not depend
    on~$t$.
  \item \label{item:2} The alternative forecast, $\hat{y}_{1t}$, is
    estimated using a rolling window of fixed length $R$ (which is
    less than $T$), so $\hat{y}_{1,t+1} =
    \psi(y_t,z_t,\dots,y_{t-R+1}, z_{t-R+1})$ where $\psi$ is a known
    function and $z_t$ is a sequence of predictors that may include
    $x_t$.
  \item \label{item:3} The series $y_t$, $\hat{y}_{1t}$, and $x_t$
    have uniformly bounded $2 r$ moments for some $r > 2$, and $\hat
    y_t y_t$ $\hat y_t x_{t-1}$, $y_t x_{t-1}$, and $x_{t-1},
    x_{t-1}'$ are $L_2$-\ned\ of size $-\frac12$ on a strong mixing
    series of size $-\frac{r}{r-2}$ for $r>2$ or a uniform mixing
    series of size $-\frac{r}{2r-2}$.
  \item \label{item:4} Define \[f_t(\beta) = (y_{t+1} - x_t'\beta)^2 -
    (y_{t+1} - \hat{y}_{1,t+1})^2 + (x_t'\beta - \hat{y}_{1,t+1})^2,\]
    $f_t = f_t(\beta_0)$, $\hat f_t = f_t(\hat\beta_t)$,
    $\bar{f}(\beta) = \oavg{t} f_t(\beta)$, and $\bar f
    = \tfrac1P \sum_{t=R+1}^{T} \hat f_t$, with $P = T - R$;
    $\bar f(\btrue)$ has uniformly positive and finite long run
    variance.
  \end{enumerate}
  Under the null hypothesis that $y_t - \hat{y}_{0t}(\btrue)$ is a
  martingale difference sequence with respect to the filtration
  $\mathcal{F}_t = \sigma((y_t, z_{t}), (y_{t-1}, z_{t-1}),\dots)$,
  $\tfrac{\sqrt{P}}{\hat\sigma} \bar f \to^d N(0,1)$, where 
  \begin{align*}
    \hat{\sigma}^2 &= \hat{S}_{ff} + 2 \Pi (\hat{S}_{fg} + \hat{S}_{gg}), &
    \hat{S}_{ff} &= \oavg{t} (\hat f_t - \bar f)^2, \\
    \hat{S}_{fg} &= \oavg{t} (\hat f_t - \bar{f})(\hat g_t - \bar g)', &
    \hat{S}_{gg} &= \oavg{t} (\hat g_t - \bar g)(\hat g_t - \bar g)',
  \end{align*}
  $\Pi = 1 - \tfrac{R}{P} \log(1 + \tfrac{P}{R})$, $\bar{g} = \oavg{t}
  \hat g_t$, $\X' = [x_1,\dots,x_T]$,
  \begin{equation*}
    g_t(\beta) =
    \Big\{\tfrac{2}{P}\sum_{s=R}^T x_s (x_s'\beta - \hat{y}_{1,s+1}) \Big\}'
    \big(\tfrac1T \X'\X \big)^{-1} x_t(y_{t+1} - x_t'\beta),
  \end{equation*}
  and $\hat g_t = g_t(\hat\beta_t)$.
\end{thm}

The following remarks are relevant to Theorem~\ref{res:1}:

\begin{rem}
  Forecasters are usually interested in the one-sided alternative that
  $\E f_t > 0$; i.e. that the alternative model is expected
  to forecast better than the benchmark.
\end{rem}

\begin{rem}
  These results are presented for one-period-ahead forecasting for
  simplicity.  They can be extended to forecasting at a longer horizon
  by appropriately modifying the variance-covariance matrix to account
  for the correlation structure of the forecast errors. (i.e. for
  $\tau$-step-ahead forecasts the errors will be an \ma($\tau-1$) process).
\end{rem}

\begin{rem}
  The requirement that the asymptotic variance of $\bar f(\btrue)$ is
  uniformly positive is much less restrictive than in \cite{Wes:96}.
  As in \cite{GiW:06} and \citet{ClW:06,ClW:07}, the assumption only
  serves to rule out pathological cases---for example, letting both
  the benchmark and the alternative model be perfectly correlated white noise. In
  \citet{Wes:96}, this assumption is a restriction on the \dgp\ as
  well as the forecasting models, but in this paper it is a
  restriction only on the models.
\end{rem}

\begin{rem}
  As in \citet{Wes:96}, $\Pi \to 1$ as $T \to \infty$ since $R$ is
  fixed.  But using West's general formula for $\Pi$ (which holds when
  $\lim \tfrac{P}{R} \in [0,\infty]$) can improve the statistic in practice,
  since researchers often invoke ``fixed $R$'' asymptotics when $R$ is
  large.  Section~\ref{sec:2} shows via Monte Carlo that this
  approximation can be accurate even when $R$ and $P$ are equal.
\end{rem}

\begin{rem}
  The statistic we present tests the null hypothesis that the forecast
  errors from the population version of the benchmark model are a
  martingale difference sequence.  This hypothesis may not be
  appropriate, depending on the loss function or utility function of
  interest.  If one wants to test the less restrictive null hypothesis that
  $y_{t} - \hat{y}_{0t}$ is uncorrelated with $\hat{y}_{1t}$ but not
  necessarily an \mds, one can replace $\hat{S}_{ff}$, $\hat{S}_{fg}$
  and $\hat{S}_{gg}$ with their \hac\ counterparts.  
  Our statistic can also be modified to test implications of
  optimal forecasts under other loss functions
  \citep[see][]{PaT:07,PaT:07b}; the statistic should be expressed as
  a forecast encompassing test using the models' generalized forecast
  errors.\footnote{See \citet{HLN:98} and \citet[Section~4]{ClW:07}.}
  Again, Lemma~\ref{res:a5} can cover these other applications.
\end{rem}

\begin{rem}
  \citet{GiW:06} and \citet{ClW:06,ClW:07} emphasize model comparison
  using a rolling window but claim in passing that their results hold
  for a fixed-length fixed window as well, where the unknown
  coefficients are estimated only once using the first $R$
  observations.  This claim is true but not obvious.  The proof for
  the rolling window is based on weak dependence that arises because
  the forecast errors are a function of a finite number of consecutive
  weakly dependent observations.  But using a fixed window introduces
  another source of dependence: all of the forecasts depend on the
  same coefficient estimates and the estimation uncertainty introduced
  by those estimates does not vanish asymptotically when $R$ is bounded.  One can
  prove that their results apply to fixed window forecasts by using a
  coupling argument similar to \citepos{Cal:11}.  Interested readers
  should refer to \citet{Cal:11} for the argument and to
  \citet{MeP:02} for a review of the necessary coupling results.
  Theorem~\ref{res:1} can be shown to hold when $\hat y_{1t}$ is
  estimated with a fixed-length fixed window using the same arguments.
\end{rem}

Since most empirical papers study more than one alternative model,
Theorem~\ref{sec:1} is of limited use on its own.\footnote{Moreover,
  we would need to account for the existence of multiple models even if
each paper only considered a single model, since there are many papers
studying the same data.}  It can, however, be
used as the basis for a procedure that allows researchers to make
multiple comparisons.  Before presenting a theoretical result, I will
discuss some issues related to multiple hypothesis testing.

Suppose we have the family of hypotheses, $H_1,\dots,H_J$.  It is
clear that testing each hypothesis individually will often have a
probability greater than the tests' nominal size of rejecting at least
once.\footnote{Unless the statistics used for each test are completely
  interdependent, the probability will be strictly greater than the
  nominal size.}  Econometricians, e.g. \citet{Whi:00},
\citet{Han:05}, \cite{HuW:10}, and \citet{ClM:12b}, have emphasized
statistics that test families of hypotheses that control the
probability any hypothesis is rejected given that they all are true,
known as \textit{weak control of familywise error}
(\wfwe).\footnote{An exception is \citet{HHK:10} who combine
  \citepos{RoW:05} StepM with \citepos{Han:05} threshold rule to
  control \sfwe\ for certain families of one-sided tests.}  This paper
focuses instead on controlling the probability that at least one true
hypothesis is rejected given any combination of true and false
hypotheses, known as \textit{strong control of familywise error}
(\sfwe).  For most empirical work, \sfwe\ is desired; fortunately,
even though these papers only prove \wfwe, statistics based on
nonparametric block bootstraps \citep{Whi:00,Han:05} can be shown to
control \sfwe\ as well \citep[this follows directly from][]{RoW:05}
and those based on the parametric bootstrap \citep{ClM:12b} essentially
control \sfwe\ if the individual null hypotheses are strengthened
slightly as below (see conclusion~\ref{it:1} of Theorem~\ref{res:2} as
well as Remark~\ref{rem:2}).  \citet{HuW:10} propose that researchers
construct critical values from the asymptotic joint distribution of
the test statistics and this procedure can control \sfwe\ as long as there is a
consistent estimator of that distribution.

Only tests with \sfwe\ can tell researchers which of the hypotheses
are false.  To see why, imagine that there are only two hypotheses:
$H_1$ is true and $H_2$ is false.  A test that rejects the true
hypothesis, $H_1$, with arbitrarily high probability still can satisfy
\wfwe: since not all of the hypotheses are true, the behavior of tests
with weak control is essentially unconstrained.  Such a test would not
satisfy \sfwe, though.  For \sfwe\ in this setting, a test must reject
$H_1$ with probability at most $\alpha$ (letting $\alpha$ be the
desired level of control).  In most research, we would view rejection
of the true hypothesis $H_1$ as a mistake.


This paper uses \citepos{RoW:05} StepM procedure, which is an iterated
extension of \citepos{Whi:00} Reality Check designed to reject as many
false hypotheses as possible while achieving \sfwe.  Their method
relies on the bootstrap to estimate the joint dependence between the
test statistic associated with each model.  The bootstrap used in this
paper is new, and is outlined in Algorithm~\ref{alg:1}.
Algorithm~\ref{alg:1} actually presents a more general version of the
bootstrap that remains valid with a misspecified benchmark model.
Imposing the null hypothesis of \mds\ leads to simpler version that
will be discussed later.
\begin{alg}\label{alg:1}
  Suppose that there are $m$ alternative forecasting models,
  $\hat{y}_{1t},\dots, \hat{y}_{mt}$, and define the data matrices
  $\X_\R = (X_1,\dots,X_R)'$ and $\X_{\p} = (X_{R+1},\dots,X_{T-1})$
  with
  \begin{equation*}
    X_t = \begin{cases}
      (y_{t+1}, x_t')' & t \leq R \\
      (y_{t+1}, x_t', \hat{y}_{1,t+1}, \dots, \hat{y}_{m,t+1})' & t > R.
    \end{cases}
  \end{equation*}
  \begin{enumerate}
  \item Draw $B$ samples of $P$ observations from $\X_{\p}$ using the
    moving or circular blocks bootstrap with block length $b$ or the
    stationary bootstrap with geometric block lengths with success
    probability $p$.  Denote each sample as $\X_{\p, l}^{*}$ and let
    $\X_l^{*} = [\X_{\R}', \X_{\p,l}^{*\prime}]'$.
  \item Estimate $\bar{f}^{*}_{li}$ and $\hat{\sigma}_{li}^{*}$ as in
    Theorem~\ref{res:1} for each bootstrap sample, $\X^{*}_l$, and each
    alternative model, $\hat{y}_{it}$.
  \end{enumerate}
\end{alg}

This procedure exploits the fact that $R$ is finite in
Theorem~\ref{res:1} and that the expected block length will grow with
$T$.  Under these asymptotics, a growing proportion of the
rolling-window forecasts are the same if we bootstrap the forecasts as
if we bootstrap the empirical data and reestimate the models.
Bootstrapping the forecasts makes the computations faster and easier,
but requires us to drop the first $R$ observations from the bootstrap
(the alternative forecasts are not defined for the first $R$
observations).  I add those observations to the beginning of each
bootstrap sample so that population means under the bootstrap-induced
probability distribution equal sample means.\footnote{This equality is
  strictly true only for the stationary and circular block
  bootstraps.}  The benchmark model still needs to be reestimated in
each bootstrap sample.

Theorem~\ref{res:2} presents the final procedure.

\begin{thm}\label{res:2}
  Suppose the conditions of Theorem~\ref{res:1} hold, with assumptions
  on the alternative model, $\hat{y}_{1t}$, holding for each of the
  $m$ models $\hat{y}_{1t}, \dots, \hat{y}_{mt}$ and let the subscript
  $l$ denote the different quantities associated with $\hat{y}_{lt}$
  (i.e. $\mathcal{F}_{lt}$, etc.)  Also assume that the long-run
  variance-covariance matrix of the vector
  $(\bar{f}_1(\btrue),\dots,\bar{f}_m(\btrue))'$ is uniformly
  positive definite.

  Let the $*$-superscript denote a random variable with the
  distribution induced by the bootstrap of Algorithm~\ref{alg:1} and
  consider the following procedure:
  \begin{enumerate}
  \item Define $M_0 = \emptyset$ and, for $j = 1,\dots,m$, let $M_j =
    \{l : \tfrac1{\hat\sigma_l} \bar{f}_{l} > \hat{d}_j\}$, where
    $\hat{d}_j$ is the $1-\alpha$ quantile of $\max_{i \notin M_{j-1}}
    \tfrac{1}{\hat{\sigma}_i^{*}}(\bar{f}_{i}^{*} -
    \bar{f_i}(\btrue[*]))$.
  \item Reject all of the models $l$ with $l \in \bigcup_{j=1}^m M_j$.
  \end{enumerate}
  The following conclusions hold:
  \begin{enumerate}
  \item\label{it:1} For each $G \subset \{1,\dots,m\}$, let
    $\mathcal{F}_t^G$ be the smallest $\sigma$-field containing
    $\mathcal{F}_{lt}$ for all $l \in G$.  Under the null hypothesis
    that $y_t - \hat{y}_{0t}$ is an \mds\ with respect to
    $\mathcal{F}_t^I$ for some $I \subset \{1,\dots,m\}$, the
    procedure outlined above, but using the moving block bootstrap
    with block length $1$, has probability at most $\alpha$ of
    rejecting one or more models in $I$.
  \item\label{it:2} Let $H_l$ be the null hypothesis that $y_t -
    \hat{y}_{0t}$ is an \mds\ with respect to
    $\mathcal{F}_{lt}$ against the one-sided alternative $\E \bar
    f_i(\btrue) > 0$.  If $p \to 0$ and $P p \to \infty$ as $T \to
    \infty$ (for the stationary bootstrap) or $b \to \infty$ and
    $\frac{b}{P} \to 0$ (for the moving or circular block bootstraps)
    then the procedure outlined above controls \sfwe\ at level
    $\alpha$ for the family of null hypotheses $H_1,\dots,H_m$.
  \item\label{it:3} Let $H_l$ be the null hypothesis that $y_t -
    \hat{y}_{0t}$ is uncorrelated with $\hat{y}_{lt}$ against the
    one-sided alternative $\E \bar f_i(\btrue) > 0$.  If $p \to 0$
    and $P p \to \infty$ as $T \to \infty$ (for the stationary
    bootstrap) or $b \to \infty$ and $\frac{b}{P} \to 0$ (for the
    moving or circular block bootstraps) then the procedure outlined
    above controls \sfwe\ at level $\alpha$ for the family of
    null hypotheses $H_1,\dots,H_m$.
  \end{enumerate}
\end{thm}

The following remarks apply to Theorem~\ref{res:2}.

\begin{rem}
  The next section will discuss the block bootstrap procedures for
  \oos\ comparisons in detail.  Note that $\bar f(\btrue[*])$ is the
  bootstrap equivalent of $\E f_t(\btrue)$, hence its role as the
  centering term in step 1 of the procedure.
\end{rem}

\begin{rem}
It is worth emphasizing that this procedure is very easy to use under
the null hypothesis of correct specification (conclusion~\ref{it:1}).  The
researcher can use a standard estimate of the statistic's asymptotic
variance (not a \hac\ estimator) and an i.i.d. bootstrap.  Even
better, the alternative forecasts only need to be estimated once,
before the bootstrap.
\end{rem}

\begin{rem}\label{rem:2}
  The key difference between conclusions~\ref{it:1} and~\ref{it:2} is
  that, in conclusion~\ref{it:2}, the forecast error is an \mds\ with
  respect to several individual series, but not necessarily with
  respect to the pooled information set generated by all of those
  series together.\footnote{It is straightforward to construct three
    series, $u_t$, $v_t$, and $w_t$, such that $w_t$ is an \mds\ with
    respect to $\sigma((u_t, w_t), (u_{t-1}, w_{t-1}),\dots)$ and
    $\sigma((v_t, w_t), (v_{t-1}, w_{t-1}),\dots)$ but not
    $\sigma((u_t, v_t, w_t), (u_{t-1}, v_{t-1}, w_{t-1}),\dots)$.
    This is essentially the scenario imposed in conclusion~\ref{it:2}.}
  Using $b = 1$ would consistently estimate the marginal distribution
  of each $\tfrac1{\sigma_i} \bar{f}_i$ under this weaker null
  hypothesis, but would not necessarily estimate the joint
  distribution of two or more of those terms correctly (in particular,
  the covariance could be wrong).  The stronger null hypothesis
  imposed in conclusion~\ref{it:1} ensures that resampling with $b = 1$
  estimates the joint distribution correctly as well.

  The same reasoning implies that a parametric bootstrap
  \citep[i.e.][]{ClM:12b} achieves control of \sfwe\ only under the
  stronger hypotheses of conclusion~\ref{it:1}.  Fortunately,
  conclusion~\ref{it:1} seems to capture the goals of most empirical work,
  so researchers can make use of the considerable simplifications that
  occur in that setting.
\end{rem}

\begin{rem}
  If calculating $\hat{\sigma}_i$ is burdensome, one can do the same
  bootstrap without studentizing the statistics.  This procedure is
  likely to have worse size and power properties, but may be more
  convenient.  One could also estimate the variance with a second
  bootstrap step, but execution might take too long to be practical,
  or use a convenient but inconsistent approximation of the variance.
  The estimator of the variance in conclusion~\ref{it:3} should have the
  best performance if it is \hac\ \cite[the results of][may be
  relevant]{GoK:96}, but that is not necessary for validity.
  See \citet{Han:05} and \citet[Section~4.2]{RoW:05} for a discussion
  of the benefits of studentization.
\end{rem}

\begin{rem}
  If the number of alternative models is large, controlling the \fwe\
  may be too strict a criterion to be useful.  \citet{RSW:08} discuss procedures to
  control criteria other than the \fwe, and one can use their
  generalizations of the StepM procedure here as well.
\end{rem}

\begin{rem}\label{rem:01}
  \citet{Pin:11} raises the issue that, since there are often several
  models that could be used as the benchmark, researchers may want to
  control for data snooping over the null hypotheses as well as the
  alternatives.  For example, one could use either \citepos{AtO:01}
  random walk or \citepos{StW:07} \textsc{ima}(1,1) as benchmark
  inflation models, so it might make sense to require an alternative
  model to outperform them both.\footnote{\citet{Pin:11} studies tests
    that reject if the alternative model outperforms either of the
    models, so the details are somewhat different in our papers.}
  Allowing multiple benchmark models can be expressed as an
  Intersection-Union test, so Theorem~\ref{res:2}'s procedure
  (conclusion~\ref{it:3}) easily accommodates this extension by taking
  each $\bar f_i$ and $\hat{\sigma}_{i}^{2}$ to be a random
  $q$-vector;\footnote{If a researcher uses multiple benchmark models,
    it would be a mistake to assume that they are all correctly
    specified, so only the null hypothesis of conclusion~\ref{it:3} is
    appropriate.} the first element of $\bar f_{i}$, written as $\bar
  f_{i1}$, compares the first benchmark model to the $i$th alternative
  and the first element of $\hat{\sigma}_i^2$ is the corresponding
  estimate of asymptotic variance, the second element corresponds to
  the second benchmark model, etc.

  Steps 1 and 2 of Theorem~\ref{res:2}'s procedure now become:
  \begin{enumerate}
  \item Define $M_0 = \emptyset$ and, for $j = 1,\dots,m$, let $M_j =
    \{(i,k): \tfrac1{\hat\sigma_{ik}} \bar{f}_{ik} > \hat{d}_j\}$,
    where $\hat{d}_j$ is the $1 - \alpha$ quantile of $\max_{(i,k)
      \notin M_{j-1}} \tfrac1{\hat\sigma_{ik}^*} \big(\bar{f}_{ik}^{*} -
    \bar{f}_{ik}(\hat{\beta}_{k,T+1})\big)$.
  \item Reject all of the hypotheses $H_i$ such that $(i,k) \in
    \bigcup_{j=1}^m M_j$ for all $k$.
  \end{enumerate}
  Also note that the asymptotic variance-covariance matrix of each
  $\bar f_i$ does not need to be positive definite as long as the
  variance of each of its elements is positive.  A formal statement of
  this result is given in Appendix~\ref{sec:B}.
\end{rem}

\section{The Bootstrap for Out-of-Sample Statistics}\label{sec:1b}
The validity of the bootstrap in Theorem~\ref{res:2} is a special case
of a result of independent interest---the validity in general of block
bootstraps for asymptotically normal \oos\ statistics.  This section
presents the general result.  In this section, let $\to^{p^{*}}$ and
$\to^{d^{*}}$ refer to convergence in probability or distribution
conditional on the observed data.  Similarly, $\E^{*}$, $\var^{*}$,
and $\cov^{*}$ refer to the expectation, variance, and covariance with
respect to the probability measure induced by the bootstrap, and
$y_t^{*}$, etc. is the random variable $y_t$ but under the
bootstrap-induced \cdf.

The notation in this section is more general than that of
Section~\ref{sec:1}.  The assumptions required are generalizations of
the conditions of Theorems~\ref{res:1} and~\ref{res:2}.  See
\citet{Wes:96,Wes:06}, \citet{WeM:98}, and \citet{Mcc:00} for a
discussion of these conditions, as theirs are nearly identical.
Theorem~\ref{res:3} gives the result.

\begin{thm}\label{res:3}
  Suppose the following conditions hold:
  \begin{enumerate}
  \item The estimator $\hat{\beta}_t$ of $\btrue$ is estimated with a
    recursive, rolling, or fixed estimation window and satisfies
    $\hat{\beta}_{t} - \beta_{0} = \hat{B}_{t} H_t$; $\hat{B}_{t}$ is
    a sequence of $k \times q$ matrices such that $\sup_t |\hat{B}_t -
    B| \to^p 0$; $H_{t}$ is a sequence of $q$-vectors such that
    \begin{equation}
      H_{t} = \begin{cases} 
        \tfrac1t \sum_{s=1}^t h_{s}(\btrue) & \text{recursive window} \\
        \tfrac1R \sum_{s=t-R+1}^t h_{s}(\btrue) & \text{rolling window} \\
        \tfrac1R \sum_{s=1}^R h_{s}(\btrue) & \text{fixed window} \\
      \end{cases}
    \end{equation}
    and $\E h_{s}(\btrue) = 0$ for all $s$.
  \item Define $\psi_t(\beta) = (f_t(\beta), h_t(\beta))$;
    $\psi_t(\beta)$ is covariance stationary and twice continuously
    differentiable in an open neighborhood $N$ of $\btrue$.  Also,
    $\E \sup_{\beta \in N} |\psi_t(\beta)|^r$ and $\E\sup_{\beta \in
      N} |\frac{\partial}{\partial \beta} \psi_t(\beta)|^r$ are
    uniformly bounded, and there exists a sequence of random variables
    $m_t$ such that $\sup_{i,\beta \in N} |\tfrac{\partial^2}{\partial
      \beta \partial \beta'} \psi_{it}(\beta)| \leq m_t$ almost surely
    and $\E m_t^2$ is uniformly finite.
  \item Both $\psi_t(\beta)$ and $\frac{\partial}{\partial \beta}
    \psi_t(\beta)$ are $L_2$-\ned\ of size $-\frac12$ on the
    series $V_t$ with bounded \ned-coefficients for $\beta$ uniformly
    in $N$; $V_t$ is strong mixing of size $-\frac{r}{r-2}$ or uniform
    mixing of size $-\frac{r}{2r-2}$ with $r > 2$.\footnote{For
      uniform mixing processes, $r = 2$ is also allowed as long as
      $\sup_{\beta \in N} \psi_t(\beta)^2$ and $\sup_{\beta \in N}
      (\partial \psi_t(\beta) / \partial \beta)^2$ are uniformly
      integrable.}
  \item The sequence $\hat f_1^{*},\dots,\hat f_T^{*}$ is constructed
    using the same procedure as the original \oos\ analysis over the
    bootstrap sample, where the bootstrap sample is generated by
    moving blocks, circular blocks, or stationary bootstrap with block
    lengths drawn from the geometric distribution.  The (expected)
    block length $b$ satisfies $b \to \infty$ and $\frac{b}{P} \to 0$.
  \item The bootstrapped estimator satisfies $\sup_t |\hat{B}_t^{*} -
    B^{*}| \to^p 0$, $B^{*} = B + o_p$ and $\frac1T \sum_{t=1}^T
    h_t^*(\btrue[*]) = 0$ a.s.
  \item The asymptotic variance matrix of $\bar{f}(\btrue)$ is
    uniformly positive definite.
  \item $R, P \to \infty$ as $T \to \infty$.
  \end{enumerate}
  Then
  \begin{equation}
    \pr[\sup_x | \pr^*[\sqrt{P} (\bar{f}^* - \bar{f}(\btrue[*]))
        \leq x] - \pr[\sqrt{P}( \bar{f} - \E \bar{f}(\btrue)) \leq x] | >
      \epsilon] \to 0
  \end{equation}
  for all $\epsilon > 0$.
\end{thm}

\begin{rem}
  \citet{Mcc:00} proves asymptotic normality under weaker smoothness
  conditions on $f_t(\beta)$ and $h_t(\beta)$: only their expectations
  must be continuously differentiable.  It may be possible to extend
  Theorem~\ref{res:3} to those weaker conditions, but the current
  proof relies on a theorem of \citepos{JoD:00} establishing the
  consistency of \hac\ estimators and their theorem requires differentiability of
  the observations.  Extending \citepos{JoD:00} result to
  nondifferentiable functions should be possible, but is beyond the
  scope of this paper.
\end{rem}

\begin{rem}
  \citet{Whi:00} and \citet{Han:05} resample the forecasts but do not
  reestimate any of them which requires the additional assumption that
  $\tfrac{P}{R} \log \log R \to 0$ or that the forecasts themselves
  have no estimated parameters.\footnote{\citet{Whi:00} lists several
    different sets of assumptions that give the same result, but these
    seem to be the most general.}
\end{rem}

\begin{rem}
  \citet{CoS:07} use the distribution of $\sqrt{P}(\bar{f}^{*} -
  \bar{f})$ to approximate that of $\sqrt{P}(\bar{f} - \E
  \bar{f}(\btrue))$.  But it is clear that $\bar{f}(\btrue[*])$
  is the bootstrap analogue of $\E \bar{f}(\btrue)$, the parameter of
  interest.  Because their bootstrap is miscentered, \citet{CoS:07}
  must redefine $\hat{\beta}_t^{*}$ to achieve consistency.  In this
  paper, though, consistency arises naturally.
\end{rem}

\begin{rem}
  It may be unnecessary to assume that $\bar{f}(\btrue)$ has positive
  definite asymptotic variance; if so, the bootstrap would work in the
  setup of \citet{ClM:05,ClM:01} and \citet{Mcc:07}.  That question is
  left to future research.
\end{rem}

\section{Monte Carlo Results}\label{sec:2}
This section presents Monte Carlo experiments demonstrating that
this paper's modified version of \citepos{ClW:07} statistic performs
similarly to their original test in the situations they study, but can
have substantially higher power when the \dgp\ has a structural
break.\footnote{All of these simulations were programmed in R
  \citep[version 2.14.0]{R} and use the \textsc{MASS}
  \citep[7.3-22]{VeR:02}, xtable \citep[version 1.6-0]{Dah:09}, and
  dbframe \citep[version 0.2.7]{Cal:10b} packages.}

The \dgp\ has three different parametrizations: one to study the
tests' size, one to study power under stationarity, and one to study
power if there is a single break in the relationship between the
target and predictors.  The \dgp\ is:
\begin{align*}
  y_t &= \gamma_{1t} + \gamma_{2t} z_{t-1} + e_t &
  \gamma_t &=
  \begin{cases}
    (0.5, 0)    & \text{size simulations} \\
    (0.5, 0.35) & \text{power (stable)} \\
    (-0.5, 0)    & t \leq \tfrac{T}{2} \quad \text{power (break)} \\
    (1, 0.35) & t > \tfrac{T}{2} \quad \text{power (break)}
  \end{cases}\\\nonumber
  z_t &= 0.15 + 0.95 z_{t-1} + v_t &
  (e_t, v_t)' &\sim iid\ N\Bigg(\begin{pmatrix} 0 \\ 0
  \end{pmatrix}
   , \begin{pmatrix} 18 & -
    0.5 \\ -0.5 & 0.025 \end{pmatrix}\Bigg)
  \\ R &= 120, 240 & P &= 120, 240, 360, 720.
\end{align*}
Both models are estimated by \ols. The benchmark model regresses $y_t$
on a constant, and the alternative regresses $y_t$ on a constant and
$z_{t-1}$.  \citet{ClW:07} argue that this \dgp\ mimics an asset
pricing application similar to \citepos{GoW:08} which we study in
Section~\ref{sec:3}.

For comparison, we study this paper's new statistic as well as \poscw
rolling-window and recursive-window test statistics.  Clark and West
only prove that their rolling-window statistic is asymptotically
normal, and only then if the benchmark model is not estimated, but
their recursive-window statistic is popular in practice and in
simulations tends to perform similarly to their rolling window test.
We use all three of these statistics to test the null that the
benchmark model's innovation is an \mds.\footnote{\citet{ClW:07}
  report the performance of the tests proposed by \citet{CCS:01} and
  \citet{ClM:05} as well, and of tests based on the naive Gaussian
  statistic.}

\begin{table}[tb]
  \centering
  \input{tex/mc1}
  \caption{Size and power of the \oos\ tests in the simulations 
    described by Section~\ref{sec:2}, at
    \testsize\% confidence.  These percentages are calculated from \totalsims\
    samples.  Pr[\textsc{cw} roll.] shows the fraction of simulations for
    which Clark and West's (2007) rolling-window statistic rejects; 
    Pr[\textsc{cw} rec.] shows the fraction of simulations for which 
    their recursive-window statistic rejects; and Pr[new] shows the fraction of
    simulations for which this paper's test rejects.}
\label{tab:mc1}
\end{table}

Table~\ref{tab:mc1} presents the simulation results.  For all of the
stable parameter values, the proposed new statistic has similar
rejection probability to \citepos{ClW:07}.  Both of Clark and West's
tests are generally slightly undersized relative to our new test,
which is itself slightly undersized: when $R$ is 120 and $P$ is 360
our test statistic has size 7.6\% and Clark and West's rolling and
recursive window tests have size 7.5\% and 6.2\% respectively, at a
nominal size of 10\%.  For the stable alternative, our new statistic
typically has slightly higher power than Clark and West's rolling
window and lower power than their recursive window.  For example, when
$R$ is 120 and $P$ is 720, the rolling-window test rejects at 66.8\%,
our statistic at 73.0\%, and the recrusive window statistic at 82.3\%,
again for a nominal size of 10\%.  In general, the statistics perform
similarly under stability.

For the simulations with a single break, the new statistic has
considerably higher power than \poscw\ original tests across all of
the choices of $R$ and $P$; the rejection probability is more than
twice as large for most parametrizations.  When $R$ is 120 and $P$ is
360 with a nominal size of 10\%, for example, the new statistic
rejects at 96.4\% while the rolling and recursive window statistics
reject at 35.5\% and 32.9\% respectively.  Results for other choices
of nominal size and sample split give similar results.  So mixing
window strategies can give a large power advantage when testing for
time-varying predictability, and performs similarly to the original
test when testing for stable outperformance.

\section{Empirical Illustration}\label{sec:3}

This section demonstrates the use of our new statistic by revisiting
\citepos{GoW:08} study of excess stock returns.  Goyal and Welch argue
that many variables thought to predict excess returns (measured as the
difference between the yearly log return of the S\&P 500 index and the
T-bill interest rate) on the basis of in-sample evidence fail to do so
out-of-sample.  To show this, Goyal and Welch look at the forecasting
performance of models using a lag of the variable of interest, and
show that these models do not significantly outperform the excess
return's recursive sample mean.

Here, I conduct the same analysis, but using this paper's \mds\ test.
The benchmark model is the excess return's sample mean (as in the
original) and the alternative models are of the form
\[\text{excess return}_{t} = \alpha_{0} + \alpha_{1}\ 
\text{predictor}_{t-1} + \varepsilon_{t},\] where $\alpha_{0}$ and
$\alpha_{1}$ are estimated by \ols\ using a \windowlength-year window.
The predictors used are listed in the ``predictor'' column of
Table~\ref{tab:em1} \citep[see][for a detailed description of the
variables]{GoW:08}.  We also consider \citepos{CaT:08} proposed
correction to the models, that the forecasts be bounded below by zero
since negative forecasts are incredible, as well as two simple
combination forecasts, the mean and the median (over both the original
and the non-negative forecasts).  The data set is annual data
beginning in 1927 and ending in 2009, and the rolling window uses
\windowlength\ observations.\footnote{This statistical analysis was
  conducted in R \citep{R} using the xtable
  \citep[version~1.6-0]{Dah:09}, and dbframe \citep[version
  0.2.7]{Cal:10b} packages.}

Table~\ref{tab:em1} presents the results for each model.  The column
``value'' gives the value of the test statistic for each model, while
the ``naive'' and ``corrected'' columns indicate whether the statistic
is greater than the standard size-\bootsize\% critical value (1.28)
and the critical value estimated by the procedure of
Theorem~\ref{res:2} (\empiricalcriticalvalue).\footnote{The bootstrap
  uses \nboot\ replications with i.i.d. sampling, as proposed in
  conclusion~\ref{it:1} of Theorem~\ref{res:2}.}  Three predictors are
significant at the naive critical values for both the original and
bounded forecasts: the dividend yield, long term interest rate, and
book to market ratio.  But none are significant after accounting for
data snooping, which highlights the importance of these methods.  The
median forecast is significant using conventional critical values as
well, but not the corrected values.

\begin{table}[tb!]
  \centering
  \empiricaltable
\caption{Results from \oos\ comparison of equity premium prediction
  models; the benchmark is the recursive sample mean of the equity
  premium and each alternative model is a constant and single lag of
  the variable listed in the ``predictor'' column.  The dataset begins
  in 1927 and ends in 2009 and is annual data. The ``value'' column
  lists the value of this paper's \oos\ statistic, the ``naive''
  column indicates whether the statistic is significant at standard
  critical values, and the ``corrected'' column indicates significance
  using the critical values proposed in Theorem~\ref{res:2} that
  account for the number of models.  See Section~\ref{sec:3} for details.}
\label{tab:em1}
\end{table}


\section{Conclusion}\label{sec:4}
This paper presents an \oos\ test statistic similar to \poscw\ that is
asymptotically normal when comparing nested or non-nested models.
Normality is achieved by estimating the alternative model using a
fixed-length rolling window---as do Clark and West---but estimating
the benchmark model with a recursive window.  Simulations indicate
that the new statistic behaves similarly to Clark and West's original
test when the \dgp\ is stable but can have much higher power when the 
\dgp\ has structural breaks.  The paper also presents a
method for comparing the benchmark model to several alternative models
simultaneously and improves block bootstrap procedures of \oos\
statistics.

\appendix
\section{Proofs of Main Theoretical Results}\label{sec:B}

Define the additional notation 
$F_t(\beta) = \tfrac{\partial}{\partial \beta} f_t(\beta)$,
$F_t = F_t(\btrue)$,
$\hat f_t^* = f_t^*(\hat\beta_t^*)$,
$F_t^*(\beta) = \tfrac{\partial}{\partial \beta} f_t^*(\beta)$,
$F_t^* = F_t^*(\btrue[*])$,
$h_t = h_t(\btrue)$,
$h_t^* = h_t^*(\btrue[*])$, 
and
\begin{equation*}
      H_{t}^* = \begin{cases} 
        \tfrac1t \sum_{s=1}^t h_{s}^* & \text{recursive window} \\
        \tfrac1R \sum_{s=t-R+1}^t h_{s}^* & \text{rolling window} \\
        \tfrac1R \sum_{s=1}^R h_{s}^* & \text{fixed window} \\
      \end{cases}
\end{equation*}
    
\begin{proof}[Proof of Theorem \ref{res:1}]
  Replace $R$ with $\log(T)$ and $P$ with $T - \log(T)$ in the
  statistic; this substitution does not affect its asymptotic
  distribution; the theorem is now an immediate consequence of
  Lemma~\ref{res:a5} with $h_t(\beta) = x_t (y_{t+1} - x_t'\beta)$.
  Note that \[f_t(\btrue)= 2 (y_{t+1} -
  x_t'\btrue)(\hat{y}_{1,t+1} - x_t'\btrue) \quad \text{a.s.},\] as
  in \citet{ClW:07}, which is an \mds, so we do not need to use a
  \hac\ estimator of the variance under the null.  Also observe that
  \[F_t(\beta) = 2 x_t(x_t'\beta - \hat y_{1,t+1}) + 2 x_t(x_t'\beta -
  y_{t+1})\] and the second term is an \mds\ under the null,
  explaining the particular form of $g_t(\beta)$.
\end{proof}

\begin{proof}[Proof of Theorem \ref{res:2}]
  Replace $R$ and $P$ as in the proof of Theorem~\ref{res:1}.
  Theorem~\ref{res:3} of this paper and Theorems~3.1 and~4.1 of
  \citet{RoW:05} complete the proof.
\end{proof}

\newcommand{\WesA}[1][]{\oclt{t}
  (F_t^{#1} - \E^{#1} F_t^{#1}) B^{#1} H_t^{#1}}
\newcommand{\WesB}[1][]{\tfrac{1}{\sqrt{P}} \E^{#1} F_t^{#1} \osum{t} (B_t^{#1} -
  B^{#1}) H_t^{#1}}
\newcommand{\WesC}[1][]{\oclt{t}
  (F_t^{#1} - \E^{#1} F_t^{#1}) (B_t^{#1} - B^{#1}) H_t^{#1}}
\begin{proof}[Proof of Theorem~\ref{res:3}]
  Expand $f_t^{*}(\hat{\beta}_t^{*})$ around $\btrue[*]$
  to get
  \begin{align*}
    \sqrt{P} \big(\bar{f}^{*} - \bar{f}(\btrue[*])\big) &= \oclt{t}
    \big(f_t^{*} - \bar{f}(\btrue[*])\big) +
    \E^* F_t^{*} B^{*} \oclt{t} H_t^* \\
    & \quad + \WesA[*] + \WesB[*] \\ & \quad + \WesC[*] + \oclt{t} w_t^*
  \end{align*}
  where (as in \citealp{Wes:96}) the $i$th element of $w_t^*$ is
  \begin{equation*}
    w_{it} = \tfrac12 (\hat\beta_t^* - \hat\beta_{T+1})'
    \Big[\tfrac{\partial^2}{\partial \beta \partial\beta'} 
    f_{it}^*(\tilde\beta_{it}^*) \Big]
    (\hat\beta_t^* - \hat\beta_{T+1})
  \end{equation*}
  and each $\tilde\beta_{it}^*$ lies between $\hat\beta_t^*$ and
  $\hat\beta_{T+1}$.  The argument that $\oclt{t} w_t^* = o_{p^*}(1)$ is
  identical to West's, using our Lemma~\ref{res:a2} in place of his
  Lemma A3.  Then
  \begin{gather}
    \WesA[*] \to^{p^*} 0 \label{eq:7} \\
    \WesB[*] \to^{p^*} 0 \label{eq:9} \\
  \intertext{and}
    \WesC[*] \to^{p^*} 0 \label{eq:10}
  \end{gather}
  from Lemma~\ref{res:a4} and the result follows from
  Lemma~\ref{res:a3}, with the form of the asymptotic variance
  following directly from \citet{Wes:96} and \citet{WeM:98}.
\end{proof}

\begin{lem}[Formalization of Remark~\ref{rem:01}]
  Suppose that the conditions of Theorem~\ref{res:2} conclusion~\ref{it:3}
  hold but $f_i = (f_{i1},\dots,f_{iq})$ and the asymptotic variance
  of $(\bar{f}_{1k}(\beta_k),\dots,\bar{f}_{mk}(\beta_k))$ is
  uniformly positive definite for each $k$.  Then the procedure
  described in Remark~\ref{rem:01} achieves \sfwe.
\end{lem}

The proof is similar to that of Theorem~\ref{res:2}. Note that we
allow $\sqrt{P}(\bar{f}_{i1}(\beta_1),\dots,\bar{f}_{iq}(\beta_q))$ to
converge to a normal with singular variance-covariance matrix as long
as each element has positive variance.

\section{Supporting Results}

\begin{lema}\label{res:a2}
  Suppose $a \in [0,\frac12)$ and the conditions of Theorem~\ref{res:3}
  hold.
  \begin{enumerate}
  \item $P^a \sup_t | H_{t} | \to^p 0$ and $P^a \sup_t | H_{t}^{*} |
    \to^{p^{*}} 0$.
  \item $P^a \sup_t | \hat{\beta}_{t} - \beta_{0} | \to^{p} 0$ and
    $P^a \sup_t | \hat{\beta}^{*}_{t} - \hat{\beta}_T |
    \to^{p^{*}} 0$.
  \item $\tfrac{1}{\sqrt{P}} \sum_{t=R+1}^T (F_t - \E F_t) = O_{p}(1)$
    and $\tfrac{1}{\sqrt{P}} \sum_{t=R+1}^T (F_t^* - \E F_t^*) =
    O_{p^*}(1)$.
  \end{enumerate}
\end{lema}

\begin{proof}[Proof of Lemma~\ref{res:a2}]
  \begin{enumerate}
  \item The process $\tfrac{1}{\sqrt{T}} h_{s}$ satisfies
    \citepos[Theorem 3.1]{JoD:00b} functional \clt.  So
    \begin{equation}
      P^a \sup_t \Big| \tfrac1t \sum_{s=1}^t h_{s} \Big| =
      P^a \sup_{\gamma \in [0,1]} \Big| \tfrac{1}{\lfloor \gamma
        T\rfloor} \sum_{s=1}^{\lfloor \gamma T \rfloor} h_{s} \Big| \to^{p} 0
    \end{equation}
    with the convergence following from the continuous mapping
    theorem.  For the bootstrapped average, \citepos{Cal:13}
    Theorem~2 ensures that
    \begin{equation}
      \tfrac{1}{\sqrt{\gamma
          T}} \sum_{s=1}^{\lfloor \gamma T \rfloor} h_{s}^*
      = O_{p^*}(\Omega^*(\hat\beta_{T+1})^{1/2})
    \end{equation}
    where $\Omega^*(\beta) = \var^* h_t^*(\beta)$.  Since
    $\hat\beta_{T+1} \to^p \btrue$, it suffices to show that
    $\sup_{\beta \in N} | \Omega^*(\beta) - \var h_t(\beta) | \to^p
    0$.  But this convergence holds for any fixed $\beta$ and our
    assumptions guarantee stochastic equicontinuity as in
    \citet{JoD:00} \citep[also see][]{Dav:94}, completing the proof.

  \item We have
    \begin{multline}
      P^a \sup_t | \hat{\beta}_t - \btrue | = P^a \sup_t |\hat{B}_{t}
      H_{t}| \\ \leq \sup_{t,u} \Big| [ \hat{B}_u - B]
      \tfrac{P^a}{t} \sum_{s=1}^t h_{s} \Big| + \sup_{t} \Big|
      B \tfrac{P^a}{t} \sum_{s=1}^t h_{s} \Big|
    \end{multline}
    and both terms converge to zero in (conditional) probability by
    the previous argument and by assumption.  The same argument holds
    for $\hat{\beta}_t^{*} - \btrue[*]$ as well.
  \item $\tfrac{1}{\sqrt{P}} \sum_{t=R+1}^T (F_t - \E F_t)$ obeys
    \citepos{Jon:97} \clt, so the result is trivial.  Also,
   \begin{equation}
     \Big\lvert\tfrac{1}{\sqrt{P}} \sum_{t=R+1}^T 
     ( F_t^* - \E^* F^*)\Big\rvert = O_{p^*}(\Omega^*(\hat\beta_{T+1})^{1/2}) = O_p(1)
    \end{equation}
    as in the proof of part 1, where now $\Omega^*(\beta) = \var^* F_t^*(\beta)$.
  \end{enumerate}
\end{proof}

\begin{lema}\label{res:a4}
  Under the conditions of Theorem~\ref{res:3}, Equations
  \eqref{eq:7}--\eqref{eq:10} and \eqref{eq:11}--\eqref{eq:13} hold.
\end{lema}

\begin{proof}[Proof of Lemma~\ref{res:a4}]
We can write
\begin{equation*}
  \Big\lvert \WesA[*] \Big\rvert \leq 
  \Big\lvert \oclt{t} (F_t^* - \E^* F^*_t) B^* \Big\rvert
  \sup_t | H_t^* |.
\end{equation*}
From Lemma~\ref{res:a2}, $\sup_t | H_t^* | \to^p 0$ and $\oclt{t}
(F_t^* - \E^* F_t^*) = O_p(1)$, establishing~\eqref{eq:7}.  The proofs
of~\eqref{eq:9}, \eqref{eq:10}, and \eqref{eq:11}--\eqref{eq:13} are
similar.
\end{proof}

\begin{lema}\label{res:a3}
  Suppose the conditions of Theorem~\ref{res:3} hold, let
  \[\psi_t^{*}(\beta) = 
  \binom{f_t^{*}(\beta) - \bar{f}(\beta)}{h_t^{*}(\beta) - \bar h(\beta)},\]
  and define $\psi_t = \psi_t(\btrue)$, $\hat\psi_t = \psi_t(\btrue[*])$, and 
  $\psi_t^* = \psi_t^*(\btrue[*])$.  Then
  \begin{equation}\label{eq:1}
    \pr\Big[\sup_x \Big| \pr^{*}\Big[ \oclt{t} \psi_{t}^{*}
    \leq x \Big] - \pr\Big[ \oclt{t} \psi_{t}
    \leq x \Big] \Big| > \epsilon \Big] \to 0
  \end{equation}
  for all positive $\epsilon$, where the inequalities hold element-by-element.
\end{lema}

\begin{proof}[Proof of Lemma~\ref{res:a3}]
  Note that both $\psi_t$ and $\psi_t^*$ satisfy
  \clt s, (\citealp[Theorem~2]{Jon:97} and
  \citealp[Theorem~1]{Cal:13}) so it suffices to prove that
  \[ \tfrac{ 1 }{ P } \var^* \osum{t} \psi_t^* 
  - \tfrac{ 1 }{ P } \var \osum{t} \psi_t \to^p
  0. \] I will prove this result for the Circular Block Bootstrap
  \citep{PoR:92}---the proof for other block bootstraps is similar and
  follows as in \cite{Cal:13}.

  \newcommand{\su}{\tfrac{ 1 }{ b T } \sum_{ \tau = 1 }^{ T }%
  \sum_{ s, t \in I( \tau ) }}

  Since $\E^* \psi_t^* = 0$ by construction, we have
  \begin{equation}\label{eq:3}
  \tfrac{ 1 }{ P } \var^* \sum_{ t = R + 1 }^T \psi_t^*
  = \su \hat \psi_s \hat \psi_t'
  \end{equation}
  almost surely, with
  \begin{equation*}
    I(\tau) =
    \begin{cases}
      \{ \tau, \dots, \tau + b - 1 \} & \text{if } \tau \leq T - b + 1 \\
      \{ \tau, \dots, T \} \cup \{ 1, \dots, \tau - ( T - b + 1 ) \}
      & \text{if } \tau > T - b + 1.
    \end{cases}
  \end{equation*}
  We can rewrite the right side of \eqref{eq:3} as
  \begin{multline}\label{eq:6}
    \su \hat\psi_s \hat\psi_t'
    = \tfrac{1}{T} \sum_{s,t=1}^T \hat\psi_s \hat\psi_t' \;
      \big(1 - \tfrac{\lvert s - t \rvert}{b}\big)^+ \\
      + \tfrac{1}{bT} \sum_{s = 1}^{b - 1} \sum_{t = T - b + s}^{T}
        (\hat\psi_s \hat\psi_t' +
         \hat\psi_t \hat\psi_s') \;
        \big(1 - \tfrac{\lvert s - t + T\rvert}{b}\big)^+.
  \end{multline}

  The first term of \eqref{eq:6} can be shown to converge in
  probability to $\tfrac{1}{P} \var \osum{t} \psi_t$ by Theorem 2.2 of
  \citet{JoD:00}.  Most of the necessary conditions of their theorem
  hold by assumption and it remains to prove their condition (2.9)
  holds, namely that
  \begin{equation}\label{eq:2}
    \sup_{\beta \in N} \Bigg\lVert \tfrac{1}{T} \sum_{t=1}^T e^{i \xi t/b} 
    \Bigg( \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} -
          \E \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} \Bigg) 
    \Bigg\rVert_2 \to 0
  \end{equation}
  for all $j$, $k$, and $\xi$.  For any fixed $\beta \in N$,
  \begin{equation*}
    \tfrac{1}{T} \sum_{t=1}^T \cos( \xi t / b )
    \Bigg( \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} -
          \E \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} \Bigg)
    \to^p 0
  \end{equation*}
  and
  \begin{equation*}
    \tfrac{1}{T} \sum_{t=1}^T \sin( \xi t / b )
    \Bigg( \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} -
          \E \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} \Bigg)
    \to^p 0
  \end{equation*}
  by a mixingale \lln\ \citep{Dav:93}.\footnote{Remember that \ned\
    processes are also mixingales.  See, for example, \citet[Section
    17.2]{Dav:94}.}  Convergence in $L_2$ for each $\beta \in N$ then
  follows from the existence of the $r$th moment ($r > 2$) and uniform
  convergence is a consequence of our bound on the second derivative
  of $\psi_t$.

  The conclusion then holds if the second term of~\eqref{eq:6} is
  $o_p(1)$.  We have, for each element of that matrix,
  \begin{equation*}
    \Big| \tfrac{1}{bT} \sum_{s = 1}^{b - 1} \sum_{t = T - b + s}^{T}
        \hat\psi_{ks} \hat\psi_{jt}
        \big(1 - \tfrac{\lvert s - t + T\rvert}{b}\big)^+ \Big| \leq
     \tfrac{1}{b T} \sum_{s = 1}^{b - 1} |\hat\psi_{ks}|
         \sum_{t = T - b + 1}^{T} | \hat\psi_{jt} |
  \end{equation*}
  almost surely.  For any positive $\delta$,
  \begin{equation*}
    \pr \Bigg[ \tfrac{1}{T} \sum_{s = 1}^{b - 1} | \hat\psi_{is} |
              > \delta \Bigg] \leq 
    \pr \Bigg[\tfrac{1}{T} \sum_{s = 1}^{b - 1} \sup_{ \beta \in N } | \psi_{is}( \beta ) |
              > \delta \Bigg] +
    \pr \big[ \hat \beta_{T+1} \notin N \big].
  \end{equation*}
  Both terms converge to zero by assumption, so $\tfrac{1}{T} \sum_{s
    = 1}^{b - 1} |\hat\psi_{is}| = o_p(1)$.  A similar
  argument shows that $\tfrac{1}{b} \sum_{t = T - b + 1}^{T} |
  \hat\psi_{jt} | = O_p(1)$, completing the proof.
\end{proof}

\begin{lema}\label{res:a5}
  If the conditions of Theorem~\ref{res:3}, except for those governing
  the bootstrap process, hold then
  \begin{equation}
    \oclt{t}
    (\hat f_t - \E f_t) \to^d N(0, \sigma^2),
  \end{equation}
  with 
  \begin{align}
  \sigma^2 &= S_{ff} + \lambda_{fh} (F B S_{fh}' + S_{fh} B' F') + 2 \lambda_{hh} F V_\beta F' \\
  (\lambda_{fh}, \lambda_{hh}) &= 
  \begin{cases}
    (1, 2) \times \big(1 - \tfrac{1}{\pi} \ln(1 + \pi)\big) 
      & \text{recursive window} \\
    \big(\tfrac{\pi}{2}, \pi - \tfrac{\pi^2}{3}\big)
      & \text{rolling window with $\pi \leq 1$} \\
    \big(1 - \tfrac{1}{2\pi}, 1 - \tfrac{1}{3\pi}\big)
      & \text{rolling window with $\pi > 1$} \\
    (0, \pi) & \text{fixed window},
  \end{cases}
  \end{align}
  $\Big(\begin{smallmatrix}S_{ff} & S_{fh} \\ S_{fh}' &
    S_{hh} \end{smallmatrix} \Big)$ the asymptotic variance of $(\bar
  f(\btrue)', \bar h(\btrue)')'$, and $V_\beta$ the asymptotic
  variance of~$\btrue[*]$.
\end{lema}

\begin{proof}[Proof of Theorem~\ref{res:3}]
  As in \citet{Wes:96} and \citet{WeM:98} (and as in the proof of
  Theorem~\ref{res:3}), expand $\hat f_t$ around
  $\btrue$ to get
  \begin{align*}
    \sqrt{P} \big(\bar{f} - \bar{f}(\btrue)\big) &= \oclt{t}
    \big(f_t - \E f_t\big) +
    \E F_t B \oclt{t} H_t \\
    & \quad + \WesA + \WesB \\ & \quad + \WesC + \oclt{t} w_t
  \end{align*}
  where (again, as in \citealp{Wes:96}) the $i$th element of $w_t$ is
  \begin{equation*}
    w_{it} = \tfrac12 (\hat\beta_t - \btrue)'
    \Big[\tfrac{\partial^2}{\partial \beta \partial\beta'} 
    f_{it}(\tilde\beta_{it}) \Big]
    (\hat\beta_t - \btrue)
  \end{equation*}
  and each $\tilde\beta_{it}$ lies between $\hat\beta_t$ and
  $\btrue$; $\oclt{t} w_t = o_{p}(1)$ as in Theorem~\ref{res:3}.
  Then, as before,
  \begin{gather}
    \WesA \to^{p} 0 \label{eq:11} \\
    \WesB \to^{p} 0 \label{eq:12}\\
  \intertext{and}
    \WesC \to^{p} 0 \label{eq:13}
  \end{gather}
  from Lemma~\ref{res:a4}.  The formula of the asymptotic variance can
  be derived exactly as in \citet{Wes:96} and \citet{WeM:98}. Since
  $f_t$ and $H_t$ both satisfy \clt s, this completes the
  proof.
\end{proof}

\bibliography{texextra/AllRefs}
\end{document}

% LocalWords:  ClW JEL ISI Google GiW Mcc ClM CoS CCS StW IMA GiR WeM fh X'X hh
% LocalWords:  PaT AllRefs isi ima uc sv PeT GoW lm il GoK RoW Econometrica PoR
% LocalWords:  Finan StepM studentizing studentization Whi HuW RSW recentering
% LocalWords:  DiM LiS Kun McCracken lt filtrations GoJR JoD McCracken's Econom
% LocalWords:  Corradi unstudentized studentized fg gg GoJ gcalhoun HLN li PoW
% LocalWords:  Econometricians reestimate PPW resample miscentered AnG eq Helle
% LocalWords:  Bunzel Yu Hsu Pincheira HHK th AtO ik DoH Wolak's Wol stepdown
% LocalWords:  Hsu's iq mk Ames Amit Goyal rfs Ivo Welch Wes covariance MeR McW
% LocalWords:  familywise prespecified CoD Gia pointwise misspecified InK MeP
% LocalWords:  VeR xtable Dah dbframe oos parametrizations iid dgp return's CaT
% LocalWords:  outperformance Hmisc Har nondifferentiable bT jt Meng
% LocalWords:  stationarity mixingale mixingales texextra Timmermann