\documentclass[12pt,fleqn]{article}
\input{tex/setup}
\input{VERSION}
\begin{document}

\author{Gray Calhoun\thanks{ Economics Department; Iowa State
    University; Ames, IA 50011.  Telephone: (515) 294-6271.  Email:
    \guillemotleft\protect\url{gcalhoun@iastate.edu}\guillemotright.  Web:
    \guillemotleft\protect\url{http://gray.clhn.co}\guillemotright.  I'd like to
    thank Helle Bunzel, Todd Clark, and Michael McCracken
    for helpful comments and discussions.  I'd also like to thank Amit
    Goyal for providing computer code and data for his 2008
    RFS paper with Ivo
    Welch \citep{GoW:08}.} \\
  Iowa State University}

\title{A simple block bootstrap for asymptotically normal
  out-of-sample test statistics}

\maketitle

\begin{abstract} \noindent%
  This paper proposes an improved block bootstrap method for
  out-of-sample statistics. Previous block bootstrap methods for these
  statistics have centered the bootstrap out-of-sample average on the
  observed out-of-sample average, which can cause the distribution to
  be miscentered under the null --- these papers have used either a
  short out-of-sample period or an adjustment to the model parameter
  estimators under the bootstrap to correct this centering
  problem. Our approach centers the bootstrap replications correctly
  under the null while continuing to use the standard formulas to
  estimate the model parameters under the bootstrap, while allowing
  the out-of-sample period to remain large. The resulting approach is
  computationally more efficient and easier to program.

\strut

\noindent Keywords: Forecast Evaluation, Martingale Difference
Sequence, Model Selection, Family-Wise Error Rate; Multiple Testing;
Bootstrap; Reality Check

\strut

\noindent JEL Classification Numbers: C22, C53

\end{abstract}

\newpage

\section{Introduction}

This paper develops a block bootstrap method that can be used to
consistently estimate the distributions of asymptotically normal
out-of-sample (\oos) test statistics. Since the asymptotic variance of
these statistics can have a complicated formula in many applications,
bootstrap methods have the potential to be very useful in applied
work. However, existing bootstraps are either based on parametric
models of the null hypothesis (typically using sieve arguments, so
that the effect of imposing a functional form dies out in large
samples; see [fill in]), only hold with strict restrictions on the
size of the test sample \citep{Whi:00,Han:05}, or require complicated
estimation strategies in the bootstrap \citep{CoS:07}, making this
approach impractical. In this paper, we show that standard
nonparametric block bootstraps are also consistent and derive the
correct centering term to ensure consistency.

The next section presents our block bootstrap result.
Section~\ref{sec:4} concludes.

\section{The Bootstrap for Out-of-Sample Statistics}\label{sec:1b}
This section
presents the general result.  In this section, let $\to^{p^{*}}$ and
$\to^{d^{*}}$ refer to convergence in probability or distribution
conditional on the observed data.  Similarly, $\E^{*}$, $\var^{*}$,
and $\cov^{*}$ refer to the expectation, variance, and covariance with
respect to the probability measure induced by the bootstrap, and
$y_t^{*}$, etc. is the random variable $y_t$ but under the
bootstrap-induced \cdf.

\begin{asmp}\label{a1}
  The estimator $\hat{\beta}_t$ of $\btrue$ is estimated with a
  recursive, rolling, or fixed estimation window and satisfies
  $\hat{\beta}_{t} - \beta_{0} = \hat{B}_{t} H_t$; $\hat{B}_{t}$ is a
  sequence of $k \times q$ matrices such that $\sup_t |\hat{B}_t - B|
  \to^p 0$; $H_{t}$ is a sequence of $q$-vectors such that
  \begin{equation}
    H_{t} = \begin{cases}
      \tfrac1t \sum_{s=1}^t h_{s}(\btrue) & \text{recursive window} \\
      \tfrac1R \sum_{s=t-R+1}^t h_{s}(\btrue) & \text{rolling window} \\
      \tfrac1R \sum_{s=1}^R h_{s}(\btrue) & \text{fixed window} \\
    \end{cases}
  \end{equation}
  and $\E h_{s}(\btrue) = 0$ for all $s$.
\end{asmp}

\begin{asmp}\label{a2}
  Define $\psi_t(\beta) = (f_t(\beta), h_t(\beta))$; $\psi_t(\beta)$
  is covariance stationary and twice continuously differentiable in an
  open neighborhood $N$ of $\btrue$.  Also, $\E \sup_{\beta \in N}
  |\psi_t(\beta)|^r$ and $\E\sup_{\beta \in N}
  |\frac{\partial}{\partial \beta} \psi_t(\beta)|^r$ are uniformly
  bounded, and there exists a sequence of random variables $m_t$ such
  that $\sup_{i,\beta \in N} |\tfrac{\partial^2}{\partial
    \beta \partial \beta'} \psi_{it}(\beta)| \leq m_t$ almost surely
  and $\E m_t^2$ is uniformly finite.
\end{asmp}

\begin{asmp}\label{a3}
  Both $\psi_t(\beta)$ and $\frac{\partial}{\partial \beta}
  \psi_t(\beta)$ are $L_2$-\ned\ of size $-\frac12$ on the series
  $V_t$ with bounded \ned-coefficients for $\beta$ uniformly in $N$;
  $V_t$ is strong mixing of size $-\frac{r}{r-2}$ or uniform mixing of
  size $-\frac{r}{2r-2}$ with $r > 2$.%
  \footnote{For uniform mixing processes, $r = 2$ is also allowed as
    long as $\sup_{\beta \in N} \psi_t(\beta)^2$ and $\sup_{\beta \in
      N} (\partial \psi_t(\beta) / \partial \beta)^2$ are uniformly
    integrable.} %
\end{asmp}

\begin{asmp}\label{a4}
  The sequence $\hat f_1^{*},\dots,\hat f_T^{*}$ is constructed using
  the same procedure as the original \oos\ analysis over the bootstrap
  sample, where the bootstrap sample is generated by moving blocks,
  circular blocks, or stationary bootstrap with block lengths drawn
  from the geometric distribution.  The (expected) block length $b$
  satisfies $b \to \infty$ and $\frac{b}{P} \to 0$.
\end{asmp}

\begin{asmp}\label{a5}
  The bootstrapped estimator satisfies $\sup_t |\hat{B}_t^{*} - B^{*}|
  \to^p 0$, $B^{*} = B + o_p$ and $\frac1T \sum_{t=1}^T
  h_t^*(\btrue[*]) = 0$ a.s.
\end{asmp}

\begin{asmp}\label{a6}
  The asymptotic variance matrix of $\bar{f}(\btrue)$ is uniformly
  positive definite.
\end{asmp}

\begin{asmp}\label{a7}
  $R, P \to \infty$ as $T \to \infty$.
\end{asmp}

\begin{thm}\label{res:3}
  Under Assumptions~\ref{a1}--\ref{a7},
  \begin{equation}
    \pr\big[\sup_x \big\lvert \pr^*[\sqrt{P} (\bar{f}^* - \bar{f}(\btrue[*])) \leq x]
    - \pr[\sqrt{P}( \bar{f} - \E f_t) \leq x] \big\rvert > \epsilon\big] \to 0
  \end{equation}
  for all $\epsilon > 0$.
\end{thm}

\begin{rem}
  \citet{Mcc:00} proves asymptotic normality under weaker smoothness
  conditions on $f_t(\beta)$ and $h_t(\beta)$: only their expectations
  must be continuously differentiable.  It may be possible to extend
  Theorem~\ref{res:3} to those weaker conditions, but the current
  proof relies on a theorem of \citepos{JoD:00} establishing the
  consistency of \hac\ estimators and their theorem requires differentiability of
  the observations.  Extending \citepos{JoD:00} result to
  nondifferentiable functions should be possible, but is beyond the
  scope of this paper.
\end{rem}

\begin{rem}
  \citet{Whi:00} and \citet{Han:05} resample the forecasts but do not
  reestimate any of them which requires the additional assumption that
  $\tfrac{P}{R} \log \log R \to 0$ or that the forecasts themselves
  have no estimated parameters.%
\footnote{\citet{Whi:00} lists several
    different sets of assumptions that give the same result, but these
    seem to be the most general.} %
\end{rem}

\begin{rem}
  \citet{CoS:07} use the distribution of $\sqrt{P}(\bar{f}^{*} -
  \bar{f})$ to approximate that of $\sqrt{P}(\bar{f} - \E
  \bar{f}(\btrue))$.  But it is clear that $\bar{f}(\btrue[*])$
  is the bootstrap analogue of $\E \bar{f}(\btrue)$, the parameter of
  interest.  Because their bootstrap is miscentered, \citet{CoS:07}
  must redefine $\hat{\beta}_t^{*}$ to achieve consistency.  In this
  paper, though, consistency arises naturally.
\end{rem}

\begin{rem}
  It may be unnecessary to assume that $\bar{f}(\btrue)$ has positive
  definite asymptotic variance; if so, the bootstrap would work in the
  setup of \citet{ClM:05,ClM:01} and \citet{Mcc:07}.  That question is
  left to future research.
\end{rem}

\section{Conclusion}\label{sec:4}
The paper improves block bootstrap procedures of \oos\
statistics.

\appendix
\section{Proofs of Main Theoretical Results}\label{sec:B}

Define the additional notation
$F_t(\beta) = \tfrac{\partial}{\partial \beta} f_t(\beta)$,
$F_t = F_t(\btrue)$,
$\hat f_t^* = f_t^*(\hat\beta_t^*)$,
$F_t^*(\beta) = \tfrac{\partial}{\partial \beta} f_t^*(\beta)$,
$F_t^* = F_t^*(\btrue[*])$,
$h_t = h_t(\btrue)$,
$h_t^* = h_t^*(\btrue[*])$,
and
\begin{equation*}
      H_{t}^* = \begin{cases}
        \tfrac1t \sum_{s=1}^t h_{s}^* & \text{recursive window} \\
        \tfrac1R \sum_{s=t-R+1}^t h_{s}^* & \text{rolling window} \\
        \tfrac1R \sum_{s=1}^R h_{s}^* & \text{fixed window} \\
      \end{cases}
\end{equation*}

\newcommand{\WesA}[1][]{\oclt{t}
  (F_t^{#1} - \E^{#1} F_t^{#1}) B^{#1} H_t^{#1}}
\newcommand{\WesB}[1][]{\tfrac{1}{\sqrt{P}} \E^{#1} F_t^{#1} \osum{t} (B_t^{#1} -
  B^{#1}) H_t^{#1}}
\newcommand{\WesC}[1][]{\oclt{t}
  (F_t^{#1} - \E^{#1} F_t^{#1}) (B_t^{#1} - B^{#1}) H_t^{#1}}
\begin{proof}[Proof of Theorem~\ref{res:3}]
  Expand $f_t^{*}(\hat{\beta}_t^{*})$ around $\btrue[*]$
  to get
  \begin{align*}
    \sqrt{P} \big(\bar{f}^{*} - \bar{f}(\btrue[*])\big) &= \oclt{t}
    \big(f_t^{*} - \bar{f}(\btrue[*])\big) +
    \E^* F_t^{*} B^{*} \oclt{t} H_t^* \\
    & \quad + \WesA[*] + \WesB[*] \\ & \quad + \WesC[*] + \oclt{t} w_t^*
  \end{align*}
  where (as in \citealp{Wes:96}) the $i$th element of $w_t^*$ is
  \begin{equation*}
    w_{it} = \tfrac12 (\hat\beta_t^* - \hat\beta_{T+1})'
    \Big[\tfrac{\partial^2}{\partial \beta \partial\beta'}
    f_{it}^*(\tilde\beta_{it}^*) \Big]
    (\hat\beta_t^* - \hat\beta_{T+1})
  \end{equation*}
  and each $\tilde\beta_{it}^*$ lies between $\hat\beta_t^*$ and
  $\hat\beta_{T+1}$.  The argument that $\oclt{t} w_t^* = o_{p^*}(1)$ is
  identical to West's, using our Lemma~\ref{res:a2} in place of his
  Lemma A3.  Then
  \begin{gather}
    \WesA[*] \to^{p^*} 0 \label{eq:7} \\
    \WesB[*] \to^{p^*} 0 \label{eq:9} \\
  \intertext{and}
    \WesC[*] \to^{p^*} 0 \label{eq:10}
  \end{gather}
  from Lemma~\ref{res:a4} and the result follows from
  Lemma~\ref{res:a3}, with the form of the asymptotic variance
  following directly from \citet{Wes:96} and \citet{WeM:98}.
\end{proof}

\section{Supporting Results}

\begin{lema}\label{res:a2}
  Suppose $a \in [0,\frac12)$ and the conditions of Theorem~\ref{res:3}
  hold.
  \begin{enumerate}
  \item $P^a \sup_t | H_{t} | \to^p 0$ and $P^a \sup_t | H_{t}^{*} |
    \to^{p^{*}} 0$.
  \item $P^a \sup_t | \hat{\beta}_{t} - \beta_{0} | \to^{p} 0$ and
    $P^a \sup_t | \hat{\beta}^{*}_{t} - \hat{\beta}_T |
    \to^{p^{*}} 0$.
  \item $\tfrac{1}{\sqrt{P}} \sum_{t=R+1}^T (F_t - \E F_t) = O_{p}(1)$
    and $\tfrac{1}{\sqrt{P}} \sum_{t=R+1}^T (F_t^* - \E F_t^*) =
    O_{p^*}(1)$.
  \end{enumerate}
\end{lema}

\begin{proof}[Proof of Lemma~\ref{res:a2}]
  \begin{enumerate}
  \item The process $\tfrac{1}{\sqrt{T}} h_{s}$ satisfies
    \citepos[Theorem 3.1]{JoD:00b} functional \clt.  So
    \begin{equation}
      P^a \sup_t \Big| \tfrac1t \sum_{s=1}^t h_{s} \Big| =
      P^a \sup_{\gamma \in [0,1]} \Big| \tfrac{1}{\lfloor \gamma
        T\rfloor} \sum_{s=1}^{\lfloor \gamma T \rfloor} h_{s} \Big| \to^{p} 0
    \end{equation}
    with the convergence following from the continuous mapping
    theorem.  For the bootstrapped average, \citepos{Cal:14}
    Theorem~2 ensures that
    \begin{equation}
      \tfrac{1}{\sqrt{\gamma
          T}} \sum_{s=1}^{\lfloor \gamma T \rfloor} h_{s}^*
      = O_{p^*}(\Omega^*(\hat\beta_{T+1})^{1/2})
    \end{equation}
    where $\Omega^*(\beta) = \var^* h_t^*(\beta)$.  Since
    $\hat\beta_{T+1} \to^p \btrue$, it suffices to show that
    $\sup_{\beta \in N} | \Omega^*(\beta) - \var h_t(\beta) | \to^p
    0$.  But this convergence holds for any fixed $\beta$ and our
    assumptions guarantee stochastic equicontinuity as in
    \citet{JoD:00} \citep[also see][]{Dav:94}, completing the proof.

  \item We have
    \begin{multline}
      P^a \sup_t | \hat{\beta}_t - \btrue | = P^a \sup_t |\hat{B}_{t}
      H_{t}| \\ \leq \sup_{t,u} \Big| [ \hat{B}_u - B]
      \tfrac{P^a}{t} \sum_{s=1}^t h_{s} \Big| + \sup_{t} \Big|
      B \tfrac{P^a}{t} \sum_{s=1}^t h_{s} \Big|
    \end{multline}
    and both terms converge to zero in (conditional) probability by
    the previous argument and by assumption.  The same argument holds
    for $\hat{\beta}_t^{*} - \btrue[*]$ as well.
  \item $\tfrac{1}{\sqrt{P}} \sum_{t=R+1}^T (F_t - \E F_t)$ obeys
    \citepos{Jon:97} \clt, so the result is trivial.  Also,
   \begin{equation}
     \Big\lvert\tfrac{1}{\sqrt{P}} \sum_{t=R+1}^T
     ( F_t^* - \E^* F^*)\Big\rvert = O_{p^*}(\Omega^*(\hat\beta_{T+1})^{1/2}) = O_p(1)
    \end{equation}
    as in the proof of part 1, where now $\Omega^*(\beta) = \var^* F_t^*(\beta)$.
  \end{enumerate}
\end{proof}

\begin{lema}\label{res:a4}
  Under the conditions of Theorem~\ref{res:3}, Equations
  \eqref{eq:7}--\eqref{eq:10} and \eqref{eq:11}--\eqref{eq:13} hold.
\end{lema}

\begin{proof}[Proof of Lemma~\ref{res:a4}]
We can write
\begin{equation*}
  \Big\lvert \WesA[*] \Big\rvert \leq
  \Big\lvert \oclt{t} (F_t^* - \E^* F^*_t) B^* \Big\rvert
  \sup_t | H_t^* |.
\end{equation*}
From Lemma~\ref{res:a2}, $\sup_t | H_t^* | \to^p 0$ and $\oclt{t}
(F_t^* - \E^* F_t^*) = O_p(1)$, establishing~\eqref{eq:7}.  The proofs
of~\eqref{eq:9}, \eqref{eq:10}, and \eqref{eq:11}--\eqref{eq:13} are
similar.
\end{proof}

\begin{lema}\label{res:a3}
  Suppose the conditions of Theorem~\ref{res:3} hold, let
  \[\psi_t^{*}(\beta) =
  \binom{f_t^{*}(\beta) - \bar{f}(\beta)}{h_t^{*}(\beta) - \bar h(\beta)},\]
  and define $\psi_t = \psi_t(\btrue)$, $\hat\psi_t = \psi_t(\btrue[*])$, and
  $\psi_t^* = \psi_t^*(\btrue[*])$.  Then
  \begin{equation}\label{eq:1}
    \pr\Big[\sup_x \Big| \pr^{*}\Big[ \oclt{t} \psi_{t}^{*}
    \leq x \Big] - \pr\Big[ \oclt{t} \psi_{t}
    \leq x \Big] \Big| > \epsilon \Big] \to 0
  \end{equation}
  for all positive $\epsilon$, where the inequalities hold element-by-element.
\end{lema}

\begin{proof}[Proof of Lemma~\ref{res:a3}]
  Note that both $\psi_t$ and $\psi_t^*$ satisfy
  \clt s, (\citealp[Theorem~2]{Jon:97} and
  \citealp[Theorem~1]{Cal:14}) so it suffices to prove that
  \[ \tfrac{ 1 }{ P } \var^* \osum{t} \psi_t^*
  - \tfrac{ 1 }{ P } \var \osum{t} \psi_t \to^p
  0. \] I will prove this result for the Circular Block Bootstrap
  \citep{PoR:92}---the proof for other block bootstraps is similar and
  follows as in \cite{Cal:14}.

  \newcommand{\su}{\tfrac{ 1 }{ b T } \sum_{ \tau = 1 }^{ T }%
  \sum_{ s, t \in I( \tau ) }}

  Since $\E^* \psi_t^* = 0$ by construction, we have
  \begin{equation}\label{eq:3}
  \tfrac{ 1 }{ P } \var^* \sum_{ t = R + 1 }^T \psi_t^*
  = \su \hat \psi_s \hat \psi_t'
  \end{equation}
  almost surely, with
  \begin{equation*}
    I(\tau) =
    \begin{cases}
      \{ \tau, \dots, \tau + b - 1 \} & \text{if } \tau \leq T - b + 1 \\
      \{ \tau, \dots, T \} \cup \{ 1, \dots, \tau - ( T - b + 1 ) \}
      & \text{if } \tau > T - b + 1.
    \end{cases}
  \end{equation*}
  We can rewrite the right side of \eqref{eq:3} as
  \begin{multline}\label{eq:6}
    \su \hat\psi_s \hat\psi_t'
    = \tfrac{1}{T} \sum_{s,t=1}^T \hat\psi_s \hat\psi_t' \;
      \big(1 - \tfrac{\lvert s - t \rvert}{b}\big)^+ \\
      + \tfrac{1}{bT} \sum_{s = 1}^{b - 1} \sum_{t = T - b + s}^{T}
        (\hat\psi_s \hat\psi_t' +
         \hat\psi_t \hat\psi_s') \;
        \big(1 - \tfrac{\lvert s - t + T\rvert}{b}\big)^+.
  \end{multline}

  The first term of \eqref{eq:6} can be shown to converge in
  probability to $\tfrac{1}{P} \var \osum{t} \psi_t$ by Theorem 2.2 of
  \citet{JoD:00}.  Most of the necessary conditions of their theorem
  hold by assumption and it remains to prove their condition (2.9)
  holds, namely that
  \begin{equation}\label{eq:2}
    \sup_{\beta \in N} \Bigg\lVert \tfrac{1}{T} \sum_{t=1}^T e^{i \xi t/b}
    \Bigg( \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} -
          \E \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} \Bigg)
    \Bigg\rVert_2 \to 0
  \end{equation}
  for all $j$, $k$, and $\xi$.  For any fixed $\beta \in N$,
  \begin{equation*}
    \tfrac{1}{T} \sum_{t=1}^T \cos( \xi t / b )
    \Bigg( \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} -
          \E \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} \Bigg)
    \to^p 0
  \end{equation*}
  and
  \begin{equation*}
    \tfrac{1}{T} \sum_{t=1}^T \sin( \xi t / b )
    \Bigg( \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} -
          \E \frac{\partial \psi_{jt}(\beta)}{\partial \beta_k} \Bigg)
    \to^p 0
  \end{equation*}
  by a mixingale \lln\ \citep{Dav:93}.%
\footnote{Remember that \ned\
    processes are also mixingales.  See, for example, \citet[Section
    17.2]{Dav:94}.} %
  Convergence in $L_2$ for each $\beta \in N$ then
  follows from the existence of the $r$th moment ($r > 2$) and uniform
  convergence is a consequence of our bound on the second derivative
  of $\psi_t$.

  The conclusion then holds if the second term of~\eqref{eq:6} is
  $o_p(1)$.  We have, for each element of that matrix,
  \begin{equation*}
    \Big| \tfrac{1}{bT} \sum_{s = 1}^{b - 1} \sum_{t = T - b + s}^{T}
        \hat\psi_{ks} \hat\psi_{jt}
        \big(1 - \tfrac{\lvert s - t + T\rvert}{b}\big)^+ \Big| \leq
     \tfrac{1}{b T} \sum_{s = 1}^{b - 1} |\hat\psi_{ks}|
         \sum_{t = T - b + 1}^{T} | \hat\psi_{jt} |
  \end{equation*}
  almost surely.  For any positive $\delta$,
  \begin{equation*}
    \pr \Bigg[ \tfrac{1}{T} \sum_{s = 1}^{b - 1} | \hat\psi_{is} |
              > \delta \Bigg] \leq
    \pr \Bigg[\tfrac{1}{T} \sum_{s = 1}^{b - 1} \sup_{ \beta \in N } | \psi_{is}( \beta ) |
              > \delta \Bigg] +
    \pr \big[ \hat \beta_{T+1} \notin N \big].
  \end{equation*}
  Both terms converge to zero by assumption, so $\tfrac{1}{T} \sum_{s
    = 1}^{b - 1} |\hat\psi_{is}| = o_p(1)$.  A similar
  argument shows that $\tfrac{1}{b} \sum_{t = T - b + 1}^{T} |
  \hat\psi_{jt} | = O_p(1)$, completing the proof.
\end{proof}

\begin{lema}\label{res:a5}
  If the conditions of Theorem~\ref{res:3}, except for those governing
  the bootstrap process, hold then
  \begin{equation}
    \oclt{t}
    (\hat f_t - \E f_t) \to^d N(0, \sigma^2),
  \end{equation}
  with
  \begin{align}
  \sigma^2 &= S_{ff} + \lambda_{fh} (F B S_{fh}' + S_{fh} B' F') + 2 \lambda_{hh} F V_\beta F' \\
  (\lambda_{fh}, \lambda_{hh}) &=
  \begin{cases}
    (1, 2) \times \big(1 - \tfrac{1}{\pi} \ln(1 + \pi)\big)
      & \text{recursive window} \\
    \big(\tfrac{\pi}{2}, \pi - \tfrac{\pi^2}{3}\big)
      & \text{rolling window with $\pi \leq 1$} \\
    \big(1 - \tfrac{1}{2\pi}, 1 - \tfrac{1}{3\pi}\big)
      & \text{rolling window with $\pi > 1$} \\
    (0, \pi) & \text{fixed window},
  \end{cases}
  \end{align}
  $\Big(\begin{smallmatrix}S_{ff} & S_{fh} \\ S_{fh}' &
    S_{hh} \end{smallmatrix} \Big)$ the asymptotic variance of $(\bar
  f(\btrue)', \bar h(\btrue)')'$, and $V_\beta$ the asymptotic
  variance of~$\btrue[*]$.
\end{lema}

\begin{proof}[Proof of Theorem~\ref{res:3}]
  As in \citet{Wes:96} and \citet{WeM:98} (and as in the proof of
  Theorem~\ref{res:3}), expand $\hat f_t$ around
  $\btrue$ to get
  \begin{align*}
    \sqrt{P} \big(\bar{f} - \bar{f}(\btrue)\big) &= \oclt{t}
    \big(f_t - \E f_t\big) +
    \E F_t B \oclt{t} H_t \\
    & \quad + \WesA + \WesB \\ & \quad + \WesC + \oclt{t} w_t
  \end{align*}
  where (again, as in \citealp{Wes:96}) the $i$th element of $w_t$ is
  \begin{equation*}
    w_{it} = \tfrac12 (\hat\beta_t - \btrue)'
    \Big[\tfrac{\partial^2}{\partial \beta \partial\beta'}
    f_{it}(\tilde\beta_{it}) \Big]
    (\hat\beta_t - \btrue)
  \end{equation*}
  and each $\tilde\beta_{it}$ lies between $\hat\beta_t$ and
  $\btrue$; $\oclt{t} w_t = o_{p}(1)$ as in Theorem~\ref{res:3}.
  Then, as before,
  \begin{gather}
    \WesA \to^{p} 0 \label{eq:11} \\
    \WesB \to^{p} 0 \label{eq:12}\\
  \intertext{and}
    \WesC \to^{p} 0 \label{eq:13}
  \end{gather}
  from Lemma~\ref{res:a4}.  The formula of the asymptotic variance can
  be derived exactly as in \citet{Wes:96} and \citet{WeM:98}. Since
  $f_t$ and $H_t$ both satisfy \clt s, this completes the
  proof.
\end{proof}

\bibliography{texextra/references}
\end{document}

% LocalWords:  ClW JEL ISI Google GiW Mcc ClM CoS CCS StW IMA GiR WeM fh X'X hh
% LocalWords:  PaT AllRefs isi ima uc sv PeT GoW lm il GoK RoW Econometrica PoR
% LocalWords:  Finan StepM studentizing studentization Whi HuW RSW recentering
% LocalWords:  DiM LiS Kun McCracken lt filtrations GoJR JoD McCracken's Econom
% LocalWords:  Corradi unstudentized studentized fg gg GoJ gcalhoun HLN li PoW
% LocalWords:  Econometricians reestimate PPW resample miscentered AnG eq Helle
% LocalWords:  Bunzel Yu Hsu Pincheira HHK th AtO ik DoH Wolak's Wol stepdown
% LocalWords:  Hsu's iq mk Ames Amit Goyal rfs Ivo Welch Wes covariance MeR McW
% LocalWords:  familywise prespecified CoD Gia pointwise misspecified InK MeP
% LocalWords:  VeR xtable Dah dbframe oos parametrizations iid dgp return's CaT
% LocalWords:  outperformance Hmisc Har nondifferentiable bT jt Meng
% LocalWords:  stationarity mixingale mixingales texextra Timmermann